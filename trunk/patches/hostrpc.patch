diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CGBuiltin.cpp llvm-project/clang/lib/CodeGen/CGBuiltin.cpp
--- llvm-project.upstream/clang/lib/CodeGen/CGBuiltin.cpp	2023-02-27 09:21:58.339668818 -0500
+++ llvm-project/clang/lib/CodeGen/CGBuiltin.cpp	2023-03-02 13:25:13.087430575 -0500
@@ -5174,7 +5174,9 @@
   case Builtin::BIprintf:
     if (getTarget().getTriple().isNVPTX() ||
         getTarget().getTriple().isAMDGCN()) {
-      if (getLangOpts().OpenMPIsDevice)
+      if (getLangOpts().OpenMPIsDevice && getTarget().getTriple().isAMDGCN())
+	return EmitHostrpcVargsFn(E, "printf_allocate", "printf_execute");
+      if (getLangOpts().OpenMPIsDevice && getTarget().getTriple().isNVPTX())
         return EmitOpenMPDevicePrintfCallExpr(E);
       if (getTarget().getTriple().isNVPTX())
         return EmitNVPTXDevicePrintfCallExpr(E);
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CGExpr.cpp llvm-project/clang/lib/CodeGen/CGExpr.cpp
--- llvm-project.upstream/clang/lib/CodeGen/CGExpr.cpp	2023-02-27 09:21:58.347668787 -0500
+++ llvm-project/clang/lib/CodeGen/CGExpr.cpp	2023-03-02 22:30:29.424789630 -0500
@@ -5462,6 +5462,16 @@
     }
   }
 
+  // With Hostrpc, GPUs can execute certain variadic functions on the host
+  if ((CGM.getTriple().isAMDGCN() || CGM.getTriple().isNVPTX()) &&
+      CGM.getLangOpts().OpenMP && FnType && dyn_cast<FunctionProtoType>(FnType)
+      && dyn_cast<FunctionProtoType>(FnType)->isVariadic() &&
+      (std::find( std::begin(HostrpcVargFns), std::end(HostrpcVargFns),
+      E->getDirectCallee()->getNameAsString()) != std::end(HostrpcVargFns)))
+    return EmitHostrpcVargsFn(
+      E, E->getDirectCallee()->getNameAsString().append("_allocate").c_str(),
+      E->getDirectCallee()->getNameAsString().append("_execute").c_str());
+
   EmitCallArgs(Args, dyn_cast<FunctionProtoType>(FnType), E->arguments(),
                E->getDirectCallee(), /*ParamsToSkip*/ 0, Order);
 
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CGGPUBuiltin.cpp llvm-project/clang/lib/CodeGen/CGGPUBuiltin.cpp
--- llvm-project.upstream/clang/lib/CodeGen/CGGPUBuiltin.cpp	2023-02-27 09:21:35.555757241 -0500
+++ llvm-project/clang/lib/CodeGen/CGGPUBuiltin.cpp	2023-03-02 13:25:13.091430559 -0500
@@ -213,3 +213,360 @@
   return EmitDevicePrintfCallExpr(E, this, GetOpenMPVprintfDeclaration(CGM),
                                   true);
 }
+
+// EmitHostrpcVargsFn:
+//
+// For printf in an OpenMP Target region on amdgn and for variable argument
+// functions that have a supporting host service function (hostrpc) a struct
+// is created to represent the vargs for each call site.
+// The struct contains the length, number of args, an array of 4-byte keys
+// that represent the type of of each arg, an array of aligned "data" values
+// for each arg, and finally the runtime string values. If an arg is a string
+// the data value is the runtime length of the string.  Each 4-byte key
+// contains the llvm type ID and the number of bits for the type.
+// encoded by the macro PACK_TY_BITLEN(x,y) ((uint32_t)x << 16) | ((uint32_t)y)
+// The llvm type ID of a string is pointer. To distinguish string pointers
+// from non-string pointers, the number of bitlen is set to 1.
+//
+// For example, here is a 4 arg printf function
+//
+// printf("format string %d %s %f \n", (int) 1, "string2", (double) 1.234);
+//
+// is represented by a struct with these 13 elements.
+//
+//  {81, 4, 983041, 720928, 983041, 196672, 25, int 1, 7, 0, double 1.234,
+//     "format string %d %s %ld\n", "string2" }
+//
+// 81 is the total length of the buffer that must be allocated.
+// 4 is the number of arguments.
+// The next 4 key values represent the data types of the 4 args.
+// The format string length is 25.
+// The integer field is next.
+// The string argument "string2" has length 7
+// The 4-byte dummy arg 0 is inserted so the next double arg is aligned.
+// The string arguments follows the header, keys, and data args.
+//
+// Before the struct is written, a hostrpc call is is emitted  to allocate
+// memory for the transfer. Then the struct is emitted.  Then a call
+// to the execute the GPU stub function that initiates the service
+// on the host.  The host runtime passes the buffer to the service routine
+// for processing.
+
+// These static helper functions support EmitHostrpcVargsFn.
+
+// For strings that vary in length at runtime this strlen_max
+// will stop at a provided maximum.
+static llvm::Function *GetOmpStrlenDeclaration(CodeGenModule &CGM) {
+  auto &M = CGM.getModule();
+  // Args are pointer to char and maxstringlen
+  llvm::Type *ArgTypes[] = {CGM.Int8PtrTy, CGM.Int32Ty};
+  llvm::FunctionType *OmpStrlenFTy =
+      llvm::FunctionType::get(CGM.Int32Ty, ArgTypes, false);
+  if (auto *F = M.getFunction("__strlen_max")) {
+    assert(F->getFunctionType() == OmpStrlenFTy);
+    return F;
+  }
+  llvm::Function *FN = llvm::Function::Create(
+      OmpStrlenFTy, llvm::GlobalVariable::ExternalLinkage, "__strlen_max", &M);
+  return FN;
+}
+
+// Deterimines if an expression is a string with variable lenth
+static bool isVarString(const clang::Expr *argX, const clang::Type *argXTy,
+                        const llvm::Value *Arg) {
+  if ((argXTy->isPointerType() || argXTy->isConstantArrayType()) &&
+      argXTy->getPointeeOrArrayElementType()->isCharType() && !argX->isLValue())
+    return true;
+  // Ensure the VarDecl has an inititalizer
+  if (const auto *DRE = dyn_cast<DeclRefExpr>(argX))
+    if (const auto *VD = dyn_cast<VarDecl>(DRE->getDecl()))
+      if (!VD->getInit() ||
+          !llvm::isa<StringLiteral>(VD->getInit()->IgnoreImplicit()))
+        return true;
+  return false;
+}
+
+// Deterimines if an argument is a string
+static bool isString(const clang::Type *argXTy) {
+  if ((argXTy->isPointerType() || argXTy->isConstantArrayType()) &&
+      argXTy->getPointeeOrArrayElementType()->isCharType())
+    return true;
+  else
+    return false;
+}
+
+// Gets a string literal to write into the transfer buffer
+static const StringLiteral *getSL(const clang::Expr *argX,
+                                  const clang::Type *argXTy) {
+  // String in argX has known constant length
+  if (!argXTy->isConstantArrayType()) {
+    // Allow constant string to be a declared variable,
+    // But it must be constant and initialized.
+    const DeclRefExpr *DRE = cast<DeclRefExpr>(argX);
+    const VarDecl *VarD = cast<VarDecl>(DRE->getDecl());
+    argX = VarD->getInit()->IgnoreImplicit();
+  }
+  const StringLiteral *SL = cast<StringLiteral>(argX);
+  return SL;
+}
+
+// Returns a function pointer to the memory allocation routine
+static llvm::Function *GetVargsFnAllocDeclaration(CodeGenModule &CGM,
+                                                  const char *GPUAllocateName) {
+  auto &M = CGM.getModule();
+  llvm::Type *ArgTypes[] = {CGM.Int32Ty};
+  llvm::Function *FN;
+  llvm::FunctionType *VargsFnAllocFuncType = llvm::FunctionType::get(
+      llvm::PointerType::getUnqual(CGM.Int8Ty), ArgTypes, false);
+
+  if (!(FN = M.getFunction(GPUAllocateName)))
+    FN = llvm::Function::Create(VargsFnAllocFuncType,
+                                llvm::GlobalVariable::ExternalLinkage,
+                                GPUAllocateName, &M);
+  assert(FN->getFunctionType() == VargsFnAllocFuncType);
+  return FN;
+}
+
+// Returns a function pointer to the GPU stub function
+static llvm::Function *
+hostrpcVargsReturnsFnDeclaration(CodeGenModule &CGM, QualType Ty,
+                                 const char *GPUStubFunctionName) {
+  auto &M = CGM.getModule();
+  llvm::Type *ArgTypes[] = {llvm::PointerType::getUnqual(CGM.Int8Ty),
+                            CGM.Int32Ty};
+  llvm::Function *FN;
+  llvm::FunctionType *VarfnFuncType =
+      llvm::FunctionType::get(CGM.getTypes().ConvertType(Ty), ArgTypes, false);
+  if (!(FN = M.getFunction(GPUStubFunctionName)))
+    FN = llvm::Function::Create(VarfnFuncType,
+                                llvm::GlobalVariable::ExternalLinkage,
+                                GPUStubFunctionName, &M);
+  assert(FN->getFunctionType() == VarfnFuncType);
+  return FN;
+}
+
+// The macro to pack the llvm type ID and numbits into 4-byte key
+#define PACK_TY_BITLEN(x, y) ((uint32_t)x << 16) | ((uint32_t)y)
+
+// Emit the code to support a host vargs function such as printf.
+RValue CodeGenFunction::EmitHostrpcVargsFn(const CallExpr *E,
+                                           const char *GPUAllocateName,
+                                           const char *GPUStubFunctionName) {
+  assert(getTarget().getTriple().isAMDGCN() || 
+		  getTarget().getTriple().isNVPTX());
+  // assert(E->getBuiltinCallee() == Builtin::BIprintf);
+  assert(E->getNumArgs() >= 1); // rpc varfn always has at least one arg.
+
+  const llvm::DataLayout &DL = CGM.getDataLayout();
+
+  CallArgList Args;
+  EmitCallArgs(Args,
+               E->getDirectCallee()->getType()->getAs<FunctionProtoType>(),
+               E->arguments(), E->getDirectCallee(),
+               /* ParamsToSkip = */ 0);
+
+  // We don't know how to emit non-scalar varargs.
+  if (std::any_of(Args.begin() + 1, Args.end(), [&](const CallArg &A) {
+        return !A.getRValue(*this).isScalar();
+      })) {
+    CGM.ErrorUnsupported(E, "non-scalar arg in GPU vargs function");
+    return RValue::get(llvm::ConstantInt::get(IntTy, 0));
+  }
+
+  unsigned NumArgs = (unsigned)Args.size();
+  llvm::SmallVector<llvm::Type *, 32> ArgTypes;
+  llvm::SmallVector<llvm::Value *, 32> VarStrLengths;
+  llvm::Value *TotalVarStrsLength = llvm::ConstantInt::get(Int32Ty, 0);
+  bool hasVarStrings = false;
+  ArgTypes.push_back(Int32Ty); // First field in struct will be total DataLen
+  ArgTypes.push_back(Int32Ty); // 2nd field in struct will be num args
+  // An array of 4-byte keys that describe the arg type
+  for (unsigned I = 0; I < NumArgs; ++I)
+    ArgTypes.push_back(Int32Ty);
+
+  // Track the size of the numeric data length and string length
+  unsigned DataLen_CT =
+      (unsigned)(DL.getTypeAllocSize(Int32Ty)) * (NumArgs + 2);
+  unsigned AllStringsLen_CT = 0;
+
+  // ---  1st Pass over Args to create ArgTypes and count size ---
+
+  size_t structOffset = 4 * (NumArgs + 2);
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Value *Arg = Args[I].getRValue(*this).getScalarVal();
+    llvm::Type *ArgType = Arg->getType();
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        hasVarStrings = true;
+        if (auto *PtrTy = dyn_cast<llvm::PointerType>(ArgType))
+          if (PtrTy->getPointerAddressSpace()) {
+            Arg = Builder.CreateAddrSpaceCast(Arg, CGM.Int8PtrTy);
+            ArgType = Arg->getType();
+          }
+        llvm::Value *VarStrLen =
+            Builder.CreateCall(GetOmpStrlenDeclaration(CGM),
+                               {Arg, llvm::ConstantInt::get(Int32Ty, 1024)});
+        VarStrLengths.push_back(VarStrLen);
+        TotalVarStrsLength = Builder.CreateAdd(TotalVarStrsLength, VarStrLen,
+                                               "sum_of_var_strings_length");
+        ArgType = Int32Ty;
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        AllStringsLen_CT += ((int)ArgString.size() + 1);
+        // change ArgType from char ptr to int to contain string length
+        ArgType = Int32Ty;
+      }
+    } // end of processing string argument
+    // if ArgTypeSize is >4 bytes we need to insert dummy align
+    // values in the struct so all stores can be aligned .
+    // These dummy fields must be inserted before the arg.
+    //
+    // In the pass below where the stores are generated careful
+    // tracking of the index into the struct is necessary.
+    size_t needsPadding = (structOffset % (size_t)DL.getTypeAllocSize(ArgType));
+    if (needsPadding) {
+      DataLen_CT += (unsigned)needsPadding;
+      structOffset += needsPadding;
+      ArgTypes.push_back(Int32Ty); // should assert that needsPadding == 4 here
+    }
+
+    ArgTypes.push_back(ArgType);
+    DataLen_CT += ((int)DL.getTypeAllocSize(ArgType));
+    structOffset += (size_t)DL.getTypeAllocSize(ArgType);
+  }
+
+  // ---  Generate call to printf_alloc to get pointer to data structure  ---
+  if (hasVarStrings)
+    TotalVarStrsLength = Builder.CreateAdd(
+        TotalVarStrsLength,
+        llvm::ConstantInt::get(Int32Ty, AllStringsLen_CT + DataLen_CT),
+        "total_buffer_size");
+  llvm::Value *BufferLen =
+      hasVarStrings
+          ? TotalVarStrsLength
+          : llvm::ConstantInt::get(Int32Ty, AllStringsLen_CT + DataLen_CT);
+
+  llvm::Value *DataStructPtr = Builder.CreateCall(
+      GetVargsFnAllocDeclaration(CGM, GPUAllocateName), {BufferLen});
+
+  // cast the generic return pointer to be a struct in device global memory
+  llvm::StructType *DataStructTy =
+      llvm::StructType::create(ArgTypes, "varfn_args_store");
+  unsigned AS = getContext().getTargetAddressSpace(LangAS::cuda_device);
+  llvm::Value *BufferPtr = Builder.CreatePointerCast(
+      DataStructPtr, llvm::PointerType::get(DataStructTy, AS),
+      "varfn_args_store_casted");
+
+  // ---  Header of struct contains length and NumArgs ---
+  llvm::Value *DataLenField = llvm::ConstantInt::get(Int32Ty, DataLen_CT);
+  llvm::Value *P = Builder.CreateStructGEP(DataStructTy, BufferPtr, 0);
+  Builder.CreateAlignedStore(
+      DataLenField, P, DL.getPrefTypeAlign(DataLenField->getType()));
+  llvm::Value *NumArgsField = llvm::ConstantInt::get(Int32Ty, NumArgs);
+  P = Builder.CreateStructGEP(DataStructTy, BufferPtr, 1);
+  Builder.CreateAlignedStore(
+      NumArgsField, P, DL.getPrefTypeAlign(NumArgsField->getType()));
+
+  // ---  2nd Pass: create array of 4-byte keys to describe each arg
+
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Type *ty = Args[I].getRValue(*this).getScalarVal()->getType();
+    llvm::Type::TypeID argtypeid =
+        Args[I].getRValue(*this).getScalarVal()->getType()->getTypeID();
+
+    // Get type size in bits. Usually 64 or 32.
+    uint32_t numbits = 0;
+    if (isString(E->getArg(I)->IgnoreParenCasts()->getType().getTypePtr()))
+      // The llvm typeID for string is pointer.  Since pointer numbits is 0,
+      // we set numbits to 1 to distinguish pointer type ID as string pointer.
+      numbits = 1;
+    else
+      numbits = ty->getScalarSizeInBits();
+    // Create a key that combines llvm typeID and size
+    llvm::Value *Key =
+        llvm::ConstantInt::get(Int32Ty, PACK_TY_BITLEN(argtypeid, numbits));
+    P = Builder.CreateStructGEP(DataStructTy, BufferPtr, I + 2);
+    Builder.CreateAlignedStore(Key, P, DL.getPrefTypeAlign(Key->getType()));
+  }
+
+  // ---  3rd Pass: Store thread-specfic data values for each arg ---
+
+  unsigned varstring_index = 0;
+  unsigned structIndex = 2 + NumArgs;
+  structOffset = 4 * structIndex;
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Value *Arg;
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        Arg = VarStrLengths[varstring_index];
+        varstring_index++;
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        int ArgStrLen = (int)ArgString.size() + 1;
+        // Change Arg from a char pointer to the integer string length
+        Arg = llvm::ConstantInt::get(Int32Ty, ArgStrLen);
+      }
+    } else {
+      Arg = Args[I].getKnownRValue().getScalarVal();
+    }
+    size_t structElementSize = (size_t)DL.getTypeAllocSize(Arg->getType());
+    size_t needsPadding = (structOffset % structElementSize);
+    if (needsPadding) {
+      // Skip over dummy fields in struct to align
+      structOffset += needsPadding; // should assert needsPadding == 4
+      structIndex++;
+    }
+    P = Builder.CreateStructGEP(DataStructTy, BufferPtr, structIndex);
+    Builder.CreateAlignedStore(Arg, P, DL.getPrefTypeAlign(Arg->getType()));
+    structOffset += structElementSize;
+    structIndex++;
+  }
+
+  // ---  4th Pass: memcpy all strings after the data values ---
+
+  // bitcast the struct in device global memory as a char buffer
+  Address BufferPtrByteAddr = Address(
+      Builder.CreatePointerCast(BufferPtr, llvm::PointerType::get(Int8Ty, AS)),
+      Int8Ty, CharUnits::fromQuantity(1));
+  // BufferPtrByteAddr is a pointer to where we want to write the next string
+  BufferPtrByteAddr = Builder.CreateConstInBoundsByteGEP(
+      BufferPtrByteAddr, CharUnits::fromQuantity(DataLen_CT));
+  varstring_index = 0;
+  for (unsigned I = 0; I < NumArgs; ++I) {
+    llvm::Value *Arg = Args[I].getKnownRValue().getScalarVal();
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        llvm::Value *varStrLength = VarStrLengths[varstring_index];
+        varstring_index++;
+        Address SrcAddr = Address(Arg, Int8Ty, CharUnits::fromQuantity(1));
+        Builder.CreateMemCpy(BufferPtrByteAddr, SrcAddr, varStrLength);
+        // update BufferPtrByteAddr for next string memcpy
+        llvm::Value *PtrAsInt = BufferPtrByteAddr.getPointer();
+        BufferPtrByteAddr = Address(
+            Builder.CreateGEP(Int8Ty,
+		    PtrAsInt, ArrayRef<llvm::Value*>(varStrLength)),
+            Int8Ty, CharUnits::fromQuantity(1));
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        int ArgStrLen = (int)ArgString.size() + 1;
+        Address SrcAddr = CGM.GetAddrOfConstantStringFromLiteral(SL);
+        Builder.CreateMemCpy(BufferPtrByteAddr, SrcAddr, ArgStrLen);
+        // update BufferPtrByteAddr for next memcpy
+        BufferPtrByteAddr = Builder.CreateConstInBoundsByteGEP(
+            BufferPtrByteAddr, CharUnits::fromQuantity(ArgStrLen));
+      }
+    }
+  }
+  return RValue::get(Builder.CreateCall(
+      hostrpcVargsReturnsFnDeclaration(CGM, E->getType(), GPUStubFunctionName),
+      {DataStructPtr, BufferLen}));
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CodeGenFunction.h llvm-project/clang/lib/CodeGen/CodeGenFunction.h
--- llvm-project.upstream/clang/lib/CodeGen/CodeGenFunction.h	2023-02-27 09:21:58.363668725 -0500
+++ llvm-project/clang/lib/CodeGen/CodeGenFunction.h	2023-03-02 22:25:08.330247204 -0500
@@ -4131,6 +4131,11 @@
   RValue EmitNVPTXDevicePrintfCallExpr(const CallExpr *E);
   RValue EmitAMDGPUDevicePrintfCallExpr(const CallExpr *E);
   RValue EmitOpenMPDevicePrintfCallExpr(const CallExpr *E);
+  std::vector<std::string> HostrpcVargFns{"fprintf", "hostrpc_varfn_uint",
+    "hostrpc_varfn_uint64", "hostrpc_varfn_int", "hostrpc_varfn_long",
+    "hostrpc_varfn_float", "hostrpc_varfn_double", "printf"} ;
+  RValue EmitHostrpcVargsFn(const CallExpr *E, const char *allocate_name,
+                            const char *execute_name);
 
   RValue EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,
                          const CallExpr *E, ReturnValueSlot ReturnValue);
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/Driver/ToolChains/Clang.cpp llvm-project/clang/lib/Driver/ToolChains/Clang.cpp
--- llvm-project.upstream/clang/lib/Driver/ToolChains/Clang.cpp	2023-02-27 09:21:58.371668694 -0500
+++ llvm-project/clang/lib/Driver/ToolChains/Clang.cpp	2023-03-02 13:25:13.095430543 -0500
@@ -8164,7 +8164,10 @@
   assert(Input.isFilename() && "Invalid input.");
   CmdArgs.push_back(Input.getFilename());
 
-  const char *Exec = getToolChain().getDriver().getClangProgramPath();
+  // Get "clang" specifically, because the driver name might be set to flang-new
+  // instead, which does not support -cc1as but does also not require custom
+  // handling of this action
+  const char *Exec = Args.MakeArgString(getToolChain().GetProgramPath("clang"));
   if (D.CC1Main && !D.CCGenDiagnostics) {
     // Invoke cc1as directly in this process.
     C.addCommand(std::make_unique<CC1Command>(
@@ -8428,6 +8431,8 @@
   const Driver &D = getToolChain().getDriver();
   const llvm::Triple TheTriple = getToolChain().getTriple();
   ArgStringList CmdArgs;
+  bool isNVPTX = false;
+  bool isAMDGCN = false;
 
   // Pass the CUDA path to the linker wrapper tool.
   for (Action::OffloadKind Kind : {Action::OFK_Cuda, Action::OFK_OpenMP}) {
@@ -8435,12 +8440,15 @@
     for (auto &I : llvm::make_range(TCRange.first, TCRange.second)) {
       const ToolChain *TC = I.second;
       if (TC->getTriple().isNVPTX()) {
+	isNVPTX = true;
         CudaInstallationDetector CudaInstallation(D, TheTriple, Args);
         if (CudaInstallation.isValid())
           CmdArgs.push_back(Args.MakeArgString(
               "--cuda-path=" + CudaInstallation.getInstallPath()));
         break;
       }
+      if (TC->getTriple().isAMDGCN())
+	isAMDGCN = true;
     }
   }
 
@@ -8464,6 +8472,11 @@
     }
   }
 
+  if (isNVPTX)
+    CmdArgs.push_back(Args.MakeArgString(D.Dir + "/../lib/hostrpc-stubs-nvptx.o"));
+  if (isAMDGCN)
+    CmdArgs.push_back(Args.MakeArgString(D.Dir + "/../lib/hostrpc-stubs-amdgcn.o"));
+
   CmdArgs.push_back(
       Args.MakeArgString("--host-triple=" + TheTriple.getTriple()));
   if (Args.hasArg(options::OPT_v))
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/Driver/ToolChains/Flang.cpp llvm-project/clang/lib/Driver/ToolChains/Flang.cpp
--- llvm-project.upstream/clang/lib/Driver/ToolChains/Flang.cpp	2023-02-27 09:21:58.371668694 -0500
+++ llvm-project/clang/lib/Driver/ToolChains/Flang.cpp	2023-02-28 09:28:13.208810525 -0500
@@ -357,6 +357,17 @@
 
   assert(Input.isFilename() && "Invalid input.");
 
+  bool IsHostOffloadingAction = JA.isHostOffloading(Action::OFK_OpenMP) ||
+                                JA.isHostOffloading(C.getActiveOffloadKinds());
+  for (const InputInfo &I : Inputs) {
+    if (&I == &Input || I.getType() == types::TY_Nothing) {
+      // This is the primary input or contains nothing.
+    } else if (IsHostOffloadingAction) {
+      CmdArgs.push_back(Args.MakeArgString("-fembed-offload-object=" +
+                                           TC.getInputFilename(I)));
+    }
+  }
+
   addDashXForInput(Args, Input, CmdArgs);
 
   CmdArgs.push_back(Input.getFilename());
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/CMakeLists.txt llvm-project/openmp/libomptarget/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/CMakeLists.txt	2023-02-27 09:21:39.107743468 -0500
+++ llvm-project/openmp/libomptarget/CMakeLists.txt	2023-03-02 13:25:13.095430543 -0500
@@ -108,6 +108,7 @@
 add_subdirectory(plugins)
 add_subdirectory(plugins-nextgen)
 add_subdirectory(DeviceRTL)
+add_subdirectory(hostrpc)
 add_subdirectory(tools)
 
 # Add tests.
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/CMakeLists.txt llvm-project/openmp/libomptarget/hostrpc/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/hostrpc/CMakeLists.txt	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/CMakeLists.txt	2023-03-02 21:10:27.352481297 -0500
@@ -0,0 +1,191 @@
+##===----------------------------------------------------------------------===##
+#
+# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+#
+##===----------------------------------------------------------------------===##
+#
+# Build hostrpc support as part of libomptarget
+#
+##===----------------------------------------------------------------------===##
+
+cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
+
+if("${CMAKE_SOURCE_DIR}" STREQUAL "${CMAKE_CURRENT_SOURCE_DIR}")
+  message(FATAL_ERROR "Direct configuration not supported, please use parent directory!")
+endif()
+
+find_package(hsa-runtime64 1.2.0 QUIET HINTS ${CMAKE_INSTALL_PREFIX} PATHS /opt/rocm/hsa)
+if (hsa-runtime64_FOUND)
+   set(HOSTRPC_BUILD_AMD TRUE)
+   list(APPEND HOSTRPC_ARCHS "amdgcn")
+else()
+   libomptarget_say("Not building hostrpc for AMDGCN because hsa-runtime64 not found")
+endif()
+
+if (LIBOMPTARGET_DEP_CUDA_FOUND)
+   set(HOSTRPC_BUILD_NVPTX TRUE)
+   list(APPEND HOSTRPC_ARCHS "nvptx")
+else()
+   libomptarget_say("Not building hostrpc for NVPTX because cuda not found")
+endif()
+
+if(HOSTRPC_ARCHS)
+   add_subdirectory(services)
+else()
+   return()
+endif()
+
+if (LLVM_DIR)
+  message("   -- Building hostrpc with LLVM ${LLVM_PACKAGE_VERSION} found with CLANG_TOOL ${CLANG_TOOL}")
+  find_program(CLANG_TOOL clang PATHS ${LLVM_TOOLS_BINARY_DIR} NO_DEFAULT_PATH)
+  find_program(PACKAGER_TOOL clang-offload-packager PATHS ${LLVM_TOOLS_BINARY_DIR} NO_DEFAULT_PATH)
+  find_program(LINK_TOOL llvm-link PATHS ${LLVM_TOOLS_BINARY_DIR} NO_DEFAULT_PATH)
+else()
+  message("   ERROR: NO LLVM FOUND! Not building hostrpc .")
+  return()
+endif()
+
+set(amdgpu_mcpus gfx700 gfx701 gfx801 gfx803 gfx900 gfx902 gfx906 gfx908 gfx90a gfx90c gfx940 gfx1010 gfx1030 gfx1031 gfx1032 gfx1033 gfx1034 gfx1035 gfx1036 gfx1100 gfx1101 gfx1102 gfx1103)
+if (DEFINED LIBOMPTARGET_AMDGCN_GFXLIST)
+  set(amdgpu_mcpus ${LIBOMPTARGET_AMDGCN_GFXLIST})
+endif()
+
+set(all_capabilities 35 37 50 52 53 60 61 62 70 72 75 80 86 89 90)
+set(LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES ${all_capabilities} CACHE STRING
+  "List of CUDA Compute Capabilities to be used to compile the NVPTX DeviceRTL.")
+string(TOLOWER ${LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES} LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES)
+if (LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES STREQUAL "all")
+  set(nvptx_sm_list ${all_capabilities})
+elseif(LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES STREQUAL "auto")
+  if (NOT LIBOMPTARGET_DEP_CUDA_FOUND)
+    libomptarget_error_say("[NVPTX] Cannot auto detect compute capability as CUDA not found.")
+  endif()
+  set(nvptx_sm_list ${LIBOMPTARGET_DEP_CUDA_ARCH})
+else()
+  string(REPLACE "," ";" nvptx_sm_list "${LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES}")
+endif()
+foreach(sm ${nvptx_sm_list})
+   list(APPEND nvptx_mcpus "sm_${sm}")
+endforeach()
+
+foreach(archname ${HOSTRPC_ARCHS})
+   if (${archname} STREQUAL "amdgcn")
+      set(triple "amdgcn-amd-amdhsa")
+      set(mcpus ${amdgpu_mcpus})
+   endif()
+   if (${archname} STREQUAL "nvptx")
+      set(triple "nvptx64-nvidia-cuda")
+      set(mcpus ${nvptx_mcpus})
+   endif()
+
+   set(ocl_atomics_cl_filename ${CMAKE_CURRENT_SOURCE_DIR}/src/oclAtomics.cl)
+   set(invoke_cpp_file_name ${CMAKE_CURRENT_SOURCE_DIR}/src/hostrpc_invoke.cpp)
+   set(hostrpc_stubs_filename ${CMAKE_CURRENT_SOURCE_DIR}/src/hostrpc_stubs.cpp)
+   set(hostrpc_fallback_filename ${CMAKE_CURRENT_SOURCE_DIR}/src/hostrpc_fallback.cpp)
+   set(h_file           ${CMAKE_CURRENT_SOURCE_DIR}/src/hostrpc.h)
+
+   if (${archname} STREQUAL "amdgcn")
+      set(opencl_cmd ${CLANG_TOOL}
+       -fvisibility=default
+       -c -emit-llvm
+       -DCL_VERSION_2_0=200 -D__OPENCL_C_VERSION__=200
+       -Dcl_khr_fp64 -Dcl_khr_fp16
+       -Dcl_khr_subgroups -Dcl_khr_int64_base_atomics -Dcl_khr_int64_extended_atomics
+       -x cl -Xclang -cl-std=CL2.0 -Xclang -finclude-default-header
+       -target amdgcn-amd-amdhsa )
+      set(ocl_atomics_cl_bc "ocl_atomics_${archname}.bc")
+      add_custom_target(${ocl_atomics_cl_bc}
+      COMMAND ${opencl_cmd} ${ocl_atomics_cl_filename} -o ${ocl_atomics_cl_bc}
+      DEPENDS ${ocl_atomics_cl_filename})
+   endif()
+
+   set(openmp_host_args
+    -fopenmp --offload-arch=${mcpu} -c -emit-llvm --offload-host-only )
+   set(host_bc_file_name host-${archname}.bc)
+   add_custom_target(${host_bc_file_name}
+      COMMAND ${CLANG_TOOL} ${openmp_host_args} ${hostrpc_fallback_filename} -o ${host_bc_file_name}
+      DEPENDS ${hostrpc_fallback_filename})
+
+   foreach(mcpu ${mcpus})
+      if (${archname} STREQUAL "amdgcn")
+         set(openmp_device_args 
+	      -I../../runtime/src  # to pickup omp.h, we may need a dependency
+	      --offload-arch=${mcpu} 
+	      -fopenmp -fopenmp-cuda-mode -mllvm -openmp-opt-disable 
+	      -std=c++17 -fvisibility=hidden 
+	      -fopenmp-targets=${triple}
+	      -c -emit-llvm --offload-device-only )
+      #-c -emit-llvm --offload-device-only -nogpulib )
+      endif()
+      if (${archname} STREQUAL "nvptx")
+         set(openmp_device_args 
+	      -I../../runtime/src  # to pickup omp.h, we may need a dependency
+	      --offload-arch=${mcpu} 
+	      -fopenmp -fopenmp-cuda-mode -mllvm -openmp-opt-disable 
+	      -std=c++17 -fvisibility=hidden 
+	      -fopenmp-targets=${triple} --cuda-feature=+ptx61,+${mcpu}
+	      -c -emit-llvm --offload-device-only -nocudalib -nogpulib 
+	      -Wno-unknown-cuda-version)
+      endif()
+
+      set(bc_filename "hostrpc-stubs-${archname}-${mcpu}.bc")
+      add_custom_target(${bc_filename}
+         COMMAND ${CLANG_TOOL} ${openmp_device_args} ${hostrpc_stubs_filename} -o ${bc_filename}
+         DEPENDS ${hostrpc_stubs_filename} ${h_file}
+         COMMENT "Built file ${bc_filename}")
+
+      set(hostrpc_invoke_cpp_bc "hostrpc-invoke-${archname}-${mcpu}.bc")
+      add_custom_target(${hostrpc_invoke_cpp_bc}
+         COMMAND ${CLANG_TOOL} ${openmp_device_args} ${invoke_cpp_file_name} -o ${hostrpc_invoke_cpp_bc}
+	 DEPENDS ${invoke_cpp_file_name}
+         COMMENT "Building bc file for hostrpc_invoke: ${hostrpc_invoke_cpp_bc}")
+
+      #  amdgcn needs to compile and link in ocl_atomics
+      if (${archname} STREQUAL "amdgcn")
+         set(invoke_resolved_bc "invoked_resolved-${archname}-${mcpu}.bc")
+         add_custom_target(${invoke_resolved_bc}
+            COMMAND ${LINK_TOOL} ${hostrpc_invoke_cpp_bc} --internalize --only-needed ${ocl_atomics_cl_bc} -o ${invoke_resolved_bc}
+	    DEPENDS ${hostrpc_invoke_cpp_bc} ${ocl_atomics_cl_bc}
+            COMMENT "Building invoke resolved bc ${invoke_resolved_bc}")
+          add_dependencies(${invoke_resolved_bc} ${hostrpc_invoke_cpp_bc} ${ocl_atomics_cl_bc})
+      else()
+          # for nvptx we dont need ocl_atomics
+         set(invoke_resolved_bc ${hostrpc_invoke_cpp_bc})
+      endif()
+
+      set(libhostrpc-bc "libhostrpc-${archname}-${mcpu}.bc")
+      add_custom_target(${libhostrpc-bc}
+         COMMAND ${LINK_TOOL} ${bc_filename} --internalize --only-needed ${invoke_resolved_bc} -o ${libhostrpc-bc}
+	 DEPENDS ${invoke_resolved_bc} ${bc_filename} 
+         COMMENT "Building hostrpc file ${libhostrpc-bc}")
+      add_dependencies(${libhostrpc-bc} ${bc_filename} ${invoke_resolved_bc})
+
+      list(APPEND packager_args "--image=file=${libhostrpc-bc},triple=${triple},arch=${mcpu},kind=openmp")
+      list(APPEND libhostrpc-bcs ${libhostrpc-bc})
+   endforeach()
+
+   set(image_file_name ${archname}-image.out)
+   add_custom_target(${image_file_name}
+      COMMAND ${PACKAGER_TOOL} -o ${image_file_name} ${packager_args}
+      COMMENT "COMMENT: BUilding ${image_file_name}")
+   add_dependencies(${image_file_name} ${libhostrpc-bcs})
+
+   set(EMBEDDED_OBJECT_FILE "hostrpc-stubs-${archname}.o")
+   add_custom_target(${EMBEDDED_OBJECT_FILE}
+      COMMAND ${CLANG_TOOL} -cc1 -emit-obj -fopenmp -fembed-offload-object=${image_file_name} -o ${EMBEDDED_OBJECT_FILE} -x ir ${host_bc_file_name}
+      COMMENT "COMMENT: Building ${EMBEDDED_OBJECT_FILE}")
+   add_dependencies(${EMBEDDED_OBJECT_FILE} ${image_file_name} ${host_bc_file_name})
+   if (${archname} STREQUAL "amdgcn")
+      add_dependencies(amdgcn_hostrpc_services ${EMBEDDED_OBJECT_FILE})
+   endif()
+   if (${archname} STREQUAL "nvptx")
+      add_dependencies(nvptx_hostrpc_services ${EMBEDDED_OBJECT_FILE})
+   endif()
+   install(FILES ${CMAKE_CURRENT_BINARY_DIR}/${EMBEDDED_OBJECT_FILE} DESTINATION ${DEVEL_PACKAGE}lib)
+
+endforeach() # end foreach archs
+set(CLANG_VERSION_MAJOR 17)
+set(header_install_dir lib/clang/${CLANG_VERSION_MAJOR}/include)
+install(FILES "${CMAKE_CURRENT_SOURCE_DIR}/src/hostrpc.h" DESTINATION ${header_install_dir})
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/services/amdgcn_hostrpc.cpp llvm-project/openmp/libomptarget/hostrpc/services/amdgcn_hostrpc.cpp
--- llvm-project.upstream/openmp/libomptarget/hostrpc/services/amdgcn_hostrpc.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/services/amdgcn_hostrpc.cpp	2023-03-02 21:02:45.962239593 -0500
@@ -0,0 +1,553 @@
+
+#include <hsa/hsa.h>
+#include <hsa/hsa_ext_amd.h>
+#include "urilocator.h"
+#include "execute_service.h"
+#include "../src/hostrpc_internal.h"
+#include <list>
+#include <atomic>
+#include <cstring>
+#include <functional>
+#include <iostream>
+#include <mutex>
+#include <thread>
+
+/*
+ *  A typical flow is as follows:
+ *  - Create and launch one or more hostcall consumers.
+ *  - Create and initialize a buffer per hsa queue.
+ *  - Register buffers with appropriate consumer.
+ *  - When a buffer is no longer used, deregister and then free it.
+ *  - Destroy the consumer(s) when they are no longer required. Must be
+ *    done before exiting the application, so that the consumer
+ *    threads can join() correctly.
+ */
+
+// Error codes for hostrpc consumer_t methods
+typedef enum {
+  CONSUMER_SUCCESS,
+  CONSUMER_ERROR_CONSUMER_ACTIVE,
+  CONSUMER_ERROR_CONSUMER_INACTIVE,
+  CONSUMER_ERROR_CONSUMER_LAUNCH_FAILED,
+  CONSUMER_ERROR_INVALID_REQUEST,
+  CONSUMER_ERROR_SERVICE_UNKNOWN,
+  CONSUMER_ERROR_INCORRECT_ALIGNMENT,
+  CONSUMER_ERROR_NULLPTR,
+  CONSUMER_ERROR_WRONGVERSION,
+  CONSUMER_ERROR_OLDHOSTVERSIONMOD,
+} consumer_error_t;
+
+/** Opaque wrapper for signal */
+typedef struct {
+  uint64_t handle;
+} signal_t;
+
+/** Field offsets in the packet control field */
+typedef enum {
+  CONTROL_OFFSET_READY_FLAG = 0,
+  CONTROL_OFFSET_RESERVED0 = 1,
+} control_offset_t;
+
+/** Field widths in the packet control field */
+typedef enum {
+  CONTROL_WIDTH_READY_FLAG = 1,
+  CONTROL_WIDTH_RESERVED0 = 31,
+} control_width_t;
+
+/** Packet header */
+typedef struct {
+  /** Tagged pointer to the next packet in an intrusive stack */
+  uint64_t next;
+  /** Bitmask that represents payload slots with valid data */
+  uint64_t activemask;
+  /** Service ID requested by the wave */
+  uint32_t service;
+  /** Control bits.
+   *  \li \c READY flag is bit 0. Indicates packet awaiting a host response.
+   */
+  uint32_t control;
+} header_t;
+
+/** \brief Hostcall state.
+ *
+ *  Holds the state of hostcalls being requested by all kernels that
+ *  share the same hostcall state. There is usually one buffer per
+ *  device queue.
+ */
+typedef struct {
+  /** Array of 2^index_size packet headers */
+  header_t *headers;
+  /** Array of 2^index_size packet payloads */
+  payload_t *payloads;
+  /** Signal used by kernels to indicate new work */
+  signal_t doorbell;
+  /** Stack of free packets */
+  uint64_t free_stack;
+  /** Stack of ready packets */
+  uint64_t ready_stack;
+  /** Number of LSBs in the tagged pointer can index into the packet arrays */
+  uint32_t index_size;
+  /** Device ID */
+  uint32_t device_id;
+} buffer_t;
+
+enum { SIGNAL_INIT = UINT64_MAX, SIGNAL_DONE = UINT64_MAX - 1 };
+
+static uint32_t get_buffer_alignment() { return alignof(payload_t); }
+
+static uint32_t set_control_field(uint32_t control, uint8_t offset,
+                                  uint8_t width, uint32_t value) {
+  uint32_t mask = ~(((1 << width) - 1) << offset);
+  control &= mask;
+  return control | (value << offset);
+}
+
+static uint32_t reset_ready_flag(uint32_t control) {
+  return set_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                           CONTROL_WIDTH_READY_FLAG, 0);
+}
+
+static uint64_t get_ptr_index(uint64_t ptr, uint32_t index_size) {
+  return ptr & ((1UL << index_size) - 1);
+}
+
+static uintptr_t align_to(uintptr_t value, uint32_t alignment) {
+  if (value % alignment == 0)
+    return value;
+  return value - (value % alignment) + alignment;
+}
+
+static uintptr_t get_header_start() {
+  return align_to(sizeof(buffer_t), alignof(header_t));
+}
+
+static uintptr_t get_payload_start(uint32_t num_packets) {
+  auto header_start = get_header_start();
+  auto header_end = header_start + sizeof(header_t) * num_packets;
+  return align_to(header_end, alignof(payload_t));
+}
+
+static size_t get_buffer_size(uint32_t num_packets) {
+  size_t buffer_size = get_payload_start(num_packets);
+  buffer_size += num_packets * sizeof(payload_t);
+  return buffer_size;
+}
+static uint64_t grab_ready_stack(buffer_t *buffer) {
+  return __atomic_exchange_n(&buffer->ready_stack, 0,
+                             std::memory_order_acquire);
+}
+static header_t *get_header(buffer_t *buffer, ulong ptr) {
+  return buffer->headers + get_ptr_index(ptr, buffer->index_size);
+}
+static payload_t *get_payload(buffer_t *buffer, ulong ptr) {
+  return buffer->payloads + get_ptr_index(ptr, buffer->index_size);
+}
+
+static signal_t create_signal() {
+  hsa_signal_t hs;
+  hsa_status_t status = hsa_signal_create(SIGNAL_INIT, 0, NULL, &hs);
+  if (status != HSA_STATUS_SUCCESS)
+    return {0};
+  return {hs.handle};
+}
+
+static hsa_amd_memory_pool_t static_host_memory_pool;
+static hsa_amd_memory_pool_t static_device_memory_pools[8];
+static hsa_agent_t static_hsa_agents[8];
+
+void save_hsa_statics(uint32_t device_id,
+                hsa_amd_memory_pool_t HostMemoryPool,
+                hsa_amd_memory_pool_t DevMemoryPool,
+                hsa_agent_t hsa_agent){
+  static_host_memory_pool = HostMemoryPool;
+  static_device_memory_pools[device_id] = DevMemoryPool;
+  static_hsa_agents[device_id] = hsa_agent;
+}
+
+// ====== START of helper functions for execute_service ======
+service_rc host_device_mem_free(void *ptr) { 
+  hsa_status_t err = hsa_amd_memory_pool_free(ptr);
+  if( err == HSA_STATUS_SUCCESS ) 
+     return _RC_SUCCESS;
+  else 
+     return _RC_ERROR_MEMFREE;
+}
+
+service_rc host_malloc(void **ptr, size_t size, uint32_t devid) {
+  hsa_amd_memory_pool_t MemoryPool = static_host_memory_pool; 
+  hsa_agent_t agent = static_hsa_agents[devid];
+  hsa_status_t err = hsa_amd_memory_pool_allocate(MemoryPool, size, 0, ptr);
+  if(err == HSA_STATUS_SUCCESS)
+    err = hsa_amd_agents_allow_access(1, &agent, NULL, *ptr);
+  if(err != HSA_STATUS_SUCCESS ) 
+    thread_abort((int) err);
+  return _RC_SUCCESS;
+}
+
+service_rc device_malloc(void **mem, size_t size, uint32_t devid){
+  hsa_amd_memory_pool_t MemoryPool = static_device_memory_pools[devid];
+  hsa_status_t err = hsa_amd_memory_pool_allocate(MemoryPool, size, 0, mem);
+  if(err != HSA_STATUS_SUCCESS ) 
+    thread_abort((int) err);
+  return _RC_SUCCESS;
+}
+
+void thread_abort(int rc) {
+  printf("thread_abort called with code %d\n", rc);
+  abort();
+}
+// ====== END helper functions for execute_service ======
+
+
+/** \brief Locked reference to critical data.
+ *
+ *         Simpler version of the LockedAccessor in HIP sources.
+ *
+ *         Protects access to the member _data with a lock acquired on
+ *         contruction/destruction. T must contain a _mutex field
+ *         which meets the BasicLockable requirements (lock/unlock)
+ */
+template <typename T> struct locked_accessor_t {
+  locked_accessor_t(T &criticalData) : _criticalData(&criticalData) {
+    _criticalData->_mutex.lock();
+  };
+  ~locked_accessor_t() { _criticalData->_mutex.unlock(); }
+  // Syntactic sugar so -> can be used to get the underlying type.
+  T *operator->() { return _criticalData; };
+private:
+  T *_criticalData;
+};
+struct record_t {
+  bool discarded;
+};
+struct critical_data_t {
+  std::unordered_map<buffer_t *, record_t> buffers;
+  std::mutex _mutex;
+};
+typedef locked_accessor_t<critical_data_t> locked_critical_data_t;
+
+typedef struct {
+  hsa_queue_t *hsa_q;
+  buffer_t *hcb;
+  uint32_t devid;
+} hsaq_buf_entry_t;
+
+extern "C" void handler_SERVICE_SANITIZER(payload_t *packt_payload,
+  uint64_t activemask, uint32_t gpu_device, UriLocator *uri_locator);
+
+static bool static_version_was_checked = false;
+struct consumer_t {
+private:
+  signal_t doorbell;
+  std::thread thread;
+  critical_data_t critical_data;
+  UriLocator *urilocator;
+  consumer_t(signal_t _doorbell) : doorbell(_doorbell) {}
+  // Table of hsa_q's and their associated buffer_t's 
+  std::list<hsaq_buf_entry_t*> hsaq_bufs;
+public:
+  static consumer_t * create_consumer();
+
+hsaq_buf_entry_t * add_hsaq_buf_entry(buffer_t *hcb,
+	       	hsa_queue_t *hsa_q, uint32_t devid) {
+  hsaq_buf_entry_t * new_hsaq_buf = new hsaq_buf_entry_t;
+  new_hsaq_buf->hcb = hcb;
+  new_hsaq_buf->devid = devid;
+  new_hsaq_buf->hsa_q = hsa_q;
+  hsaq_bufs.push_back(new_hsaq_buf);
+  return new_hsaq_buf; 
+}
+
+hsaq_buf_entry_t * find_hsaq_buf_entry(hsa_queue_t *hsa_q) {
+  for(auto hsaq_buf : hsaq_bufs) {
+    if (hsaq_buf->hsa_q == hsa_q)
+      return hsaq_buf;
+  }
+  return NULL;
+}
+
+consumer_error_t check_version(uint device_vrm) const {
+  if (device_vrm == (unsigned int)HOSTRPC_VRM)
+    return CONSUMER_SUCCESS;
+  uint device_version_release = device_vrm >> 6;
+  if (device_version_release != HOSTRPC_VERSION_RELEASE) {
+    printf("ERROR Incompatible device and host release\n      Device "
+           "release(%d)\n      Host release(%d)\n",
+           device_version_release, HOSTRPC_VERSION_RELEASE);
+    return CONSUMER_ERROR_WRONGVERSION;
+  }
+  if (device_vrm > HOSTRPC_VRM) {
+    printf("ERROR Incompatible device and host version \n       Device "
+           "version(%d)\n      Host version(%d)\n",
+           device_vrm, HOSTRPC_VERSION_RELEASE);
+    printf("          Upgrade libomptarget runtime on your system.\n");
+    return CONSUMER_ERROR_OLDHOSTVERSIONMOD;
+  }
+  if (device_vrm < HOSTRPC_VRM) {
+    unsigned int host_ver = ((unsigned int)HOSTRPC_VRM) >> 12;
+    unsigned int host_rel = (((unsigned int)HOSTRPC_VRM) << 20) >> 26;
+    unsigned int host_mod = (((unsigned int)HOSTRPC_VRM) << 26) >> 26;
+    unsigned int dev_ver = ((unsigned int)device_vrm) >> 12;
+    unsigned int dev_rel = (((unsigned int)device_vrm) << 20) >> 26;
+    unsigned int dev_mod = (((unsigned int)device_vrm) << 26) >> 26;
+    printf("WARNING:  Device mod version < host mod version \n          Device "
+           "version: %d.%d.%d\n          Host version:   %d.%d.%d\n",
+           dev_ver, dev_rel, dev_mod, host_ver, host_rel, host_mod);
+    printf("          Consider rebuild binary with more recent compiler.\n");
+  }
+  return CONSUMER_SUCCESS;
+}
+
+void process_packets(buffer_t *buffer,
+                                              uint64_t ready_stack) const {
+  // This function is always called from consume_packets, which owns
+  // the lock for the critical data.
+
+  // Each wave can submit at most one packet at a time, and all
+  // waves independently push ready packets. The stack of packets at
+  // this point cannot contain multiple packets from the same wave,
+  // so consuming ready packets in a latest-first order does not
+  // affect any wave.
+  for (decltype(ready_stack) iter = ready_stack, next = 0; iter; iter = next) {
+
+    // Remember the next packet pointer. The current packet will
+    // get reused from the free stack after we process it.
+    auto header = get_header(buffer, iter);
+    next = header->next;
+
+    auto payload = get_payload(buffer, iter);
+    uint64_t activemask = header->activemask;
+
+    // split the 32-bit service number into service_id and VRM to be checked
+    // if device hostrpc or stubs are ahead of this host runtime.
+    uint service_id = (header->service << 16) >> 16;
+    if (!static_version_was_checked) {
+      uint device_vrm = ((uint)(header->service) >> 16);
+      consumer_error_t  err = check_version(device_vrm);
+      if (err != CONSUMER_SUCCESS)
+        thread_abort((int) err);
+      static_version_was_checked = true;
+    }
+
+    if (service_id == HOSTRPC_SERVICE_SANITIZER) {
+      handler_SERVICE_SANITIZER(payload, activemask, buffer->device_id, urilocator);
+    } else {
+      // Serialize calls to execute_service for each active lane
+      // TODO: One could use ffs to skip inactive lanes faster.
+      for (uint32_t wi = 0; wi != 64; ++wi) {
+        uint64_t flag = activemask & ((uint64_t)1 << wi);
+        if (flag == 0)
+          continue;
+        execute_service(service_id, buffer->device_id, payload->slots[wi]);
+      }
+    }
+    __atomic_store_n(&header->control, reset_ready_flag(header->control),
+                     std::memory_order_release);
+  }
+}
+
+// FIXME: This cannot be const because it locks critical data.
+// A lock-free implementaiton might make that possible.
+void consume_packets() {
+  /* TODO: The consumer iterates over all registered buffers in an
+     unspecified order, and for each buffer, processes packets also
+     in an unspecified order. This may need a more efficient
+     strategy based on the turnaround time for the services
+     requested by all these packets.
+   */
+  uint64_t signal_value = SIGNAL_INIT;
+  uint64_t timeout = 1024 * 1024;
+
+  while (true) {
+    hsa_signal_t hs{doorbell.handle};
+    signal_value =
+        hsa_signal_wait_scacquire(hs, HSA_SIGNAL_CONDITION_NE, signal_value,
+                                timeout, HSA_WAIT_STATE_BLOCKED);
+    if (signal_value == SIGNAL_DONE) {
+      return;
+    }
+
+    locked_critical_data_t data(critical_data);
+
+    for (auto ii = data->buffers.begin(), ie = data->buffers.end(); ii != ie;
+         /* don't increment here */) {
+      auto record = ii->second;
+      if (record.discarded) {
+        ii = data->buffers.erase(ii);
+        continue;
+      }
+
+      buffer_t *buffer = ii->first;
+      uint64_t F = grab_ready_stack(buffer);
+      if (F)
+        process_packets(buffer, F);
+      ++ii;
+    }
+  }
+  return;
+}
+
+consumer_error_t launch_service_thread() {
+  if (thread.joinable())
+    return CONSUMER_ERROR_CONSUMER_ACTIVE;
+  thread = std::thread(&consumer_t::consume_packets, this);
+  if (!thread.joinable())
+    return CONSUMER_ERROR_CONSUMER_LAUNCH_FAILED;
+  return CONSUMER_SUCCESS;
+}
+
+consumer_error_t terminate_service_thread() {
+  if (!thread.joinable())
+    return CONSUMER_ERROR_CONSUMER_INACTIVE;
+  hsa_signal_t signal = {doorbell.handle};
+  hsa_signal_store_screlease(signal, SIGNAL_DONE);
+  thread.join();
+  return CONSUMER_SUCCESS;
+}
+
+void register_buffer(void *b) {
+  locked_critical_data_t data(critical_data);
+  auto buffer = reinterpret_cast<buffer_t *>(b);
+  auto &record = data->buffers[buffer];
+  record.discarded = false;
+  buffer->doorbell = doorbell;
+  urilocator = new UriLocator();
+}
+
+consumer_error_t deregister_buffer(void *b) {
+  locked_critical_data_t data(critical_data);
+  auto buffer = reinterpret_cast<buffer_t *>(b);
+  if (data->buffers.count(buffer) == 0)
+    return CONSUMER_ERROR_INVALID_REQUEST;
+  auto &record = data->buffers[buffer];
+  if (record.discarded)
+    return CONSUMER_ERROR_INVALID_REQUEST;
+  record.discarded = true;
+  return CONSUMER_SUCCESS;
+}
+
+// destructor triggered by delete static_consumer_ptr in hostrpc_terminate().
+~consumer_t() {
+  for(auto hsaq_buf : hsaq_bufs) {
+    if (hsaq_buf) {
+      deregister_buffer(hsaq_buf->hcb);
+      delete hsaq_buf;
+    }
+  }
+  hsaq_bufs.clear();
+  terminate_service_thread();
+  delete urilocator;
+  critical_data.buffers.clear();
+  hsa_signal_t hs{doorbell.handle};
+  hsa_signal_destroy(hs);
+}
+
+buffer_t * create_buffer_t(uint32_t num_packets, uint32_t devid) {
+  if (num_packets == 0) {
+    printf("num_packets cannot be zero \n");
+    abort();
+  }
+  size_t size = get_buffer_size(num_packets);
+  uint32_t align = get_buffer_alignment();
+  void *newbuffer = NULL;
+  service_rc err = host_malloc(&newbuffer, size + align, devid);
+  if (!newbuffer || (err != _RC_SUCCESS )) {
+    printf("call to host_malloc failed \n");
+    abort();
+  }
+
+  if ((uintptr_t)newbuffer % get_buffer_alignment() != 0) {
+    printf("ERROR: incorrect alignment \n");
+    abort();
+  }
+
+  //  Initialize the buffer_t 
+  buffer_t *hb = (buffer_t *) newbuffer;
+
+  hb->headers = (header_t *)((uint8_t *)hb + get_header_start());
+  hb->payloads = (payload_t *)((uint8_t *)hb + get_payload_start(num_packets));
+
+  uint32_t index_size = 1;
+  if (num_packets > 2)
+    index_size = 32 - __builtin_clz(num_packets);
+  hb->index_size = index_size;
+  hb->headers[0].next = 0;
+
+  uint64_t next = 1UL << index_size;
+  for (uint32_t ii = 1; ii != num_packets; ++ii) {
+    hb->headers[ii].next = next;
+    next = ii;
+  }
+  hb->free_stack = next;
+  hb->ready_stack = 0;
+  hb->device_id = devid;
+  return hb;
+}
+
+}; // end of class/struct consumer_t
+
+consumer_t * consumer_t::create_consumer() {
+  signal_t doorbell = create_signal();
+  if (doorbell.handle == 0) {
+    return nullptr;
+  }
+  return new consumer_t(doorbell);
+}
+
+// Currently, a single instance of consumer_t is created and saved statically.
+// This instance starts a single service thread for ALL devices. 
+static consumer_t *static_consumer_ptr = NULL;
+
+// This is the main hostrpc function called by the amdgpu plugin when
+// launching a kernel on a designated hsa_queue_t. This function should only
+// be called if any kernel in the device image requires hostrpc services. 
+extern "C" unsigned long 
+hostrpc_assign_buffer(hsa_agent_t agent, hsa_queue_t *this_Q,
+                   uint32_t device_id, hsa_amd_memory_pool_t HostMemoryPool,
+                   hsa_amd_memory_pool_t DevMemoryPool){
+  // Create and launch the services thread
+  if (!static_consumer_ptr) {
+    static_consumer_ptr = consumer_t::create_consumer();
+    consumer_error_t err = static_consumer_ptr->launch_service_thread();
+    if( err != CONSUMER_SUCCESS)
+      thread_abort(err);
+  }
+
+  // quick return to kernel launch if this hsa q is being reused
+  hsaq_buf_entry_t *hsaq_buf = static_consumer_ptr->find_hsaq_buf_entry(this_Q);
+  if (hsaq_buf)
+    return (unsigned long) hsaq_buf->hcb;
+
+  // Helper functions for execute_service need these hsa values saved
+  save_hsa_statics(device_id, HostMemoryPool, DevMemoryPool, agent);
+
+  // Get values needed to determine buffer size 
+  uint32_t numCu;
+  hsa_agent_get_info(agent,
+      (hsa_agent_info_t)HSA_AMD_AGENT_INFO_COMPUTE_UNIT_COUNT, &numCu);
+  // ErrorCheck(Could not get number of cus, err);
+  uint32_t waverPerCu;
+  hsa_agent_get_info(agent,
+    (hsa_agent_info_t)HSA_AMD_AGENT_INFO_MAX_WAVES_PER_CU, &waverPerCu);
+  // ErrorCheck(Could not get number of waves per cu, err);
+  unsigned int minpackets = numCu * waverPerCu;
+
+  //  Create and initialize the new buffer to return to kernel launch
+  buffer_t *hcb = static_consumer_ptr->create_buffer_t(minpackets, device_id);
+
+  // Register the buffer for the consumer thread
+  static_consumer_ptr->register_buffer(hcb);
+
+  // Cache in hsaq_bufs for reuse 
+  hsaq_buf = static_consumer_ptr->add_hsaq_buf_entry(hcb, this_Q, device_id);
+  return (unsigned long) hcb;
+}
+
+extern "C" hsa_status_t hostrpc_terminate() {
+  if (static_consumer_ptr) {
+    // The consumer_t destructor takes care of all memory returns
+    delete static_consumer_ptr;
+    static_consumer_ptr = NULL;
+  }
+  return HSA_STATUS_SUCCESS;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/services/amdgcn_urilocator.cpp llvm-project/openmp/libomptarget/hostrpc/services/amdgcn_urilocator.cpp
--- llvm-project.upstream/openmp/libomptarget/hostrpc/services/amdgcn_urilocator.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/services/amdgcn_urilocator.cpp	2023-03-02 13:25:13.095430543 -0500
@@ -0,0 +1,226 @@
+/*
+//===---- UriLocator.cpp: Extract URI path using vendor specific HSA-API
+calls.----------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===------------------------------------------------------------------------------------===//
+
+Copyright (c) 2021 - 2021 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+ */
+
+#include "urilocator.h"
+#include <cstdlib>
+#include <fcntl.h>
+#include <sstream>
+#include <sys/stat.h>
+#include <unistd.h>
+
+static bool GetFileHandle(const char *fname, int *fd_ptr, size_t *sz_ptr) {
+  if ((fd_ptr == nullptr) || (sz_ptr == nullptr)) {
+    return false;
+  }
+
+  // open system function call, return false on fail
+  struct stat stat_buf;
+  *fd_ptr = open(fname, O_RDONLY);
+  if (*fd_ptr < 0) {
+    return false;
+  }
+
+  // Retrieve stat info and size
+  if (fstat(*fd_ptr, &stat_buf) != 0) {
+    close(*fd_ptr);
+    return false;
+  }
+
+  *sz_ptr = stat_buf.st_size;
+
+  return true;
+}
+
+hsa_status_t UriLocator::createUriRangeTable() {
+
+  auto execCb = [](hsa_executable_t exec, void *data) -> hsa_status_t {
+    int execState = 0;
+    hsa_status_t status;
+    status =
+        hsa_executable_get_info(exec, HSA_EXECUTABLE_INFO_STATE, &execState);
+    if (status != HSA_STATUS_SUCCESS)
+      return status;
+    if (execState != HSA_EXECUTABLE_STATE_FROZEN)
+      return status;
+
+    auto loadedCodeObjectCb = [](hsa_executable_t exec,
+                                 hsa_loaded_code_object_t lcobj,
+                                 void *data) -> hsa_status_t {
+      hsa_status_t result;
+      uint64_t loadBAddr = 0, loadSize = 0;
+      uint32_t uriLen = 0;
+      int64_t delta = 0;
+      uint64_t *argsCb = static_cast<uint64_t *>(data);
+      hsa_ven_amd_loader_1_03_pfn_t *fnTab =
+          reinterpret_cast<hsa_ven_amd_loader_1_03_pfn_t *>(argsCb[0]);
+      std::vector<UriRange> *rangeTab =
+          reinterpret_cast<std::vector<UriRange> *>(argsCb[1]);
+
+      if (!fnTab->hsa_ven_amd_loader_loaded_code_object_get_info)
+        return HSA_STATUS_ERROR;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_BASE,
+          (void *)&loadBAddr);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_SIZE,
+          (void *)&loadSize);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_URI_LENGTH,
+          (void *)&uriLen);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_DELTA,
+          (void *)&delta);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      char *uri = new char[uriLen + 1];
+      uri[uriLen] = '\0';
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_URI, (void *)uri);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      rangeTab->push_back(UriRange{loadBAddr, loadBAddr + loadSize - 1, delta,
+                                   std::string{uri, uriLen + 1}});
+      delete[] uri;
+      return HSA_STATUS_SUCCESS;
+    };
+
+    uint64_t *args = static_cast<uint64_t *>(data);
+    hsa_ven_amd_loader_1_03_pfn_t *fnExtTab =
+        reinterpret_cast<hsa_ven_amd_loader_1_03_pfn_t *>(args[0]);
+    return fnExtTab->hsa_ven_amd_loader_executable_iterate_loaded_code_objects(
+        exec, loadedCodeObjectCb, data);
+  };
+
+  if (!fn_table_.hsa_ven_amd_loader_iterate_executables)
+    return HSA_STATUS_ERROR;
+
+  uint64_t callbackArgs[2] = {(uint64_t)&fn_table_, (uint64_t)&rangeTab_};
+  return fn_table_.hsa_ven_amd_loader_iterate_executables(execCb,
+                                                          (void *)callbackArgs);
+}
+
+// Encoding of uniform-resource-identifier(URI) is detailed in
+// https://llvm.org/docs/AMDGPUUsage.html#loaded-code-object-path-uniform-resource-identifier-uri
+// The below code currently extracts the uri of loaded code object using
+// file-uri.
+std::pair<uint64_t, uint64_t> UriLocator::decodeUriAndGetFd(UriInfo &uri,
+                                                            int *uri_fd) {
+
+  std::ostringstream ss;
+  char cur;
+  uint64_t offset = 0, size = 0;
+  if (uri.uriPath.size() == 0)
+    return {0, 0};
+  auto pos = uri.uriPath.find("//");
+  if (pos == std::string::npos || uri.uriPath.substr(0, pos) != "file:") {
+    uri.uriPath = "";
+    return {0, 0};
+  }
+  auto rspos = uri.uriPath.find('#');
+  if (rspos != std::string::npos) {
+    // parse range specifier
+    std::string offprefix = "offset=", sizeprefix = "size=";
+    auto sbeg = uri.uriPath.find('&', rspos);
+    auto offbeg = rspos + offprefix.size() + 1;
+    std::string offstr = uri.uriPath.substr(offbeg, sbeg - offbeg);
+    auto sizebeg = sbeg + sizeprefix.size() + 1;
+    std::string sizestr =
+        uri.uriPath.substr(sizebeg, uri.uriPath.size() - sizebeg);
+    offset = std::stoull(offstr, nullptr, 0);
+    size = std::stoull(sizestr, nullptr, 0);
+    rspos -= 1;
+  } else {
+    rspos = uri.uriPath.size() - 1;
+  }
+  pos += 2;
+  // decode filepath
+  for (auto i = pos; i <= rspos;) {
+    cur = uri.uriPath[i];
+    if (isalnum(cur) || cur == '/' || cur == '-' || cur == '_' || cur == '.' ||
+        cur == '~') {
+      ss << cur;
+      i++;
+    } else {
+      // characters prefix with '%' char
+      char tbits = uri.uriPath[i + 1], lbits = uri.uriPath[i + 2];
+      uint8_t t = (tbits < 58) ? (tbits - 48) : ((tbits - 65) + 10);
+      uint8_t l = (lbits < 58) ? (lbits - 48) : ((lbits - 65) + 10);
+      ss << (char)(((0b00000000 | t) << 4) | l);
+      i += 3;
+    }
+  }
+  uri.uriPath = ss.str();
+  size_t fd_size;
+  GetFileHandle(uri.uriPath.c_str(), uri_fd, &fd_size);
+  // As per URI locator syntax, range_specifier is optional
+  // if range_specifier is absent return total size of the file
+  // and set offset to begin at 0.
+  if (size == 0)
+    size = fd_size;
+  return {offset, size};
+}
+
+UriLocator::UriInfo UriLocator::lookUpUri(uint64_t device_pc) {
+  UriInfo errorstate{"", 0};
+
+  if (!init_) {
+
+    hsa_status_t result;
+    result = hsa_system_get_major_extension_table(
+        HSA_EXTENSION_AMD_LOADER, 1, sizeof(fn_table_), &fn_table_);
+    if (result != HSA_STATUS_SUCCESS)
+      return errorstate;
+    result = createUriRangeTable();
+    if (result != HSA_STATUS_SUCCESS) {
+      rangeTab_.clear();
+      return errorstate;
+    }
+    init_ = true;
+  }
+
+  for (auto &seg : rangeTab_)
+    if (seg.startAddr_ <= device_pc && device_pc <= seg.endAddr_)
+      return UriInfo{seg.Uri_.c_str(), seg.elfDelta_};
+
+  return errorstate;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/services/CMakeLists.txt llvm-project/openmp/libomptarget/hostrpc/services/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/hostrpc/services/CMakeLists.txt	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/services/CMakeLists.txt	2023-03-02 13:25:13.095430543 -0500
@@ -0,0 +1,44 @@
+##===----------------------------------------------------------------------===##
+#
+# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+#
+##===----------------------------------------------------------------------===##
+#
+# Build libhostrpc_services.a library for plugins to link to
+#
+##===----------------------------------------------------------------------===##
+#find_package(hsa-runtime64 1.2.0 HINTS ${CMAKE_INSTALL_PREFIX} PATHS /opt/rocm/hsa)
+if(HOSTRPC_BUILD_AMD)
+   add_library(amdgcn_hostrpc_services STATIC amdgcn_hostrpc.cpp execute_service.cpp devsanitizer.cpp amdgcn_urilocator.cpp )
+   target_include_directories(
+      amdgcn_hostrpc_services
+      PRIVATE
+      ${LIBOMPTARGET_INCLUDE_DIR}
+   )
+   target_include_directories(
+      amdgcn_hostrpc_services
+      PUBLIC
+      ${hsa-runtime64_DIR}/../../../include)
+   target_link_libraries(amdgcn_hostrpc_services hsa-runtime64::hsa-runtime64)
+   if(SANITIZER_AMDGPU)
+      add_definitions(-DSANITIZER_AMDGPU=1)
+      set(ASAN_LIB ${LLVM_LIBRARY_DIR}/clang/${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}/lib/linux/libclang_rt.asan-x86_64.so)
+      target_link_libraries(amdgcn_hostrpc_services ${ASAN_LIB})
+   endif()
+   set_property(TARGET amdgcn_hostrpc_services PROPERTY POSITION_INDEPENDENT_CODE ON)
+endif()
+
+if(HOSTRPC_BUILD_NVPTX)
+   add_library(nvptx_hostrpc_services STATIC execute_service.cpp)
+   target_include_directories(
+      nvptx_hostrpc_services
+      PRIVATE
+      ${LIBOMPTARGET_INCLUDE_DIR}
+   )
+   set_property(TARGET nvptx_hostrpc_services PROPERTY POSITION_INDEPENDENT_CODE ON)
+endif()
+
+
+
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/services/devsanitizer.cpp llvm-project/openmp/libomptarget/hostrpc/services/devsanitizer.cpp
--- llvm-project.upstream/openmp/libomptarget/hostrpc/services/devsanitizer.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/services/devsanitizer.cpp	2023-03-02 13:25:13.095430543 -0500
@@ -0,0 +1,144 @@
+/*
+//===---- UriLocator.cpp: Definition of handler for Sanitizer Service. ----===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+Copyright (c) 2021 - 2021 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+ */
+
+#include "execute_service.h"
+#include "urilocator.h"
+#include <algorithm>
+#include <assert.h>   //to exp
+#include <inttypes.h> //to exp
+#include <string>
+#include <tuple>
+#include <vector>
+
+// Address sanitizer runtime entry-function to report the invalid device memory
+// access this will be defined in llvm-project/compiler-rt/lib/asan, and will
+// have effect only when compiler-rt is build for AMDGPU. Note: This API is
+// runtime interface of asan library and only defined for linux os.
+extern "C" void __asan_report_nonself_error(
+    uint64_t *callstack, uint32_t n_callstack, uint64_t *addr, uint32_t naddr,
+    uint64_t *entity_ids, uint32_t n_entities, bool is_write,
+    uint32_t access_size, bool is_abort, const char *name, int64_t vma_adjust,
+    int fd, uint64_t file_extent_size, uint64_t file_extent_start = 0);
+
+namespace {
+extern "C" void handler_SERVICE_SANITIZER(payload_t *packt_payload,
+                                                  uint64_t activemask,
+                                                  uint32_t gpu_device,
+                                                  UriLocator *uri_locator) {
+  // An address results in invalid access in each active lane
+  uint64_t device_failing_addresses[64];
+  // An array of identifications of entities requesting a report.
+  // index 0       - contains device id
+  // index 1,2,3   - contains wg_idx, wg_idy, wg_idz respectively.
+  // index 4 to 67 - contains reporting wave ids in a wave-front.
+  uint64_t entity_id[68], callstack[1];
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  uint32_t n_activelanes = __builtin_popcountl(activemask);
+  uint64_t access_info = 0, access_size = 0;
+  bool is_abort = true;
+#endif
+#endif
+  entity_id[0] = gpu_device;
+
+  assert(packt_payload != nullptr && "packet payload is null?");
+
+  int indx = 0, en_idx = 1;
+  bool first_workitem = false;
+  for (uint32_t wi = 0; wi != 64; ++wi) {
+    uint64_t flag = activemask & ((uint64_t)1 << wi);
+    if (flag == 0)
+      continue;
+
+    auto data_slot = packt_payload->slots[wi];
+    // encoding of packet payload arguments is
+    // defined in device-libs/asanrtl/src/report.cl
+    if (!first_workitem) {
+      device_failing_addresses[indx] = data_slot[0];
+      callstack[0] = data_slot[1];
+      entity_id[en_idx] = data_slot[2];
+      entity_id[++en_idx] = data_slot[3];
+      entity_id[++en_idx] = data_slot[4];
+      entity_id[++en_idx] = data_slot[5];
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+      access_info = data_slot[6];
+      access_size = data_slot[7];
+#endif
+#endif
+      first_workitem = true;
+    } else {
+      device_failing_addresses[indx] = data_slot[0];
+      entity_id[en_idx] = data_slot[5];
+    }
+    indx++;
+    en_idx++;
+  }
+
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  bool is_write = false;
+  if (access_info & 0xFFFFFFFF00000000)
+    is_abort = false;
+  if (access_info & 1)
+    is_write = true;
+#endif
+#endif
+
+  std::string fileuri;
+  uint64_t size = 0, offset = 0;
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  int64_t loadAddrAdjust = 0;
+#endif
+#endif
+  int uri_fd = -1;
+
+  if (uri_locator) {
+    UriLocator::UriInfo fileuri_info = uri_locator->lookUpUri(callstack[0]);
+    std::tie(offset, size) =
+        uri_locator->decodeUriAndGetFd(fileuri_info, &uri_fd);
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+    loadAddrAdjust = fileuri_info.loadAddressDiff;
+#endif
+#endif
+  }
+
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  __asan_report_nonself_error(
+      callstack, 1, device_failing_addresses, n_activelanes, entity_id,
+      n_activelanes + 4, is_write, access_size, is_abort,
+      /*thread key*/ "amdgpu", loadAddrAdjust, uri_fd, size, offset);
+#endif
+#endif
+}
+} // end anonymous namespace
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/services/execute_service.cpp llvm-project/openmp/libomptarget/hostrpc/services/execute_service.cpp
--- llvm-project.upstream/openmp/libomptarget/hostrpc/services/execute_service.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/services/execute_service.cpp	2023-03-02 21:21:30.261789776 -0500
@@ -0,0 +1,947 @@
+
+/*
+ *  execute_service.cpp:  hostrpc services
+ *
+MIT License
+
+Copyright © 2020 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is furnished
+to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+*/
+
+// #include <hsa/hsa.h>
+// #include <hsa/hsa_ext_amd.h>
+#include "../src/hostrpc_internal.h"
+#include "execute_service.h"
+#include <ctype.h>
+#include <stdarg.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <assert.h>
+#include <cstring>
+#include <tuple>
+
+// MAXVARGS applies to non-printf vargs functions only.
+#define MAXVARGS 32
+// NUMFPREGS and FPREGSZ are part of x86 vargs ABI that
+// is recreated with the printf support.
+#define NUMFPREGS 8
+#define FPREGSZ 16
+
+typedef int uint128_t __attribute__((mode(TI)));
+struct hostrpc_pfIntRegs {
+  uint64_t rdi, rsi, rdx, rcx, r8, r9;
+};
+typedef struct hostrpc_pfIntRegs hostrpc_pfIntRegs_t; // size = 48 bytes
+
+struct hostrpc_pfRegSaveArea {
+  hostrpc_pfIntRegs_t iregs;
+  uint128_t freg[NUMFPREGS];
+};
+typedef struct hostrpc_pfRegSaveArea
+    hostrpc_pfRegSaveArea_t; // size = 304 bytes
+
+struct hostrpc_ValistExt {
+  uint32_t gp_offset;      /* offset to next available gpr in reg_save_area */
+  uint32_t fp_offset;      /* offset to next available fpr in reg_save_area */
+  void *overflow_arg_area; /* args that are passed on the stack */
+  hostrpc_pfRegSaveArea_t *reg_save_area; /* int and fp registers */
+  size_t overflow_size;
+} __attribute__((packed));
+typedef struct hostrpc_ValistExt hostrpc_ValistExt_t;
+
+/// Prototype for host fallback functions
+typedef uint32_t hostrpc_varfn_uint_t(void *, ...);
+typedef uint64_t hostrpc_varfn_uint64_t(void *, ...);
+typedef double   hostrpc_varfn_double_t(void *, ...);
+
+static service_rc hostrpc_printf(char *buf, size_t bufsz, uint32_t *rc);
+static service_rc hostrpc_fprintf(char *buf, size_t bufsz, uint32_t *rc);
+
+template <typename T, typename FT>
+static service_rc hostrpc_varfn(char *buf, size_t bufsz, T *rc);
+
+static void handler_SERVICE_PRINTF(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  uint uint_value;
+  service_rc rc = hostrpc_printf(device_buffer, bufsz, &uint_value);
+  payload[0] = (uint64_t)uint_value; // what the printf returns
+  payload[1] = (uint64_t)rc;         // Any errors in the service function
+  service_rc rcmem = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)rcmem;
+}
+static void handler_SERVICE_FPRINTF(uint32_t device_id,
+                                            uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  uint uint_value;
+  service_rc rc = hostrpc_fprintf(device_buffer, bufsz, &uint_value);
+  payload[0] = (uint64_t)uint_value; // what the printf returns
+  payload[1] = (uint64_t)rc;         // Any errors in the service function
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)err;
+}
+
+template<typename T, typename TF>
+static void handler_SERVICE_VARFN(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  T return_value;
+  service_rc rc = hostrpc_varfn<T, TF>
+	  (device_buffer, bufsz, &return_value);
+  payload[0] = (uint64_t)return_value; // What the vargs function pointer returns
+  payload[1] = (uint64_t)rc;         // any errors in the service function
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)err;
+}
+
+static void handler_SERVICE_MALLOC_PRINTF(uint32_t device_id,
+                 uint64_t *payload) {
+  void *ptr = NULL;
+  // CPU device ID 0 is the fine grain memory
+  size_t sz = (size_t)payload[0];
+  service_rc err = host_malloc(&ptr, sz, device_id);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+
+//  SERVICE_MALLOC & SERVICE_FREE are for allocating a heap of device memory 
+//  only used by the device to be used for device side malloc and free. 
+//  This is called by __ockl_devmem_request. For allocating memory visible 
+//  to both host and device user SERVICE_MALLOC_PRINTF. The corresponding
+//  vargs function will release this  
+static void handler_SERVICE_MALLOC(uint32_t device_id, uint64_t *payload) {
+  void *ptr = NULL;
+  size_t sz = (size_t)payload[0];
+  service_rc err = device_malloc(&ptr, sz, device_id);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+
+#if 0
+void fort_ptr_assign_i8(void *arg0, void *arg1, void *arg2, void *arg3, void *arg4) {
+  printf("\n\n ERROR: hostrpc service FTNASSIGN is not functional\n\n");
+};
+service_rc FtnAssignWrapper(void *arg0, void *arg1, void *arg2, void *arg3, void *arg4) {
+  fort_ptr_assign_i8(arg0, arg1, arg2, arg3, arg4);
+  return HSA_STATUS_SUCCESS;
+}
+
+service_rc ftn_assign_wrapper(void *arg0, void *arg1, void *arg2, void *arg3,
+                                void *arg4) {
+  return FtnAssignWrapper(arg0, arg1, arg2, arg3, arg4);
+}
+
+static void handler_SERVICE_FTNASSIGN(uint32_t device_id,
+                                              uint64_t *payload) {
+  void *ptr = NULL;
+  service_rc err = ftn_assign_wrapper((void *)payload[0], (void *)payload[1],
+                                        (void *)payload[2], (void *)payload[3],
+                                        (void *)payload[4]);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+#endif
+
+static void handler_SERVICE_FREE(uint32_t device_id,
+                                         uint64_t *payload) {
+  char *device_buffer = (char *)payload[0];
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[0] = (uint64_t)err;
+}
+
+static void handler_SERVICE_FUNCTIONCALL(uint32_t device_id,
+                                                 void (*payload)())  {
+  void (*fptr)() = payload;
+  (*fptr)();
+}
+
+// The consumer thread will serialize each active lane and 
+// call execute_service for each service request. 
+// execute_service functions are architecturally independent.
+void execute_service(uint32_t service_id, uint32_t device_id,
+                                    uint64_t *payload) {
+  switch (service_id) {
+  case HOSTRPC_SERVICE_PRINTF:
+    handler_SERVICE_PRINTF(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_FPRINTF:
+    handler_SERVICE_FPRINTF(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_VARFNUINT:
+    handler_SERVICE_VARFN<uint, hostrpc_varfn_uint_t>(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_VARFNUINT64:
+    handler_SERVICE_VARFN<uint64_t, hostrpc_varfn_uint64_t>(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_VARFNDOUBLE:
+    handler_SERVICE_VARFN<double, hostrpc_varfn_double_t>(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_VARFNINT:
+    handler_SERVICE_VARFN<int, hostrpc_varfn_int_t>(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_VARFNLONG:
+    handler_SERVICE_VARFN<long, hostrpc_varfn_long_t>(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_VARFNFLOAT:
+    handler_SERVICE_VARFN<float, hostrpc_varfn_float_t>(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_MALLOC_PRINTF:
+    handler_SERVICE_MALLOC_PRINTF(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_MALLOC:
+    handler_SERVICE_MALLOC(device_id, payload);
+    break;
+//  case HOSTRPC_SERVICE_FTNASSIGN:
+//    handler_SERVICE_FTNASSIGN(device_id, payload);
+//    break;
+  case HOSTRPC_SERVICE_FREE:
+    handler_SERVICE_FREE(device_id, payload);
+    break;
+  case HOSTRPC_SERVICE_FUNCTIONCALL:
+    {
+    void (*fptr)() = (void (*)()) payload[0];
+    handler_SERVICE_FUNCTIONCALL(device_id, fptr);
+    }
+    break;
+  default:
+    printf("ERROR: hostrpc got a bad service id:%d\n", service_id);
+    thread_abort((int) _RC_INVALIDSERVICE_ERROR);
+  }
+}
+
+// Support for hostrpc_printf service
+
+// Handle overflow when building the va_list for vprintf
+static service_rc hostrpc_pfGetOverflow(hostrpc_ValistExt_t *valist,
+                                              size_t needsize) {
+  if (needsize < valist->overflow_size)
+    return _RC_SUCCESS;
+
+  // Make the overflow area bigger
+  size_t stacksize;
+  void *newstack;
+  if (valist->overflow_size == 0) {
+    // Make initial save area big to reduce mallocs
+    stacksize = (FPREGSZ * NUMFPREGS) * 2;
+    if (needsize > stacksize)
+      stacksize = needsize; // maybe a big string
+  } else {
+    // Initial save area not big enough, double it
+    stacksize = valist->overflow_size * 2;
+  }
+  if (!(newstack = malloc(stacksize))) {
+    return _RC_STATUS_ERROR;
+  }
+  memset(newstack, 0, stacksize);
+  if (valist->overflow_size) {
+    memcpy(newstack, valist->overflow_arg_area, valist->overflow_size);
+    free(valist->overflow_arg_area);
+  }
+  valist->overflow_arg_area = newstack;
+  valist->overflow_size = stacksize;
+  return _RC_SUCCESS;
+}
+
+// Add an integer to the va_list for vprintf
+static service_rc hostrpc_pfAddInteger(hostrpc_ValistExt_t *valist,
+                                             char *val, size_t valsize,
+                                             size_t *stacksize) {
+  uint64_t ival;
+  switch (valsize) {
+  case 1:
+    ival = *(uint8_t *)val;
+    break;
+  case 2:
+    ival = *(uint32_t *)val;
+    break;
+  case 4:
+    ival = (*(uint32_t *)val);
+    break;
+  case 8:
+    ival = *(uint64_t *)val;
+    break;
+  default: {
+    return _RC_STATUS_ERROR;
+  }
+  }
+  //  Always copy 8 bytes, sizeof(ival)
+  if ((valist->gp_offset + sizeof(ival)) <= sizeof(hostrpc_pfIntRegs_t)) {
+    memcpy(((char *)valist->reg_save_area + valist->gp_offset), &ival,
+           sizeof(ival));
+    valist->gp_offset += sizeof(ival);
+    return _RC_SUCCESS;
+  }
+  // Ensure valist overflow area is big enough
+  size_t needsize = (size_t)*stacksize + sizeof(ival);
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  // Copy to overflow
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, &ival,
+         sizeof(ival));
+
+  *stacksize += sizeof(ival);
+  return _RC_SUCCESS;
+}
+
+// Add a String argument when building va_list for vprintf
+static service_rc hostrpc_pfAddString(hostrpc_ValistExt_t *valist,
+                                            char *val, size_t strsz,
+                                            size_t *stacksize) {
+  size_t valsize =
+      sizeof(char *); // ABI captures pointer to string,  not string
+  if ((valist->gp_offset + valsize) <= sizeof(hostrpc_pfIntRegs_t)) {
+    memcpy(((char *)valist->reg_save_area + valist->gp_offset), val, valsize);
+    valist->gp_offset += valsize;
+    return _RC_SUCCESS;
+  }
+  size_t needsize = (size_t)*stacksize + valsize;
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, val,
+         valsize);
+  *stacksize += valsize;
+  return _RC_SUCCESS;
+}
+
+// Add a floating point value when building va_list for vprintf
+static service_rc hostrpc_pfAddFloat(hostrpc_ValistExt_t *valist,
+                                           char *numdata, size_t valsize,
+                                           size_t *stacksize) {
+  // FIXME, we can used load because doubles are now aligned
+  double dval;
+  if (valsize == 4) {
+    float fval;
+    memcpy(&fval, numdata, 4);
+    dval = (double)fval; // Extend single to double per abi
+  } else if (valsize == 8) {
+    memcpy(&dval, numdata, 8);
+  } else {
+    return _RC_STATUS_ERROR;
+  }
+  if ((valist->fp_offset + FPREGSZ) <= sizeof(hostrpc_pfRegSaveArea_t)) {
+    memcpy(((char *)valist->reg_save_area + (size_t)(valist->fp_offset)), &dval,
+           sizeof(double));
+    valist->fp_offset += FPREGSZ;
+    return _RC_SUCCESS;
+  }
+  size_t needsize = (size_t)*stacksize + sizeof(double);
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, &dval,
+         sizeof(double));
+  // move only by the size of the double (8 bytes)
+  *stacksize += sizeof(double);
+  return _RC_SUCCESS;
+}
+
+// We would like to get llvm typeID enum from Type.h. e.g.
+// #include "../../../../../llvm/include/llvm/IR/Type.h"
+// But we cannot include LLVM headers in a runtime function.
+// So we a have a manual copy of llvm TypeID enum from Type.h
+enum TypeID {
+  // PrimitiveTypes
+  HalfTyID = 0,  ///< 16-bit floating point type
+  BFloatTyID,    ///< 16-bit floating point type (7-bit significand)
+  FloatTyID,     ///< 32-bit floating point type
+  DoubleTyID,    ///< 64-bit floating point type
+  X86_FP80TyID,  ///< 80-bit floating point type (X87)
+  FP128TyID,     ///< 128-bit floating point type (112-bit significand)
+  PPC_FP128TyID, ///< 128-bit floating point type (two 64-bits, PowerPC)
+  VoidTyID,      ///< type with no size
+  LabelTyID,     ///< Labels
+  MetadataTyID,  ///< Metadata
+  X86_MMXTyID,   ///< MMX vectors (64 bits, X86 specific)
+  X86_AMXTyID,   ///< AMX vectors (8192 bits, X86 specific)
+  TokenTyID,     ///< Tokens
+
+  // Derived types... see DerivedTypes.h file.
+  IntegerTyID,       ///< Arbitrary bit width integers
+  FunctionTyID,      ///< Functions
+  PointerTyID,       ///< Pointers
+  StructTyID,        ///< Structures
+  ArrayTyID,         ///< Arrays
+  FixedVectorTyID,   ///< Fixed width SIMD vector type
+  ScalableVectorTyID,///< Scalable SIMD vector type
+  TypedPointerTyID,   ///< Typed pointer used by some GPU targets
+  TargetExtTyID,      ///< Target extension type
+};
+
+// Build an extended va_list for vprintf by unpacking the buffer
+static service_rc hostrpc_pfBuildValist(hostrpc_ValistExt_t *valist,
+                                              int NumArgs, char *keyptr,
+                                              char *dataptr, char *strptr,
+                                              size_t *data_not_used) {
+  hostrpc_pfRegSaveArea_t *regs;
+  size_t regs_size = sizeof(*regs);
+  regs = (hostrpc_pfRegSaveArea_t *)malloc(regs_size);
+  if (!regs)
+    return _RC_STATUS_ERROR;
+  memset(regs, 0, regs_size);
+  *valist = (hostrpc_ValistExt_t){
+      .gp_offset = 0,
+      .fp_offset = 0,
+      .overflow_arg_area = NULL,
+      .reg_save_area = regs,
+      .overflow_size = 0,
+  };
+
+  size_t num_bytes;
+  size_t bytes_consumed;
+  size_t strsz;
+  size_t fillerNeeded;
+
+  size_t stacksize = 0;
+
+  for (int argnum = 0; argnum < NumArgs; argnum++) {
+    num_bytes = 0;
+    strsz = 0;
+    unsigned int key = *(unsigned int *)keyptr;
+    unsigned int llvmID = key >> 16;
+    unsigned int numbits = (key << 16) >> 16;
+
+    switch (llvmID) {
+    case FloatTyID:  ///<  2: 32-bit floating point type
+    case DoubleTyID: ///<  3: 64-bit floating point type
+    case FP128TyID:  ///<  5: 128-bit floating point type (112-bit mantissa)
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+      if (valist->fp_offset == 0)
+        valist->fp_offset = sizeof(hostrpc_pfIntRegs_t);
+      if (hostrpc_pfAddFloat(valist, dataptr, num_bytes, &stacksize))
+        return _RC_ADDFLOAT_ERROR;
+      break;
+
+    case IntegerTyID: ///< 11: Arbitrary bit width integers
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+      if (hostrpc_pfAddInteger(valist, dataptr, num_bytes, &stacksize))
+        return _RC_ADDINT_ERROR;
+      break;
+
+    case PointerTyID:     ///< 15: Pointers
+      if (numbits == 1) { // This is a pointer to string
+        num_bytes = 4;
+        bytes_consumed = num_bytes;
+        strsz = (size_t) * (unsigned int *)dataptr;
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        if (hostrpc_pfAddString(valist, (char *)&strptr, strsz, &stacksize))
+          return _RC_ADDSTRING_ERROR;
+      } else {
+        num_bytes = 8;
+        bytes_consumed = num_bytes;
+        fillerNeeded = ((size_t)dataptr) % num_bytes;
+        if (fillerNeeded) {
+          dataptr += fillerNeeded; // dataptr is now aligned
+          bytes_consumed += fillerNeeded;
+        }
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        if (hostrpc_pfAddInteger(valist, dataptr, num_bytes, &stacksize))
+          return _RC_ADDINT_ERROR;
+      }
+      break;
+
+    case HalfTyID:           ///<  1: 16-bit floating point type
+    case ArrayTyID:          ///< 14: Arrays
+    case StructTyID:         ///< 13: Structures
+    case FunctionTyID:       ///< 12: Functions
+    case TokenTyID:          ///< 10: Tokens
+    case X86_MMXTyID:        ///<  9: MMX vectors (64 bits, X86 specific)
+    case MetadataTyID:       ///<  8: Metadata
+    case LabelTyID:          ///<  7: Labels
+    case PPC_FP128TyID:      ///<  6: 128-bit floating point type (two 64-bits,
+                             ///<  PowerPC)
+    case X86_FP80TyID:       ///<  4: 80-bit floating point type (X87)
+    case FixedVectorTyID:    ///< 16: Fixed width SIMD vector type
+    case ScalableVectorTyID: ///< 17: Scalable SIMD vector type
+    case TypedPointerTyID:   ///< Typed pointer used by some GPU targets
+    case TargetExtTyID:      ///< Target extension type
+    case VoidTyID:
+      return _RC_UNSUPPORTED_ID_ERROR;
+      break;
+    default:
+      return _RC_INVALID_ID_ERROR;
+    }
+
+    dataptr += num_bytes;
+    strptr += strsz;
+    *data_not_used -= bytes_consumed;
+    keyptr += 4;
+  }
+  return _RC_SUCCESS;
+} // end hostrpc_pfBuildValist
+
+/*
+ *  The buffer to pack arguments for all vargs functions has thes 4 sections:
+ *  1. Header        datalen 4 bytes
+ *                   numargs 4 bytes
+ *  2. Keys          A 4-byte key for each arg including string args
+ *                   Each 4-byte key contains llvmID and numbits to
+ *                   describe the datatype.
+ *  3. args_data     Ths data values for each argument.
+ *                   Each arg is aligned according to its size.
+ *                   If the field is a string
+ *                   the dataptr contains the string length.
+ *  4. strings_data  Exection time string values
+ */
+static service_rc hostrpc_fprintf(char *buf, size_t bufsz, uint *rc) {
+
+  // FIXME: Put the collection of these 6 values in a function
+  //        All service routines that use vargs will need these values.
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  // Skip past the file pointer and format string argument
+  size_t fillerNeeded = ((size_t)dataptr) % 8;
+  if (fillerNeeded)
+    dataptr += fillerNeeded; // dataptr is now aligned on 8 byte
+  // Cannot convert directly to FILE*, so convert to 8-byte size_t first
+  FILE *fileptr = (FILE *)*((size_t *)dataptr);
+  dataptr += sizeof(FILE *); // skip past file ptr
+  NumArgs = NumArgs - 2;
+  keyptr += 8; // All keys are 4 bytes
+  size_t strsz = (size_t) * (unsigned int *)dataptr;
+  dataptr += 4; //  for strings the data value is the size, not a key
+  char *fmtstr = strptr;
+  strptr += strsz;
+  data_not_used -= (sizeof(FILE *) + 4); // 12
+
+  hostrpc_ValistExt_t valist;
+  va_list *real_va_list;
+  real_va_list = (va_list *)&valist;
+
+  if (hostrpc_pfBuildValist(&valist, NumArgs, keyptr, dataptr, strptr,
+                            &data_not_used) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Roll back offsets and save stack pointer
+  valist.gp_offset = 0;
+  valist.fp_offset = sizeof(hostrpc_pfIntRegs_t);
+  void *save_stack = valist.overflow_arg_area;
+
+  *rc = vfprintf(fileptr, fmtstr, *real_va_list);
+
+  if (valist.reg_save_area)
+    free(valist.reg_save_area);
+  if (save_stack)
+    free(save_stack);
+
+  return _RC_SUCCESS;
+}
+//  This the main service routine for printf
+static service_rc hostrpc_printf(char *buf, size_t bufsz, uint *rc) {
+  if (bufsz == 0)
+    return _RC_SUCCESS;
+
+  // Get 6 values needed to unpack the buffer
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  if (NumArgs <= 0)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Skip past the format string argument
+  char *fmtstr = strptr;
+  NumArgs--;
+  keyptr += 4;
+  size_t strsz = (size_t) * (unsigned int *)dataptr;
+  dataptr += 4; // for strings the data value is the size, not a real pointer
+  strptr += strsz;
+  data_not_used -= 4;
+
+  hostrpc_ValistExt_t valist;
+  va_list *real_va_list;
+  real_va_list = (va_list *)&valist;
+
+  if (hostrpc_pfBuildValist(&valist, NumArgs, keyptr, dataptr, strptr,
+                            &data_not_used) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Roll back offsets and save stack pointer for
+  valist.gp_offset = 0;
+  valist.fp_offset = sizeof(hostrpc_pfIntRegs_t);
+  void *save_stack = valist.overflow_arg_area;
+
+  *rc = vprintf(fmtstr, *real_va_list);
+
+  if (valist.reg_save_area)
+    free(valist.reg_save_area);
+  if (save_stack)
+    free(save_stack);
+
+  return _RC_SUCCESS;
+}
+
+//---------------- Support for hostrpc_varfn_* service ---------------------
+//
+
+// These are the helper functions for hostrpc_varfn_<TYPE>_ functions
+static uint64_t getuint32(char *val) {
+  uint32_t i32 = *(uint32_t *)val;
+  return (uint64_t)i32;
+}
+static uint64_t getuint64(char *val) { return *(uint64_t *)val; }
+
+static void *getfnptr(char *val) {
+  uint64_t ival = *(uint64_t *)val;
+  return (void *)ival;
+}
+
+// build argument array
+static service_rc hostrpc_build_vargs_array(int NumArgs, char *keyptr,
+                                                  char *dataptr, char *strptr,
+                                                  size_t *data_not_used,
+                                                  uint64_t *a[MAXVARGS]) {
+  size_t num_bytes;
+  size_t bytes_consumed;
+  size_t strsz;
+  size_t fillerNeeded;
+
+  uint argcount = 0;
+
+  for (int argnum = 0; argnum < NumArgs; argnum++) {
+    num_bytes = 0;
+    strsz = 0;
+    unsigned int key = *(unsigned int *)keyptr;
+    unsigned int llvmID = key >> 16;
+    unsigned int numbits = (key << 16) >> 16;
+
+    switch (llvmID) {
+    case FloatTyID:  ///<  2: 32-bit floating point type
+    case DoubleTyID: ///<  3: 64-bit floating point type
+    case FP128TyID:  ///<  5: 128-bit floating point type (112-bit mantissa)
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+
+      if (num_bytes == 4)
+        a[argcount] = (uint64_t *)getuint32(dataptr);
+      else
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+
+      break;
+
+    case IntegerTyID: ///< 11: Arbitrary bit width integers
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+
+      if (num_bytes == 4)
+        a[argcount] = (uint64_t *)getuint32(dataptr);
+      else
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+
+      break;
+
+    case PointerTyID:     ///< 15: Pointers
+      if (numbits == 1) { // This is a pointer to string
+        num_bytes = 4;
+        bytes_consumed = num_bytes;
+        strsz = (size_t) * (unsigned int *)dataptr;
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        a[argcount] = (uint64_t *)((char *)strptr);
+
+      } else {
+        num_bytes = 8;
+        bytes_consumed = num_bytes;
+        fillerNeeded = ((size_t)dataptr) % num_bytes;
+        if (fillerNeeded) {
+          dataptr += fillerNeeded; // dataptr is now aligned
+          bytes_consumed += fillerNeeded;
+        }
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+      }
+      break;
+
+    case HalfTyID:           ///<  1: 16-bit floating point type
+    case ArrayTyID:          ///< 14: Arrays
+    case StructTyID:         ///< 13: Structures
+    case FunctionTyID:       ///< 12: Functions
+    case TokenTyID:          ///< 10: Tokens
+    case X86_MMXTyID:        ///<  9: MMX vectors (64 bits, X86 specific)
+    case MetadataTyID:       ///<  8: Metadata
+    case LabelTyID:          ///<  7: Labels
+    case PPC_FP128TyID:      ///<  6: 128-bit floating point type (two 64-bits,
+                             ///<  PowerPC)
+    case X86_FP80TyID:       ///<  4: 80-bit floating point type (X87)
+    case FixedVectorTyID:    ///< 16: Fixed width SIMD vector type
+    case ScalableVectorTyID: ///< 17: Scalable SIMD vector type
+    case TypedPointerTyID:   ///< Typed pointer used by some GPU targets
+    case TargetExtTyID:      ///< Target extension type
+    case VoidTyID:
+      return _RC_UNSUPPORTED_ID_ERROR;
+      break;
+    default:
+      return _RC_INVALID_ID_ERROR;
+    }
+
+    // Move to next argument
+    dataptr += num_bytes;
+    strptr += strsz;
+    *data_not_used -= bytes_consumed;
+    keyptr += 4;
+    argcount++;
+  }
+  return _RC_SUCCESS;
+}
+
+// Make the vargs function call to the function pointer fnptr
+// by casting fnptr to vfnptr. Return uint32_t
+template <typename T, typename FT>
+static service_rc hostrpc_call_fnptr(uint32_t NumArgs, void *fnptr,
+                                                uint64_t *a[MAXVARGS],
+                                                T *rc) {
+  //
+  // Users are instructed that their first arg must be a dummy
+  // so that device interface is same as host interface. To match device
+  // interface we make the first arg be the function pointer.
+  //
+  FT * vfnptr = (FT *) fnptr;
+
+  switch (NumArgs) {
+  case 1:
+    *rc = vfnptr(fnptr, a[0]);
+    break;
+  case 2:
+    *rc = vfnptr(fnptr, a[0], a[1]);
+    break;
+  case 3:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2]);
+    break;
+  case 4:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3]);
+    break;
+  case 5:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4]);
+    break;
+  case 6:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5]);
+    break;
+  case 7:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6]);
+    break;
+  case 8:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7]);
+    break;
+  case 9:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8]);
+    break;
+  case 10:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9]);
+    break;
+  case 11:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10]);
+    break;
+  case 12:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11]);
+    break;
+  case 13:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12]);
+    break;
+  case 14:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13]);
+    break;
+  case 15:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14]);
+    break;
+  case 16:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15]);
+    break;
+  case 17:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16]);
+    break;
+  case 18:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17]);
+    break;
+  case 19:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18]);
+    break;
+  case 20:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19]);
+    break;
+  case 21:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20]);
+    break;
+  case 22:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21]);
+    break;
+  case 23:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22]);
+    break;
+  case 24:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23]);
+    break;
+  case 25:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24]);
+    break;
+  case 26:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25]);
+    break;
+  case 27:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26]);
+    break;
+  case 28:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27]);
+    break;
+  case 29:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28]);
+    break;
+  case 30:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29]);
+    break;
+  case 31:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29], a[30]);
+    break;
+  case 32:
+    *rc = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29], a[30], a[31]);
+    break;
+  default:
+    return _RC_EXCEED_MAXVARGS_ERROR;
+  }
+  return _RC_SUCCESS;
+}
+
+template <typename T, typename FT>
+static service_rc hostrpc_varfn(char *buf, size_t bufsz, T *rc) {
+  if (bufsz == 0)
+    return _RC_SUCCESS;
+
+  // Get 6 values needed to unpack the buffer
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  // skip the function pointer arg including any align buffer
+  if (((size_t)dataptr) % (size_t)8) {
+    dataptr += 4;
+    data_not_used -= 4;
+  }
+  void *fnptr = getfnptr(dataptr);
+  NumArgs--;
+  keyptr += 4;
+  dataptr += 8;
+  data_not_used -= 4;
+
+  if (NumArgs <= 0)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  uint64_t *a[MAXVARGS];
+  if (hostrpc_build_vargs_array(NumArgs, keyptr, dataptr, strptr,
+                                &data_not_used, a) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  if (hostrpc_call_fnptr<T,FT>(NumArgs, fnptr, a, rc) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  return _RC_SUCCESS;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/services/execute_service.h llvm-project/openmp/libomptarget/hostrpc/services/execute_service.h
--- llvm-project.upstream/openmp/libomptarget/hostrpc/services/execute_service.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/services/execute_service.h	2023-03-02 13:25:13.099430527 -0500
@@ -0,0 +1,40 @@
+#ifndef HOSTRPC_EXECUTE_SERVICE_H
+#define HOSTRPC_EXECUTE_SERVICE_H
+
+#include <stdint.h>
+#include <cstdlib>
+// Error return codes for service handler functions
+typedef enum service_rc {
+  _RC_SUCCESS = 0,
+  _RC_STATUS_UNKNOWN = 1,
+  _RC_STATUS_ERROR = 2,
+  _RC_STATUS_TERMINATE = 3,
+  _RC_DATA_USED_ERROR = 4,
+  _RC_ADDINT_ERROR = 5,
+  _RC_ADDFLOAT_ERROR = 6,
+  _RC_ADDSTRING_ERROR = 7,
+  _RC_UNSUPPORTED_ID_ERROR = 8,
+  _RC_INVALID_ID_ERROR = 9,
+  _RC_ERROR_INVALID_REQUEST = 10,
+  _RC_EXCEED_MAXVARGS_ERROR = 11,
+  _RC_INVALIDSERVICE_ERROR = 12,
+  _RC_ERROR_MEMFREE = 13,
+} service_rc;
+
+// helper functions defined in hostrpc.cpp used by execute_service
+service_rc host_malloc(void **mem, size_t size, uint32_t device_id);
+service_rc device_malloc(void **mem, size_t size, uint32_t device_id);
+service_rc host_device_mem_free(void *mem);
+void thread_abort(int);
+
+typedef struct {
+  uint64_t slots[64][8];
+} payload_t;
+
+// extern functions defined in execute_service.cpp used by hostrpc.cpp
+void execute_service(uint32_t service_id, uint32_t devid, uint64_t *payload);
+// The handler for SERVICE_SANITIZER is not serialized
+//void handler_SERVICE_SANITIZER(payload_t *payload, uint64_t activemask,
+                                                  //const uint32_t devid,
+                                                  //UriLocator *uri_locator);
+#endif
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/services/urilocator.h llvm-project/openmp/libomptarget/hostrpc/services/urilocator.h
--- llvm-project.upstream/openmp/libomptarget/hostrpc/services/urilocator.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/services/urilocator.h	2023-03-02 13:25:13.099430527 -0500
@@ -0,0 +1,66 @@
+/*
+//===--- UriLocator.h: Schema of URI Locator  -----------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+Copyright (c) 2021 - 2021 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+*/
+
+#ifndef URILOCATOR_H
+#define URILOCATOR_H
+#include "hsa/hsa_ven_amd_loader.h"
+#include <string>
+#include <vector>
+
+
+
+class UriLocator {
+
+public:
+  struct UriInfo {
+    std::string uriPath;
+    int64_t loadAddressDiff;
+  };
+
+  struct UriRange {
+    uint64_t startAddr_, endAddr_;
+    int64_t elfDelta_;
+    std::string Uri_;
+  };
+
+  bool init_ = false;
+  std::vector<UriRange> rangeTab_;
+  hsa_ven_amd_loader_1_03_pfn_t fn_table_;
+
+  hsa_status_t createUriRangeTable();
+
+  ~UriLocator() {}
+
+  UriInfo lookUpUri(uint64_t device_pc);
+  std::pair<uint64_t, uint64_t> decodeUriAndGetFd(UriInfo &uri_path,
+                                                  int *uri_fd);
+};
+
+#endif
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_fallback.cpp llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_fallback.cpp
--- llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_fallback.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_fallback.cpp	2023-03-02 21:23:47.813235489 -0500
@@ -0,0 +1,128 @@
+///
+///  hostrpc_fallback.cpp: definitions of host fallback functions for stubs.
+///                        The vargs _allocate and _execute stubs are generated
+///                        by device clang codegen and so should never be called
+///                        from the host. 
+///
+
+#include <climits>
+#include <cstdlib>
+#include <cstring>
+#include <stdarg.h>
+#include <stdint.h>
+#include <stdio.h>
+
+extern "C" {
+
+void hostrpc_fptr0(void *fnptr) {
+  void (*fptr)() = (void (*)())fnptr;
+  (*fptr)();
+}
+
+typedef uint hostrpc_varfn_uint_t(void *, ...);
+typedef uint64_t hostrpc_varfn_uint64_t(void *, ...);
+typedef double hostrpc_varfn_double_t(void *, ...);
+typedef int hostrpc_varfn_int_t(void *, ...);
+typedef long hostrpc_varfn_long_t(void *, ...);
+typedef float hostrpc_varfn_float_t(void *, ...);
+
+uint hostrpc_varfn_uint(void *fnptr, ...) {
+  hostrpc_varfn_uint_t *local_fnptr = (hostrpc_varfn_uint_t *)fnptr;
+  uint rc = local_fnptr(fnptr);
+  return rc;
+}
+uint64_t hostrpc_varfn_uint64(void *fnptr, ...) {
+  hostrpc_varfn_uint64_t *local_fnptr = (hostrpc_varfn_uint64_t *)fnptr;
+  uint64_t rc = local_fnptr(fnptr);
+  return rc;
+}
+double hostrpc_varfn_double(void *fnptr, ...) {
+  hostrpc_varfn_double_t *local_fnptr = (hostrpc_varfn_double_t *)fnptr;
+  double rc = local_fnptr(fnptr);
+  return rc;
+}
+int hostrpc_varfn_int(void *fnptr, ...) {
+  hostrpc_varfn_int_t *local_fnptr = (hostrpc_varfn_int_t *)fnptr;
+  int rc = local_fnptr(fnptr);
+  return rc;
+}
+long hostrpc_varfn_long(void *fnptr, ...) {
+  hostrpc_varfn_long_t *local_fnptr = (hostrpc_varfn_long_t *)fnptr;
+  long rc = local_fnptr(fnptr);
+  return rc;
+}
+float hostrpc_varfn_float(void *fnptr, ...) {
+  hostrpc_varfn_float_t *local_fnptr = (hostrpc_varfn_float_t *)fnptr;
+  float rc = local_fnptr(fnptr);
+  return rc;
+}
+
+static void _error(const char *fname) {
+  printf("ERROR: Calls to function %s are for device only execution\n", fname);
+}
+char *printf_allocate(uint32_t bufsz) {
+  _error((const char *)"printf_allocate");
+  return NULL;
+}
+int printf_execute(char *bufptr, uint32_t bufsz) {
+  _error("printf_execute");
+  return 0;
+}
+char *hostrpc_varfn_uint_allocate(uint32_t bufsz) {
+  _error("hostrpc_varfn_uint_allocate");
+  return NULL;
+}
+char *hostrpc_varfn_uint64_allocate(uint32_t bufsz) {
+  _error("hostrpc_varfn_uint64_allocate");
+  return NULL;
+}
+char *hostrpc_varfn_double_allocate(uint32_t bufsz) {
+  _error("hostrpc_varfn_double_allocate");
+  return NULL;
+}
+char *hostrpc_varfn_int_allocate(uint32_t bufsz) {
+  _error("hostrpc_varfn_int_allocate");
+  return NULL;
+}
+char *hostrpc_varfn_long_allocate(uint32_t bufsz) {
+  _error("hostrpc_varfn_long_allocate");
+  return NULL;
+}
+char *hostrpc_varfn_float_allocate(uint32_t bufsz) {
+  _error("hostrpc_varfn_float_allocate");
+  return NULL;
+}
+uint32_t hostrpc_varfn_uint_execute(char *bufptr, uint32_t bufsz) {
+  _error("hostrpc_varfn_uint_execute");
+  return 0;
+}
+uint64_t hostrpc_varfn_uint64_execute(char *bufptr, uint32_t bufsz) {
+  _error("hostrpc_varfn_uint64_execute");
+  return 0;
+}
+double hostrpc_varfn_double_execute(char *bufptr, uint32_t bufsz) {
+  _error("hostrpc_varfn_double_execute");
+  return 0;
+}
+uint32_t hostrpc_varfn_int_execute(char *bufptr, uint32_t bufsz) {
+  _error("hostrpc_varfn_int_execute");
+  return 0;
+}
+uint64_t hostrpc_varfn_long_execute(char *bufptr, uint32_t bufsz) {
+  _error("hostrpc_varfn_long_execute");
+  return 0;
+}
+double hostrpc_varfn_float_execute(char *bufptr, uint32_t bufsz) {
+  _error("hostrpc_varfn_float_execute");
+  return 0;
+}
+
+char *global_allocate(uint32_t bufsz) {
+  printf("HOST FALLBACK EXECUTION OF global_allocate not yet implemented\n");
+  return NULL;
+}
+int global_free(char *ptr) {
+  printf("HOST FALLBACK EXECUTION OF global_free not yet implemented\n");
+  return 0;
+}
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc.h llvm-project/openmp/libomptarget/hostrpc/src/hostrpc.h
--- llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/src/hostrpc.h	2023-03-02 20:58:15.591245394 -0500
@@ -0,0 +1,38 @@
+#ifndef __HOSTRPC_H__
+#define __HOSTRPC_H__
+
+//    hostrpc_stubs.h:
+
+#if defined(__cplusplus)
+#define EXTERN extern "C"
+#else
+#define EXTERN extern
+#endif
+
+//  TODO: Provide fortran interface via hipfort
+
+#include <stdint.h>
+#include <stdio.h>
+
+// Interfaces user-callable hostrpc functions
+EXTERN int fprintf(FILE *, const char *, ...);
+EXTERN int printf(const char *, ...);
+
+typedef uint32_t hostrpc_varfn_uint_t(void *, ...);
+typedef uint64_t hostrpc_varfn_uint64_t(void *, ...);
+typedef double   hostrpc_varfn_double_t(void *, ...);
+typedef float    hostrpc_varfn_float_t(void *, ...);
+typedef int      hostrpc_varfn_int_t(void *, ...);
+typedef long     hostrpc_varfn_long_t(void *, ...);
+
+EXTERN uint32_t hostrpc_varfn_uint(void *fnptr, ...);
+EXTERN uint64_t hostrpc_varfn_uint64(void *fnptr, ...);
+EXTERN double   hostrpc_varfn_double(void *fnptr, ...);
+EXTERN float    hostrpc_varfn_float(void *fnptr, ...);
+EXTERN int      hostrpc_varfn_int(void *fnptr, ...);
+EXTERN long     hostrpc_varfn_long(void *fnptr, ...);
+
+// This should be depricated for vargs functions.
+EXTERN void     hostrpc_fptr0(void *fptr);
+
+#endif // __HOSTRPC_H__
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_internal.h llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_internal.h
--- llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_internal.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_internal.h	2023-03-02 21:08:43.256880988 -0500
@@ -0,0 +1,117 @@
+#ifndef __HOSTRPC_INTERNAL_H__
+#define __HOSTRPC_INTERNAL_H__
+
+/*
+ *   hostrpc_internal.h:  
+
+MIT License
+
+Copyright © 2020 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is furnished
+to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+*/
+
+#if defined(__cplusplus)
+#define EXTERN extern "C"
+#else
+#define EXTERN extern
+#endif
+
+#define NOINLINE __attribute__((noinline))
+
+#include <stdint.h>
+#include <stdio.h>
+
+//  These are the interfaces for the device stubs */
+EXTERN int fprintf(FILE *, const char *, ...);
+EXTERN char *fprintf_allocate(uint32_t bufsz);
+EXTERN int printf(const char *, ...);
+EXTERN char *printf_allocate(uint32_t bufsz);
+EXTERN int printf_execute(char *bufptr, uint32_t bufsz);
+
+EXTERN char *hostrpc_varfn_uint_allocate(uint32_t bufsz);
+EXTERN char *hostrpc_varfn_uint64_allocate(uint32_t bufsz);
+EXTERN char *hostrpc_varfn_double_allocate(uint32_t bufsz);
+EXTERN char *hostrpc_varfn_int_allocate(uint32_t bufsz);
+EXTERN char *hostrpc_varfn_long_allocate(uint32_t bufsz);
+EXTERN char *hostrpc_varfn_float_allocate(uint32_t bufsz);
+EXTERN uint32_t hostrpc_varfn_uint_execute(char *bufptr, uint32_t bufsz);
+EXTERN uint64_t hostrpc_varfn_uint64_execute(char *bufptr, uint32_t bufsz);
+EXTERN double hostrpc_varfn_double_execute(char *bufptr, uint32_t bufsz);
+EXTERN int  hostrpc_varfn_int_execute(char *bufptr, uint32_t bufsz);
+EXTERN long hostrpc_varfn_long_execute(char *bufptr, uint32_t bufsz);
+EXTERN float hostrpc_varfn_float_execute(char *bufptr, uint32_t bufsz);
+
+EXTERN uint32_t __strlen_max(char *instr, uint32_t maxstrlen);
+
+typedef uint32_t hostrpc_varfn_uint_t(void *, ...);
+typedef uint64_t hostrpc_varfn_uint64_t(void *, ...);
+typedef double hostrpc_varfn_double_t(void *, ...);
+typedef int hostrpc_varfn_int_t(void *, ...);
+typedef long hostrpc_varfn_long_t(void *, ...);
+typedef float hostrpc_varfn_float_t(void *, ...);
+
+EXTERN void hostrpc_fptr0(void *fptr);
+EXTERN uint32_t hostrpc_varfn_uint(void *fnptr, ...);
+EXTERN uint64_t hostrpc_varfn_uint64(void *fnptr, ...);
+EXTERN double hostrpc_varfn_double(void *fnptr, ...);
+EXTERN int hostrpc_varfn_int(void *fnptr, ...);
+EXTERN long hostrpc_varfn_long(void *fnptr, ...);
+EXTERN float hostrpc_varfn_float(void *fnptr, ...);
+
+// Please update at least the patch level when adding a new service.
+// This will ensure that applications that use a new device stub do not
+// try to use backlevel hostrpc host runtimes that do not have the
+// implmented host version of the service.
+//
+#define HOSTRPC_VERSION 0
+#define HOSTRPC_RELEASE 8
+#define HOSTRPC_PATCH 0
+// HOSTRPC_VRM fits in two bytes allowing for 64 patches, 64 releases, and 15
+// versions
+#define HOSTRPC_VRM                                                            \
+  ((HOSTRPC_VERSION * 4096) + (HOSTRPC_RELEASE * 64) + HOSTRPC_PATCH)
+#define HOSTRPC_VERSION_RELEASE ((HOSTRPC_VERSION * 64) + HOSTRPC_RELEASE)
+typedef short hostcall_version_t;
+
+#define PACK_VERS(x) ((uint32_t)HOSTRPC_VRM << 16) | ((uint32_t)x)
+
+enum hostcall_service_id {
+  HOSTRPC_SERVICE_UNUSED,
+  HOSTRPC_SERVICE_TERMINATE,
+  HOSTRPC_SERVICE_PRINTF,
+  HOSTRPC_SERVICE_MALLOC,
+  HOSTRPC_SERVICE_MALLOC_PRINTF,
+  HOSTRPC_SERVICE_FREE,
+  HOSTRPC_SERVICE_DEMO,
+  HOSTRPC_SERVICE_FUNCTIONCALL,
+  HOSTRPC_SERVICE_VARFNUINT,
+  HOSTRPC_SERVICE_VARFNUINT64,
+  HOSTRPC_SERVICE_VARFNDOUBLE,
+  HOSTRPC_SERVICE_FPRINTF,
+  HOSTRPC_SERVICE_FTNASSIGN,
+  HOSTRPC_SERVICE_SANITIZER,
+  HOSTRPC_SERVICE_VARFNINT,
+  HOSTRPC_SERVICE_VARFNLONG,
+  HOSTRPC_SERVICE_VARFNFLOAT,
+};
+typedef enum hostcall_service_id hostcall_service_id_t;
+
+#endif // __HOSTRPC_INTERNAL_H__
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_invoke.cpp llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_invoke.cpp
--- llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_invoke.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_invoke.cpp	2023-03-02 13:25:13.099430527 -0500
@@ -0,0 +1,487 @@
+
+#include <stdint.h>
+
+#define GLOB_ATTR __attribute__((address_space(1)))
+
+// mem order codes: A=acquire, X=relaxed, R=release
+
+// headers for amdgcn opencl atomics
+extern "C"  
+uint64_t oclAtomic64Load_A(GLOB_ATTR uint64_t *Address);
+extern "C"  
+uint64_t oclAtomic64Load_X(GLOB_ATTR uint64_t *Address);
+extern "C"  
+uint32_t oclAtomic32Load_A(GLOB_ATTR const uint32_t *Address);
+extern "C"  
+bool oclAtomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr);
+extern "C"  
+bool oclAtomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr); 
+
+// headers for cuda nvptx atomics
+extern "C"  
+__attribute__((nothrow)) unsigned long long __ullAtomicAdd_system(
+		unsigned long long *address, unsigned long long val);
+extern "C" 
+__attribute__((nothrow)) unsigned long long __ullAtomicCAS_system(
+		unsigned long long int *address,
+                                         unsigned long long int compare,
+                                         unsigned long long int val);
+// headers for builtins
+int __builtin_popcountl(unsigned long);
+int __builtin_popcount(unsigned);
+int __builtin_amdgcn_readfirstlane(int);
+unsigned long __builtin_amdgcn_read_exec();
+unsigned int __builtin_amdgcn_mbcnt_hi(unsigned int, unsigned int);
+unsigned int __builtin_amdgcn_mbcnt_lo(unsigned int, unsigned int);
+int __nvvm_read_ptx_sreg_tid_x();
+
+// We need __ockl_hsa_signal_add and __ockl_lane_u32 from ockl
+typedef  uint64_t hsa_signal_value_t;
+typedef  uint64_t hsa_signal_t;
+// typedef struct hsa_signal_s {
+  //uint64_t handle;
+//} hsa_signal_t;
+typedef enum __ockl_memory_order_e {
+  __ockl_memory_order_relaxed = __ATOMIC_RELAXED,
+  __ockl_memory_order_acquire = __ATOMIC_ACQUIRE,
+  __ockl_memory_order_release = __ATOMIC_RELEASE,
+  __ockl_memory_order_acq_rel = __ATOMIC_ACQ_REL,
+  __ockl_memory_order_seq_cst = __ATOMIC_SEQ_CST,
+} __ockl_memory_order;
+extern "C" void __ockl_hsa_signal_add(hsa_signal_t signal, 
+		hsa_signal_value_t value, __ockl_memory_order mo);
+extern "C" uint32_t __ockl_lane_u32();
+
+#pragma omp begin declare target device_type(nohost)
+
+typedef enum { STATUS_SUCCESS, STATUS_BUSY } status_t;
+
+typedef enum {
+    CONTROL_OFFSET_READY_FLAG = 0,
+    CONTROL_OFFSET_RESERVED0 = 1,
+} control_offset_t;
+
+typedef enum {
+    CONTROL_WIDTH_READY_FLAG = 1,
+    CONTROL_WIDTH_RESERVED0 = 31,
+} control_width_t;
+
+typedef uint64_t LaneMask_t;
+
+typedef struct {
+    uint64_t next;
+    LaneMask_t activemask;
+    uint32_t service;
+    uint32_t control;
+} header_t;
+
+typedef struct {
+    // 64 slots of 8 uint64_ts each (4KB/payload)
+    uint64_t slots[64][8];
+} payload_t;
+
+typedef struct {
+    GLOB_ATTR header_t *headers;
+    GLOB_ATTR payload_t *payloads;
+    hsa_signal_t doorbell;
+    uint64_t free_stack;
+    uint64_t ready_stack;
+    uint32_t index_size;
+    uint32_t device_id;
+} buffer_t;
+
+namespace impl{
+
+// These functions have arch-specific variants
+void deviceSleepHostWait();
+//  These still dont have nvptx variants
+void send_signal(hsa_signal_t signal);
+uint32_t first_lane_id(uint32_t val);
+uint32_t lane_id();
+uint64_t get_mask();
+uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address);
+uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address);
+uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address);
+bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr);
+bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr);
+void write_needs_host_services_symbol();
+
+#pragma omp begin declare variant match(device = {arch(amdgcn)})
+
+inline void write_needs_host_services_symbol() {
+// The global variable "__needs_host_services" is used to detect that
+// host services are required. If hostrpc_invoke is not called, the symbol
+// will not be present and the runtime can avoid allocating and initialising
+// service_thread_buf.
+__asm__("; hostcall_invoke: record need for hostcall support\n\t"
+        ".type __needs_host_services,@object\n\t"
+        ".global __needs_host_services\n\t"
+        ".comm __needs_host_services,4":::);
+}
+
+inline uint32_t lane_id() { 
+return __builtin_amdgcn_mbcnt_hi(~0u, __builtin_amdgcn_mbcnt_lo(~0u, 0u));
+};
+
+inline uint32_t first_lane_id(uint32_t me) { return __builtin_amdgcn_readfirstlane(me); }
+inline uint64_t get_mask() { return __builtin_amdgcn_read_exec(); }
+inline void send_signal(hsa_signal_t signal) {
+__ockl_hsa_signal_add(signal, 1, __ockl_memory_order_release);
+}
+inline void deviceSleepHostWait(){
+  __builtin_amdgcn_s_sleep(1);
+}
+inline uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address) {
+  return oclAtomic64Load_A(Address); 
+}
+inline uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address) {
+  return oclAtomic64Load_X(Address); 
+}
+inline uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address) {
+  return oclAtomic32Load_A(Address); 
+}
+inline bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr) {
+  return oclAtomic64CAS_AX(Address, e_Val, new_ptr);
+}
+inline bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr) {
+  return oclAtomic64CAS_RX(Address, e_Val, new_ptr);
+}
+
+#pragma omp end declare variant
+
+#pragma omp begin declare variant match(                                       \
+    device = {arch(nvptx, nvptx64)}, implementation = {extension(match_any)})
+
+inline void write_needs_host_services_symbol() {
+// The global variable "__needs_host_services" is used to detect that
+// host services are required. If hostrpc_invoke is not called, the symbol
+// will not be present and the runtime can avoid allocating and initialising
+// service_thread_buf.
+__asm__(".global .align 4 .u32 __needs_host_services = 1;");
+}
+inline void send_signal(hsa_signal_t signal) {
+   __ullAtomicAdd_system((unsigned long long *) signal, 1); }
+
+inline void deviceSleepHostWait(){
+  int32_t start = __nvvm_read_ptx_sreg_clock();
+  for (;;) {
+    if ((__nvvm_read_ptx_sreg_clock()-start) >= 1000 )
+      break;
+  }
+}
+
+uint64_t get_mask() {
+  unsigned int Mask;
+  asm("activemask.b32 %0;" : "=r"(Mask));
+  uint64_t mask64 = (uint64_t) Mask;
+  return mask64;
+}
+
+//  FIXME: nvptx needs to use lane_id somehow here
+inline uint32_t first_lane_id(unsigned int lane_id) {
+   unsigned int mask = (unsigned int) get_mask ()  ;
+   if (mask == 0)
+     return 0;
+   unsigned int pos = 0;
+   unsigned int m = 1;
+   while (!(mask & m)) {
+     m = m<< 1;
+     pos++;
+   }
+   return pos;
+};
+
+inline uint32_t lane_id() { return (uint32_t) (__nvvm_read_ptx_sreg_tid_x() & 31); };
+
+inline uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address) {
+  unsigned long long result = __ullAtomicAdd_system((unsigned long long *) Address, 0);
+  return (uint64_t) result;
+}
+inline uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address) {
+  unsigned long long result = __ullAtomicAdd_system((unsigned long long *) Address, 0);
+  return (uint64_t) result;
+}
+inline uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address) {
+  return __uAtomicAdd((uint32_t *) Address, 0);
+}
+inline bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr) {
+  unsigned long long result = __ullAtomicCAS_system((unsigned long long *) Address, 
+		 (unsigned long long) *e_Val, (unsigned long long) new_ptr);
+  return (bool) result;
+}
+inline bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t * e_Val, uint64_t new_ptr) {
+  unsigned long long result = __ullAtomicCAS_system((unsigned long long *) Address, 
+		 (unsigned long long) *e_Val, (unsigned long long) new_ptr);
+  return (bool) result;
+}
+
+#pragma omp end declare variant
+
+} // end namespace impl
+
+static uint64_t
+get_ptr_index(uint64_t ptr, uint32_t index_size)
+{
+    return ptr & (((uint64_t)1 << index_size) - 1);
+}
+
+static GLOB_ATTR header_t *
+get_header(GLOB_ATTR buffer_t *buffer, uint64_t ptr)
+{
+    return buffer->headers + get_ptr_index(ptr, buffer->index_size);
+}
+
+static GLOB_ATTR payload_t *
+get_payload(GLOB_ATTR buffer_t *buffer, uint64_t ptr)
+{
+    return buffer->payloads + get_ptr_index(ptr, buffer->index_size);
+}
+
+// get_control_field only used by get_ready_flag
+static uint32_t
+get_control_field(uint32_t control, uint32_t offset, uint32_t width)
+{
+    return (control >> offset) & ((1 << width) - 1);
+}
+
+// get_ready_flag only called by lead lane of get_return_value
+//                on atomically loaded control field of packet header
+static uint32_t
+get_ready_flag(uint32_t control)
+{
+    return get_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                             CONTROL_WIDTH_READY_FLAG);
+}
+
+// set_control_field only used by set_ready_flag
+static uint32_t
+set_control_field(uint32_t control, uint32_t offset, uint32_t width, uint32_t value)
+{
+    uint32_t mask = ~(((1 << width) - 1) << offset);
+    return (control & mask) | (value << offset);
+}
+
+static uint32_t
+set_ready_flag(uint32_t control)
+{
+    return set_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                             CONTROL_WIDTH_READY_FLAG, 1);
+}
+
+static uint64_t
+pop(GLOB_ATTR uint64_t *top, GLOB_ATTR buffer_t *buffer)
+{
+    uint64_t F = impl::atomic64Load_A(top);
+    // F is guaranteed to be non-zero, since there are at least as
+    // many packets as there are waves, and each wave can hold at most
+    // one packet.
+    while (true) {
+        GLOB_ATTR  header_t *P = get_header(buffer, F);
+        uint64_t N = impl::atomic64Load_X(&P->next);
+        if (impl::atomic64CAS_AX(top, &F, N))
+            break;
+	impl::deviceSleepHostWait();
+    }
+
+    return F;
+}
+
+/** \brief Use the first active lane to get a free packet and
+ *         broadcast to the whole wave.
+ */
+static uint64_t
+pop_free_stack(GLOB_ATTR buffer_t *buffer, uint32_t me, uint32_t low)
+{
+    uint64_t packet_ptr = 0;
+    if (me==low) {
+        packet_ptr = pop(&buffer->free_stack, buffer);
+    }
+
+    uint32_t ptr_lo = packet_ptr;
+    uint32_t ptr_hi = packet_ptr >> 32;
+    ptr_lo = impl::first_lane_id(ptr_lo);
+    ptr_hi = impl::first_lane_id(ptr_hi);
+
+    return ((uint64_t)ptr_hi << 32) | ptr_lo;
+}
+
+static void
+push(GLOB_ATTR uint64_t *top, uint64_t ptr, GLOB_ATTR buffer_t *buffer)
+{
+    uint64_t F = impl::atomic64Load_X(top);
+    GLOB_ATTR header_t *P = get_header(buffer, ptr);
+
+    while (true) {
+        P->next = F;
+        if (impl::atomic64CAS_RX(top, &F, ptr))
+            break;
+	impl::deviceSleepHostWait();
+    }
+}
+
+/** \brief Use the first active lane in a wave to submit a ready
+ *         packet and signal the host.
+ */
+static void
+push_ready_stack(GLOB_ATTR buffer_t *buffer, uint64_t ptr, uint32_t me , uint32_t low)
+{
+    if (me==low) {
+        push(&buffer->ready_stack, ptr, buffer);
+	impl::send_signal(buffer->doorbell);
+    }
+}
+
+static uint64_t
+inc_ptr_tag(uint64_t ptr, uint32_t index_size)
+{
+    // Unit step for the tag.
+    uint64_t inc = 1UL << index_size;
+    ptr += inc;
+    // When the tag for index 0 wraps, increment the tag.
+    return ptr == 0 ? inc : ptr;
+}
+
+/** \brief Return the packet after incrementing the ABA tag
+ */
+static void
+return_free_packet(GLOB_ATTR buffer_t *buffer, uint64_t ptr, uint32_t me, uint32_t low)
+{
+    if (me==low) {
+        ptr = inc_ptr_tag(ptr, buffer->index_size);
+        push(&buffer->free_stack, ptr, buffer);
+    }
+}
+
+void 
+static fill_packet(GLOB_ATTR header_t *header, GLOB_ATTR payload_t *payload,
+            uint32_t service_id, uint64_t arg0, uint64_t arg1, uint64_t arg2, uint64_t arg3,
+            uint64_t arg4, uint64_t arg5, uint64_t arg6, uint64_t arg7, uint32_t me, uint32_t low)
+{
+    uint64_t active = impl::get_mask();
+    if (me==low) {
+        header->service = service_id;
+        header->activemask = active;
+        uint32_t control = set_ready_flag(0);
+        header->control = control;
+    }
+    GLOB_ATTR uint64_t *ptr = payload->slots[me];
+    ptr[0] = arg0;
+    ptr[1] = arg1;
+    ptr[2] = arg2;
+    ptr[3] = arg3;
+    ptr[4] = arg4;
+    ptr[5] = arg5;
+    ptr[6] = arg6;
+    ptr[7] = arg7;
+}
+
+//  result is 8*8=64 bytes per lane
+//  Total payload could be 64 lanes * 64 bytes = 4KB
+typedef struct {
+    uint64_t arg0;
+    uint64_t arg1;
+    uint64_t arg2;
+    uint64_t arg3;
+    uint64_t arg4;
+    uint64_t arg5;
+    uint64_t arg6;
+    uint64_t arg7;
+} hostrpc_result_t;
+
+/** \brief Wait for the host response and return the first two uint64_t
+ *         entries per workitem.
+ *
+ *  After the packet is submitted in READY state, the wave spins until
+ *  the host changes the state to DONE. Each workitem reads the first
+ *  two uint64_t elements in its slot and returns this.
+ */
+static hostrpc_result_t
+get_return_value(GLOB_ATTR header_t *header, GLOB_ATTR payload_t *payload, uint32_t me, uint32_t low)
+{
+    // The while loop needs to be executed by all active
+    // lanes. Otherwise, later reads from ptr are performed only by
+    // the first thread, while other threads reuse a value cached from
+    // previous operations. The use of readfirstlane in the while loop
+    // prevents this reordering.
+    //
+    // In the absence of the readfirstlane, only one thread has a
+    // sequenced-before relation from the atomic load on
+    // header->control to the ordinary loads on ptr. As a result, the
+    // compiler is free to reorder operations in such a way that the
+    // ordinary loads are performed only by the first thread. The use
+    // of readfirstlane provides a stronger code-motion barrier, and
+    // it effectively "spreads out" the sequenced-before relation to
+    // the ordinary stores in other threads too.
+    while (true) {
+        uint32_t ready_flag = 1;
+	if (me==low) {
+            uint32_t control =  impl::atomic32Load_A(
+			    (GLOB_ATTR uint32_t *) &header->control);
+            ready_flag = get_ready_flag(control);
+        }
+	ready_flag = impl::first_lane_id(ready_flag);
+        if (ready_flag == 0) 
+          break;
+	impl::deviceSleepHostWait();
+    }
+
+    GLOB_ATTR uint64_t *ptr = (GLOB_ATTR uint64_t *)(payload->slots + me);
+    hostrpc_result_t retval;
+    retval.arg0 = *ptr++;
+    retval.arg1 = *ptr++;
+    retval.arg2 = *ptr++;
+    retval.arg3 = *ptr++;
+    retval.arg4 = *ptr++;
+    retval.arg5 = *ptr++;
+    retval.arg6 = *ptr++;
+    retval.arg7 = *ptr;
+
+    return retval;
+}
+
+/** \brief The implementation that should be hidden behind an ABI
+ *
+ *  The transaction is a wave-wide operation, where the service_id
+ *  must be uniform, but the parameters are different for each
+ *  workitem. Parameters from all active lanes are written into a
+ *  hostcall packet. The hostcall blocks until the host processes the
+ *  request, and returns the response it receiveds.
+ *
+ *  TODO: This function and everything above it should eventually move
+ *  to a separate library that is loaded by the language runtime. The
+ *  function itself will be exposed as an orindary function symbol to
+ *  be linked into kernel objects that are loaded after this library.
+ */
+
+//  service_thread_buf is a global constant symbol that contains the 
+//  pointer to the buffer used by the service thread. This is written
+//  by nextgen plugin method writeGlobalToDevice only when the device
+//  image requires host services. 
+//  This is the alternative to using reserved IMPLICIT kern arg hostcall
+//  because nvptx arch does not have implicit kern args.
+uint64_t [[clang::address_space(4)]] service_thread_buf
+      [[clang::loader_uninitialized]] __attribute__((used, retain, weak,
+      visibility("protected")));
+
+extern "C" __attribute__((noinline)) hostrpc_result_t
+hostrpc_invoke( uint32_t service_id,
+                       uint64_t arg0, uint64_t arg1, uint64_t arg2, uint64_t arg3,
+                       uint64_t arg4, uint64_t arg5, uint64_t arg6, uint64_t arg7)
+{
+  impl::write_needs_host_services_symbol();
+  uint32_t me = impl::lane_id();
+  uint32_t low = impl::first_lane_id(me);
+
+  GLOB_ATTR buffer_t * buffer = (GLOB_ATTR buffer_t *) service_thread_buf;
+  uint64_t packet_ptr = pop_free_stack(buffer,me,low);
+  GLOB_ATTR header_t *header = get_header(buffer, packet_ptr);
+  GLOB_ATTR payload_t *payload = get_payload(buffer, packet_ptr);
+  fill_packet(header, payload, service_id, arg0, arg1, arg2, arg3, arg4,
+              arg5, arg6, arg7, me, low);
+  push_ready_stack(buffer, packet_ptr, me, low);
+  hostrpc_result_t retval = get_return_value(header, payload, me, low);
+  return_free_packet(buffer, packet_ptr, me, low);
+  return retval;
+}
+
+#pragma omp end declare target
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_stubs.cpp llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_stubs.cpp
--- llvm-project.upstream/openmp/libomptarget/hostrpc/src/hostrpc_stubs.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/src/hostrpc_stubs.cpp	2023-03-02 21:01:50.610447499 -0500
@@ -0,0 +1,230 @@
+///
+///  hostrpc_stubs.cpp: definitions of device stubs
+///
+// GPUs typically do not support vargs style functions.  So to implement
+// printf or any vargs function as a hostrpc service requires the compiler
+// to generate code to allocate a buffer, fill the buffer with the value of
+// each argument, and then call a stub to execute the service with a pointer to
+// the buffer. The clang compiler does this in the CGGPUBuiltin.cpp source.
+// Here we define printf_allocate and printf_execute device functions that are
+// generated by the clang compiler when it encounters a printf statement.
+// printf_allocate is implemented as a hostrpc stub. We assume that the
+// host routine for printf_execute will free the buffer that was allocated
+// by printf_allocate.
+
+#include "hostrpc_internal.h"
+#include <stdint.h>
+#include <stdio.h>
+
+#pragma omp declare target
+
+// #pragma omp begin declare variant match(device = {kind(gpu)})
+
+typedef struct hostrpc_result_s {
+  uint64_t arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7;
+} hostrpc_result_t;
+
+extern "C" hostrpc_result_t hostrpc_invoke(uint32_t id, uint64_t arg0,
+                                           uint64_t arg1, uint64_t arg2,
+                                           uint64_t arg3, uint64_t arg4,
+                                           uint64_t arg5, uint64_t arg6,
+                                           uint64_t arg7);
+
+static hostrpc_result_t
+hostrpc_invoke_zeros(uint32_t id, uint64_t arg0 = 0, uint64_t arg1 = 0,
+                     uint64_t arg2 = 0, uint64_t arg3 = 0, uint64_t arg4 = 0,
+                     uint64_t arg5 = 0, uint64_t arg6 = 0, uint64_t arg7 = 0) {
+  return hostrpc_invoke(id, arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7);
+}
+
+extern "C" {
+// This definition of __ockl_devmem_request and __ockl_sanitizer_report needs to
+// override the weak symbol for __ockl_devmem_request and
+// __ockl_sanitizer_report in rocm device lib ockl.bc because ockl uses
+// hostcall but OpenMP uses hostrpc.
+__attribute__((noinline)) uint64_t __ockl_devmem_request(uint64_t addr,
+                                                         uint64_t size) {
+  uint64_t arg0;
+  if (size) { // allocation request
+    arg0 = size;
+    hostrpc_result_t result =
+        hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_MALLOC), arg0);
+    return result.arg1;
+  } else { // free request
+    arg0 = addr;
+    hostrpc_result_t result =
+        hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_FREE), arg0);
+    return result.arg0;
+  }
+}
+
+void __ockl_sanitizer_report(uint64_t addr, uint64_t pc, uint64_t wgidx,
+                             uint64_t wgidy, uint64_t wgidz, uint64_t wave_id,
+                             uint64_t is_read, uint64_t access_size) {
+  hostrpc_result_t result =
+      hostrpc_invoke(PACK_VERS(HOSTRPC_SERVICE_SANITIZER), addr, pc, wgidx,
+                     wgidy, wgidz, wave_id, is_read, access_size);
+}
+void f90print_(char *s) { printf("%s\n", s); }
+void f90printi_(char *s, int *i) { printf("%s %d\n", s, *i); }
+void f90printl_(char *s, long *i) { printf("%s %ld\n", s, *i); }
+void f90printf_(char *s, float *f) { printf("%s %f\n", s, *f); }
+void f90printd_(char *s, double *d) { printf("%s %g\n", s, *d); }
+
+char *printf_allocate(uint32_t bufsz) {
+  uint64_t arg0 = (uint64_t)bufsz;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_MALLOC_PRINTF), arg0);
+  return (char *)result.arg1;
+}
+int printf_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_PRINTF), arg0, arg1);
+  return (int)result.arg0;
+}
+
+char *hostrpc_varfn_allocate(uint32_t bufsz) {
+  uint64_t arg0 = (uint64_t)bufsz;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_MALLOC_PRINTF), arg0);
+  return (char *)result.arg1;
+}
+
+char *hostrpc_varfn_uint_allocate(uint32_t bufsz) 
+  { return hostrpc_varfn_allocate(bufsz); }
+char *hostrpc_varfn_uint64_allocate(uint32_t bufsz) 
+  { return hostrpc_varfn_allocate(bufsz); }
+char *hostrpc_varfn_int_allocate(uint32_t bufsz) 
+  { return hostrpc_varfn_allocate(bufsz); }
+char *hostrpc_varfn_long_allocate(uint32_t bufsz) 
+  { return hostrpc_varfn_allocate(bufsz); }
+char *hostrpc_varfn_double_allocate(uint32_t bufsz) 
+  { return hostrpc_varfn_allocate(bufsz); }
+char *hostrpc_varfn_float_allocate(uint32_t bufsz) 
+  { return hostrpc_varfn_allocate(bufsz); }
+
+void hostrpc_fptr0(void *fptr) {
+  uint64_t arg0 = (uint64_t)fptr;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_FUNCTIONCALL), arg0);
+}
+
+char *fprintf_allocate(uint32_t bufsz) {
+  uint64_t arg0 = (uint64_t)bufsz;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_MALLOC_PRINTF), arg0);
+  return (char *)result.arg1;
+}
+int fprintf_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_FPRINTF), arg0, arg1);
+  return (int)result.arg0;
+}
+
+#if 0
+uint64_t __tgt_fort_ptr_assn_i8(void *varg0, void *varg1, void *varg2,
+                                void *varg3, void *varg4) {
+  uint64_t arg0, arg1, arg2, arg3, arg4;
+  arg0 = (uint64_t)varg0;
+  arg1 = (uint64_t)varg1;
+  arg2 = (uint64_t)varg2;
+  arg3 = (uint64_t)varg3;
+  arg4 = (uint64_t)varg4;
+  hostrpc_result_t result = hostrpc_invoke_zeros(
+      PACK_VERS(HOSTRPC_SERVICE_FTNASSIGN), arg0, arg1, arg2, arg3, arg4);
+  return (uint64_t)result.arg0;
+}
+#endif
+
+uint32_t hostrpc_varfn_uint_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_VARFNUINT), arg0, arg1);
+  return (uint32_t)result.arg0;
+}
+uint64_t hostrpc_varfn_uint64_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_VARFNUINT64), arg0, arg1);
+  return (uint64_t)result.arg0;
+}
+double hostrpc_varfn_double_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_VARFNDOUBLE), arg0, arg1);
+  union {
+    uint64_t val;
+    double dval;
+  } unionarg;
+  unionarg.val = result.arg0;
+  return unionarg.dval;
+}
+int hostrpc_varfn_int_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_VARFNINT), arg0, arg1);
+  return (int) result.arg0;
+}
+float hostrpc_varfn_float_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_VARFNFLOAT), arg0, arg1);
+  return (float) result.arg0;
+}
+long hostrpc_varfn_long_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostrpc_result_t result =
+      hostrpc_invoke_zeros(PACK_VERS(HOSTRPC_SERVICE_VARFNLONG), arg0, arg1);
+  union {
+    uint64_t val;
+    long lval;
+  } unionarg;
+  unionarg.val = result.arg0;
+  return unionarg.lval;
+}
+
+#if 0
+#pragma omp begin declare variant match(                                       \
+        device = {arch(amdgcn)}, implementation = {extension(match_any)})
+
+#pragma omp end declare variant
+
+#pragma omp begin declare variant match(                                       \
+        device = {arch(nvptx, nvptx64)},                                       \
+            implementation = {extension(match_any)})
+
+#pragma omp end declare variant
+
+#endif
+
+// This function is used for printf arguments that are variable length strings
+// The clang compiler will generate calls to this only when a string length is
+// not a compile time constant.
+uint32_t __strlen_max(char *instr, uint32_t maxstrlen) {
+  for (uint32_t i = 0; i < maxstrlen; i++)
+    if (instr[i] == (char)0)
+      return (uint32_t)(i + 1);
+  return maxstrlen;
+}
+
+} // end extern "C"
+
+#pragma omp end declare target
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostrpc/src/oclAtomics.cl llvm-project/openmp/libomptarget/hostrpc/src/oclAtomics.cl
--- llvm-project.upstream/openmp/libomptarget/hostrpc/src/oclAtomics.cl	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostrpc/src/oclAtomics.cl	2023-03-02 13:25:13.099430527 -0500
@@ -0,0 +1,25 @@
+
+#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
+#pragma OPENCL EXTENSION cl_khr_int64_extended_atomics : enable
+
+#define __atomic_ulong atomic_ulong
+
+// mem order codes: A=acquire, X=relaxed, R=release
+extern inline ulong oclAtomic64Load_A(__global __atomic_ulong * Address){
+  return  __opencl_atomic_load(Address, memory_order_acquire, memory_scope_all_svm_devices);
+}
+extern inline ulong oclAtomic64Load_X(__global __atomic_ulong * Address){
+  return  __opencl_atomic_load(Address, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+extern inline uint oclAtomic32Load_A(__global const atomic_uint * Address){
+  return  __opencl_atomic_load(Address, memory_order_acquire, memory_scope_all_svm_devices);
+}
+extern inline int oclAtomic64CAS_AX(__global __atomic_ulong * Address,  ulong * e_val, ulong new_val) {
+   return __opencl_atomic_compare_exchange_strong( Address, e_val, new_val,
+     memory_order_acquire, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+extern inline int oclAtomic64CAS_RX(__global __atomic_ulong * Address,  ulong * e_val, ulong new_val) {
+   return __opencl_atomic_compare_exchange_strong(Address, e_val, new_val,
+     memory_order_release, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+#undef __atomic_ulong
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt	2023-02-27 09:21:59.251665276 -0500
+++ llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt	2023-03-02 13:25:13.099430527 -0500
@@ -61,6 +61,9 @@
   set(LDFLAGS_UNDEFINED "-Wl,-z,defs")
 endif()
 
+if(HOSTRPC_BUILD_AMD)
+   set(hostrpc_link_opt "-Wl,--whole-archive amdgcn_hostrpc_services -Wl,--no-whole-archive")
+endif()
 add_llvm_library(omptarget.rtl.amdgpu SHARED
   impl/impl.cpp
   impl/interop_hsa.cpp
@@ -85,11 +88,16 @@
   elf_common
   ${LIBOMPTARGET_DEP_LIBRARIES}
   ${OPENMP_PTHREAD_LIB}
+  ${hostrpc_link_opt}
   "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/../exports"
   ${LDFLAGS_UNDEFINED}
 
   NO_INSTALL_RPATH
 )
+add_dependencies(omptarget.rtl.amdgpu omptarget.devicertl.amdgpu )
+if(HOSTRPC_BUILD_AMD)
+   add_dependencies(omptarget.rtl.amdgpu amdgcn_hostrpc_services)
+endif()
 
 target_include_directories(
   omptarget.rtl.amdgpu
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp llvm-project/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp
--- llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp	2023-02-27 09:21:39.111743452 -0500
+++ llvm-project/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp	2023-03-02 13:25:13.099430527 -0500
@@ -53,16 +53,18 @@
 // implement a fallback for toolchains that do not yet have a hostrpc library.
 extern "C" {
 uint64_t hostrpc_assign_buffer(hsa_agent_t Agent, hsa_queue_t *ThisQ,
-                               uint32_t DeviceId);
-hsa_status_t hostrpc_init();
+                               uint32_t DeviceId,
+                        hsa_amd_memory_pool_t HostMemoryPool,
+                        hsa_amd_memory_pool_t DevMemoryPool);
 hsa_status_t hostrpc_terminate();
 
-__attribute__((weak)) hsa_status_t hostrpc_init() { return HSA_STATUS_SUCCESS; }
 __attribute__((weak)) hsa_status_t hostrpc_terminate() {
   return HSA_STATUS_SUCCESS;
 }
 __attribute__((weak)) uint64_t hostrpc_assign_buffer(hsa_agent_t, hsa_queue_t *,
-                                                     uint32_t DeviceId) {
+                                                     uint32_t DeviceId,
+                               hsa_amd_memory_pool_t HostMemoryPool,
+                               hsa_amd_memory_pool_t DevMemoryPool) {
   DP("Warning: Attempting to assign hostrpc to device %u, but hostrpc library "
      "missing\n",
      DeviceId);
@@ -998,9 +1000,6 @@
       return;
     }
 
-    // Init hostcall soon after initializing hsa
-    hostrpc_init();
-
     Err = findAgents([&](hsa_device_type_t DeviceType, hsa_agent_t Agent) {
       if (DeviceType == HSA_DEVICE_TYPE_CPU) {
         CPUAgents.push_back(Agent);
@@ -1104,6 +1103,7 @@
     DeviceStateStore.clear();
     KernelArgPoolMap.clear();
     // Terminate hostrpc before finalizing hsa
+    DP("Terminating hostrpc service thread and buffer if allocated \n");
     hostrpc_terminate();
 
     hsa_status_t Err;
@@ -1503,7 +1503,9 @@
         static pthread_mutex_t HostcallInitLock = PTHREAD_MUTEX_INITIALIZER;
         pthread_mutex_lock(&HostcallInitLock);
         uint64_t Buffer = hostrpc_assign_buffer(
-            DeviceInfo().HSAAgents[DeviceId], Queue, DeviceId);
+            DeviceInfo().HSAAgents[DeviceId], Queue, DeviceId,
+            DeviceInfo().HostFineGrainedMemoryPool,
+            DeviceInfo().getDeviceMemoryPool(DeviceId));
         pthread_mutex_unlock(&HostcallInitLock);
         if (!Buffer) {
           DP("hostrpc_assign_buffer failed, gpu would dereference null and "
@@ -1531,6 +1533,9 @@
 
         // initialise pointer for implicit_argument_count == 0 ABI
         ImplArgs->HostcallPtr = Buffer;
+	DP("Hostrpc buffer allocated at %p and service thread started\n",(void*) Buffer);
+      } else {
+	DP("No hostrpc buffer or service thread required\n");
       }
 
       Packet->kernarg_address = KernArg;
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt	2023-02-27 09:21:59.251665276 -0500
+++ llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt	2023-03-02 13:25:13.099430527 -0500
@@ -85,11 +85,14 @@
   PluginInterface
   ${LIBOMPTARGET_DEP_LIBRARIES}
   ${OPENMP_PTHREAD_LIB}
+  -Wl,--whole-archive amdgcn_hostrpc_services -Wl,--no-whole-archive
   "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/../exports"
   ${LDFLAGS_UNDEFINED}
 
   NO_INSTALL_RPATH
 )
+message("====  Adding dependency for libamdgcn_hostrpc_services.a") 
+add_dependencies(omptarget.rtl.amdgpu.nextgen amdgcn_hostrpc_services)
 
 target_include_directories(
   omptarget.rtl.amdgpu.nextgen
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp
--- llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp	2023-02-28 09:27:09.124959722 -0500
+++ llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp	2023-03-02 13:25:13.099430527 -0500
@@ -58,6 +58,25 @@
 namespace target {
 namespace plugin {
 
+extern "C" {
+uint64_t hostrpc_assign_buffer(hsa_agent_t Agent, hsa_queue_t *ThisQ,
+  uint32_t DeviceId, hsa_amd_memory_pool_t HostMemoryPool,
+                        hsa_amd_memory_pool_t DevMemoryPool);
+hsa_status_t hostrpc_terminate();
+__attribute__((weak)) hsa_status_t hostrpc_terminate() {
+  return HSA_STATUS_SUCCESS;
+}
+__attribute__((weak)) uint64_t hostrpc_assign_buffer(hsa_agent_t, hsa_queue_t *,
+  uint32_t DeviceId, hsa_amd_memory_pool_t HostMemoryPool,
+                               hsa_amd_memory_pool_t DevMemoryPool) {
+  // FIXME:THIS SHOULD BE HARD FAIL 
+  DP("Warning: Attempting to assign hostrpc to device %u, but hostrpc library "
+     "missing\n",
+     DeviceId);
+  return 0;
+}
+}
+
 /// Forward declarations for all specialized data structures.
 struct AMDGPUKernelTy;
 struct AMDGPUDeviceTy;
@@ -129,6 +148,10 @@
                        "Error in hsa_amd_agent_iterate_memory_pools: %s");
 }
 
+extern "C" uint64_t hostrpc_assign_buffer(hsa_agent_t Agent, hsa_queue_t *ThisQ,
+   uint32_t DeviceId, hsa_amd_memory_pool_t HostMemoryPool,
+                        hsa_amd_memory_pool_t DevMemoryPool);
+extern "C" hsa_status_t hostrpc_terminate();
 } // namespace utils
 
 /// Utility class representing generic resource references to AMDGPU resources.
@@ -384,6 +407,9 @@
     return It->second;
   }
 
+  /// Does device image contain Symbol
+  bool hasDeviceSymbol(GenericDeviceTy &Device, StringRef SymbolName) const;
+
 private:
   /// The exectuable loaded on the agent.
   hsa_executable_t Executable;
@@ -397,6 +423,7 @@
   /// Create an AMDGPU kernel with a name and an execution mode.
   AMDGPUKernelTy(const char *Name, OMPTgtExecModeFlags ExecutionMode)
       : GenericKernelTy(Name, ExecutionMode),
+        GlobalTy_device_st_buf("service_thread_buf", sizeof(uint64_t)),
         ImplicitArgsSize(sizeof(utils::AMDGPUImplicitArgsTy)) {}
 
   /// Initialize the AMDGPU kernel.
@@ -444,8 +471,18 @@
       INFO(OMP_INFOTYPE_PLUGIN_KERNEL, Device.getDeviceId(),
            "Could not read extra information for kernel %s.", getName());
 
+    needs_host_services = AMDImage.hasDeviceSymbol(Device, "__needs_host_services");
+    if (needs_host_services) {
+      //GenericGlobalHandlerTy * GHandler = Plugin::createGlobalHandler();
+      if (auto Err = rpc_buf_handler->getGlobalMetadataFromDevice(Device, AMDImage, GlobalTy_device_st_buf ))
+      return Err;
+    }
+
     return Plugin::success();
   }
+  bool needs_host_services;
+  GlobalTy GlobalTy_device_st_buf;
+  GenericGlobalHandlerTy * rpc_buf_handler = Plugin::createGlobalHandler();
 
   /// Launch the AMDGPU kernel function.
   Error launchImpl(GenericDeviceTy &GenericDevice, uint32_t NumThreads,
@@ -637,6 +674,7 @@
     // Push the barrier with the lock acquired.
     return pushBarrierImpl(OutputSignal, InputSignal1, InputSignal2);
   }
+  hsa_queue_t * getHsaQueue()  { return Queue; }
 
 private:
   /// Push a barrier packet that will wait up to two input signals. Assumes the
@@ -757,6 +795,7 @@
 /// devices. This class relies on signals to implement streams and define the
 /// dependencies between asynchronous operations.
 struct AMDGPUStreamTy {
+  AMDGPUQueueTy* getQueue() { return &Queue; };
 private:
   /// Utility struct holding arguments for async H2H memory copies.
   struct MemcpyArgsTy {
@@ -1424,6 +1463,9 @@
 
     return Plugin::success();
   }
+  AMDGPUMemoryPoolTy * getCoarseGrainedMemoryPool(){
+    return CoarseGrainedMemoryPools[0];
+  }
 
   /// Retrieve and construct all memory pools from the device agent(s).
   virtual Error retrieveAllMemoryPools() = 0;
@@ -2236,7 +2278,6 @@
 Expected<hsa_executable_symbol_t>
 AMDGPUDeviceImageTy::findDeviceSymbol(GenericDeviceTy &Device,
                                       StringRef SymbolName) const {
-
   AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(Device);
   hsa_agent_t Agent = AMDGPUDevice.getAgent();
 
@@ -2251,6 +2292,16 @@
   return Symbol;
 }
 
+bool AMDGPUDeviceImageTy::hasDeviceSymbol(GenericDeviceTy &Device,
+                                      StringRef SymbolName) const {
+  AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(Device);
+  hsa_agent_t Agent = AMDGPUDevice.getAgent();
+  hsa_executable_symbol_t Symbol;
+  hsa_status_t Status = hsa_executable_get_symbol_by_name(
+      Executable, SymbolName.data(), &Agent, &Symbol);
+  return( Status == HSA_STATUS_SUCCESS) ;
+}
+
 template <typename ResourceTy>
 Error AMDGPUResourceRef<ResourceTy>::create(GenericDeviceTy &Device) {
   if (Resource)
@@ -2418,6 +2469,7 @@
 
   /// Deinitialize the plugin.
   Error deinitImpl() override {
+    utils::hostrpc_terminate();
     // The HSA runtime was not initialized, so nothing from the plugin was
     // actually initialized.
     if (!Initialized)
@@ -2595,6 +2647,24 @@
 
   AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(GenericDevice);
   AMDGPUStreamTy &Stream = AMDGPUDevice.getStream(AsyncInfoWrapper);
+  if (needs_host_services) {
+    int32_t devid =  AMDGPUDevice.getDeviceId() ;
+    hsa_amd_memory_pool_t host_mem_pool = HostDevice.getFineGrainedMemoryPool().get();
+    hsa_amd_memory_pool_t device_mem_pool = AMDGPUDevice.getCoarseGrainedMemoryPool()->get();
+    hsa_queue_t * hsa_queue = Stream.getQueue()->getHsaQueue();
+    uint64_t Buffer = utils::hostrpc_assign_buffer( AMDGPUDevice.getAgent(),
+		    hsa_queue, devid, host_mem_pool, device_mem_pool);
+    GlobalTy GlobalTy_host_st_buf("service_thread_buf", sizeof(uint64_t), &Buffer);
+    if (auto Err = rpc_buf_handler->writeGlobalToDevice(AMDGPUDevice, 
+			    GlobalTy_host_st_buf, GlobalTy_device_st_buf)){
+      DP("Missing symbol %s, continue execution anyway.\n",
+        GlobalTy_host_st_buf.getName().data());
+      consumeError(std::move(Err));
+    }
+    DP("Hostrpc buffer allocated at %p and service thread started\n",(void*) Buffer);
+  } else {
+    DP("No hostrpc buffer or service thread required\n");
+  }
 
   // Push the kernel launch into the stream.
   return Stream.pushKernelLaunch(*this, AllArgs, NumThreads, NumBlocks,
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt	2023-02-27 09:21:59.251665276 -0500
+++ llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt	2023-03-02 13:25:13.099430527 -0500
@@ -26,6 +26,10 @@
 set(LIBOMPTARGET_DLOPEN_LIBCUDA OFF)
 option(LIBOMPTARGET_FORCE_DLOPEN_LIBCUDA "Build with dlopened libcuda" ${LIBOMPTARGET_DLOPEN_LIBCUDA})
 
+set(hostrpc_link_opt "")
+if(HOSTRPC_BUILD_NVPTX)
+   set(hostrpc_link_opt "-Wl,--whole-archive nvptx_hostrpc_services -Wl,--no-whole-archive")
+endif()
 add_llvm_library(omptarget.rtl.cuda.nextgen SHARED
   src/rtl.cpp
 
@@ -38,10 +42,14 @@
   MemoryManager
   PluginInterface
   ${OPENMP_PTHREAD_LIB}
+  ${hostrpc_link_opt}
   "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/../exports,-z,defs"
 
   NO_INSTALL_RPATH
 )
+if(HOSTRPC_BUILD_AMD)
+   add_dependencies(omptarget.rtl.cuda.nextgen nvptx_hostrpc_services)
+endif()
 
 if(LIBOMPTARGET_DEP_CUDA_FOUND AND NOT LIBOMPTARGET_FORCE_DLOPEN_LIBCUDA)
   libomptarget_say("Building CUDA plugin linked against libcuda")
