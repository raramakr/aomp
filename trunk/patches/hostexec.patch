diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CGBuiltin.cpp llvm-project/clang/lib/CodeGen/CGBuiltin.cpp
--- llvm-project.upstream/clang/lib/CodeGen/CGBuiltin.cpp	2023-03-28 09:58:57.814057203 -0400
+++ llvm-project/clang/lib/CodeGen/CGBuiltin.cpp	2023-03-28 15:19:22.754748049 -0400
@@ -5230,6 +5230,10 @@
   case Builtin::BIprintf:
     if (getTarget().getTriple().isNVPTX() ||
         getTarget().getTriple().isAMDGCN()) {
+
+      if (getLangOpts().OpenMPIsDevice && getTarget().getTriple().isAMDGCN())
+        return EmitHostexecAllocAndExecFns(E,
+          "printf_allocate", "printf_execute");
       if (getLangOpts().OpenMPIsDevice)
         return EmitOpenMPDevicePrintfCallExpr(E);
       if (getTarget().getTriple().isNVPTX())
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CGExpr.cpp llvm-project/clang/lib/CodeGen/CGExpr.cpp
--- llvm-project.upstream/clang/lib/CodeGen/CGExpr.cpp	2023-03-23 09:58:44.257817296 -0400
+++ llvm-project/clang/lib/CodeGen/CGExpr.cpp	2023-03-28 15:19:22.754748049 -0400
@@ -5465,6 +5465,16 @@
       }
     }
   }
+  // GPUs can execute hostexec variadic functions, printf, and fprintf on host.
+  if ((CGM.getTriple().isAMDGCN() || CGM.getTriple().isNVPTX()) &&
+      CGM.getLangOpts().OpenMP && FnType &&
+      dyn_cast<FunctionProtoType>(FnType) &&
+      dyn_cast<FunctionProtoType>(FnType)->isVariadic() &&
+      (std::find(std::begin(HostexecFns), std::end(HostexecFns),
+       E->getDirectCallee()->getNameAsString()) != std::end(HostexecFns)))
+     return EmitHostexecAllocAndExecFns(E,
+         E->getDirectCallee()->getNameAsString().append("_allocate").c_str(),
+         E->getDirectCallee()->getNameAsString().append("_execute").c_str());
 
   EmitCallArgs(Args, dyn_cast<FunctionProtoType>(FnType), E->arguments(),
                E->getDirectCallee(), /*ParamsToSkip*/ 0, Order);
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CGGPUBuiltin.cpp llvm-project/clang/lib/CodeGen/CGGPUBuiltin.cpp
--- llvm-project.upstream/clang/lib/CodeGen/CGGPUBuiltin.cpp	2023-02-27 09:21:35.555757241 -0500
+++ llvm-project/clang/lib/CodeGen/CGGPUBuiltin.cpp	2023-03-28 15:19:22.758748031 -0400
@@ -213,3 +213,360 @@
   return EmitDevicePrintfCallExpr(E, this, GetOpenMPVprintfDeclaration(CGM),
                                   true);
 }
+
+// EmitHostexecAllocAndExecFns:
+//
+// For printf in an OpenMP Target region on amdgcn and for variable argument
+// functions that have a supporting host service function struct
+// is created to represent the vargs for each call site.
+// The struct contains the length, number of args, an array of 4-byte keys
+// that represent the type of of each arg, an array of aligned "data" values
+// for each arg, and finally the runtime string values. If an arg is a string
+// the data value is the runtime length of the string.  Each 4-byte key
+// contains the llvm type ID and the number of bits for the type.
+// encoded by the macro PACK_TY_BITLEN(x,y) ((uint32_t)x << 16) | ((uint32_t)y)
+// The llvm type ID of a string is pointer. To distinguish string pointers
+// from non-string pointers, the number of bitlen is set to 1.
+//
+// For example, here is a 4 arg printf function
+//
+// printf("format string %d %s %f \n", (int) 1, "string2", (double) 1.234);
+//
+// is represented by a struct with these 13 elements.
+//
+//  {81, 4, 983041, 720928, 983041, 196672, 25, int 1, 7, 0, double 1.234,
+//     "format string %d %s %ld\n", "string2" }
+//
+// 81 is the total length of the buffer that must be allocated.
+// 4 is the number of arguments.
+// The next 4 key values represent the data types of the 4 args.
+// The format string length is 25.
+// The integer field is next.
+// The string argument "string2" has length 7
+// The 4-byte dummy arg 0 is inserted so the next double arg is aligned.
+// The string arguments follows the header, keys, and data args.
+//
+// Before the struct is written, a hostexec alloc call is emitted to allocate
+// memory for the transfer. Then the struct is emitted.  Then a call
+// to the execute the GPU stub function that initiates the service
+// on the host.  The host runtime passes the buffer to the service routine
+// for processing.
+
+// These static helper functions support EmitHostexecAllocAndExecFns.
+
+// For strings that vary in length at runtime this strlen_max
+// will stop at a provided maximum.
+static llvm::Function *GetOmpStrlenDeclaration(CodeGenModule &CGM) {
+  auto &M = CGM.getModule();
+  // Args are pointer to char and maxstringlen
+  llvm::Type *ArgTypes[] = {CGM.Int8PtrTy, CGM.Int32Ty};
+  llvm::FunctionType *OmpStrlenFTy =
+      llvm::FunctionType::get(CGM.Int32Ty, ArgTypes, false);
+  if (auto *F = M.getFunction("__strlen_max")) {
+    assert(F->getFunctionType() == OmpStrlenFTy);
+    return F;
+  }
+  llvm::Function *FN = llvm::Function::Create(
+      OmpStrlenFTy, llvm::GlobalVariable::ExternalLinkage, "__strlen_max", &M);
+  return FN;
+}
+
+// Deterimines if an expression is a string with variable lenth
+static bool isVarString(const clang::Expr *argX, const clang::Type *argXTy,
+                        const llvm::Value *Arg) {
+  if ((argXTy->isPointerType() || argXTy->isConstantArrayType()) &&
+      argXTy->getPointeeOrArrayElementType()->isCharType() && !argX->isLValue())
+    return true;
+  // Ensure the VarDecl has an inititalizer
+  if (const auto *DRE = dyn_cast<DeclRefExpr>(argX))
+    if (const auto *VD = dyn_cast<VarDecl>(DRE->getDecl()))
+      if (!VD->getInit() ||
+          !llvm::isa<StringLiteral>(VD->getInit()->IgnoreImplicit()))
+        return true;
+  return false;
+}
+
+// Deterimines if an argument is a string
+static bool isString(const clang::Type *argXTy) {
+  if ((argXTy->isPointerType() || argXTy->isConstantArrayType()) &&
+      argXTy->getPointeeOrArrayElementType()->isCharType())
+    return true;
+  else
+    return false;
+}
+
+// Gets a string literal to write into the transfer buffer
+static const StringLiteral *getSL(const clang::Expr *argX,
+                                  const clang::Type *argXTy) {
+  // String in argX has known constant length
+  if (!argXTy->isConstantArrayType()) {
+    // Allow constant string to be a declared variable,
+    // But it must be constant and initialized.
+    const DeclRefExpr *DRE = cast<DeclRefExpr>(argX);
+    const VarDecl *VarD = cast<VarDecl>(DRE->getDecl());
+    argX = VarD->getInit()->IgnoreImplicit();
+  }
+  const StringLiteral *SL = cast<StringLiteral>(argX);
+  return SL;
+}
+
+// Returns a function pointer to the memory allocation routine
+static llvm::Function *GetVargsFnAllocDeclaration(CodeGenModule &CGM,
+                                                  const char *GPUAllocateName) {
+  auto &M = CGM.getModule();
+  llvm::Type *ArgTypes[] = {CGM.Int32Ty};
+  llvm::Function *FN;
+  llvm::FunctionType *VargsFnAllocFuncType = llvm::FunctionType::get(
+      llvm::PointerType::getUnqual(CGM.Int8Ty), ArgTypes, false);
+
+  if (!(FN = M.getFunction(GPUAllocateName)))
+    FN = llvm::Function::Create(VargsFnAllocFuncType,
+                                llvm::GlobalVariable::ExternalLinkage,
+                                GPUAllocateName, &M);
+  assert(FN->getFunctionType() == VargsFnAllocFuncType);
+  return FN;
+}
+
+// Returns a function pointer to the GPU stub function
+static llvm::Function *
+hostexecVargsReturnsFnDeclaration(CodeGenModule &CGM, QualType Ty,
+                                 const char *GPUStubFunctionName) {
+  auto &M = CGM.getModule();
+  llvm::Type *ArgTypes[] = {llvm::PointerType::getUnqual(CGM.Int8Ty),
+                            CGM.Int32Ty};
+  llvm::Function *FN;
+  llvm::FunctionType *VarfnFuncType =
+      llvm::FunctionType::get(CGM.getTypes().ConvertType(Ty), ArgTypes, false);
+  if (!(FN = M.getFunction(GPUStubFunctionName)))
+    FN = llvm::Function::Create(VarfnFuncType,
+                                llvm::GlobalVariable::ExternalLinkage,
+                                GPUStubFunctionName, &M);
+  assert(FN->getFunctionType() == VarfnFuncType);
+  return FN;
+}
+
+// The macro to pack the llvm type ID and numbits into 4-byte key
+#define PACK_TY_BITLEN(x, y) ((uint32_t)x << 16) | ((uint32_t)y)
+
+// Emit the code to support a host vargs function such as printf.
+RValue CodeGenFunction::EmitHostexecAllocAndExecFns(const CallExpr *E,
+                                           const char *GPUAllocateName,
+                                           const char *GPUStubFunctionName) {
+  assert(getTarget().getTriple().isAMDGCN() ||
+         getTarget().getTriple().isNVPTX());
+  // assert(E->getBuiltinCallee() == Builtin::BIprintf);
+  assert(E->getNumArgs() >= 1); // hostexec always has at least one arg.
+
+  const llvm::DataLayout &DL = CGM.getDataLayout();
+
+  CallArgList Args;
+  EmitCallArgs(Args,
+               E->getDirectCallee()->getType()->getAs<FunctionProtoType>(),
+               E->arguments(), E->getDirectCallee(),
+               /* ParamsToSkip = */ 0);
+
+  // We don't know how to emit non-scalar varargs.
+  if (std::any_of(Args.begin() + 1, Args.end(), [&](const CallArg &A) {
+        return !A.getRValue(*this).isScalar();
+      })) {
+    CGM.ErrorUnsupported(E, "non-scalar arg in GPU vargs function");
+    return RValue::get(llvm::ConstantInt::get(IntTy, 0));
+  }
+
+  unsigned NumArgs = (unsigned)Args.size();
+  llvm::SmallVector<llvm::Type *, 32> ArgTypes;
+  llvm::SmallVector<llvm::Value *, 32> VarStrLengths;
+  llvm::Value *TotalVarStrsLength = llvm::ConstantInt::get(Int32Ty, 0);
+  bool hasVarStrings = false;
+  ArgTypes.push_back(Int32Ty); // First field in struct will be total DataLen
+  ArgTypes.push_back(Int32Ty); // 2nd field in struct will be num args
+  // An array of 4-byte keys that describe the arg type
+  for (unsigned I = 0; I < NumArgs; ++I)
+    ArgTypes.push_back(Int32Ty);
+
+  // Track the size of the numeric data length and string length
+  unsigned DataLen_CT =
+      (unsigned)(DL.getTypeAllocSize(Int32Ty)) * (NumArgs + 2);
+  unsigned AllStringsLen_CT = 0;
+
+  // ---  1st Pass over Args to create ArgTypes and count size ---
+
+  size_t structOffset = 4 * (NumArgs + 2);
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Value *Arg = Args[I].getRValue(*this).getScalarVal();
+    llvm::Type *ArgType = Arg->getType();
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        hasVarStrings = true;
+        if (auto *PtrTy = dyn_cast<llvm::PointerType>(ArgType))
+          if (PtrTy->getPointerAddressSpace()) {
+            Arg = Builder.CreateAddrSpaceCast(Arg, CGM.Int8PtrTy);
+            ArgType = Arg->getType();
+          }
+        llvm::Value *VarStrLen =
+            Builder.CreateCall(GetOmpStrlenDeclaration(CGM),
+                               {Arg, llvm::ConstantInt::get(Int32Ty, 1024)});
+        VarStrLengths.push_back(VarStrLen);
+        TotalVarStrsLength = Builder.CreateAdd(TotalVarStrsLength, VarStrLen,
+                                               "sum_of_var_strings_length");
+        ArgType = Int32Ty;
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        AllStringsLen_CT += ((int)ArgString.size() + 1);
+        // change ArgType from char ptr to int to contain string length
+        ArgType = Int32Ty;
+      }
+    } // end of processing string argument
+    // if ArgTypeSize is >4 bytes we need to insert dummy align
+    // values in the struct so all stores can be aligned .
+    // These dummy fields must be inserted before the arg.
+    //
+    // In the pass below where the stores are generated careful
+    // tracking of the index into the struct is necessary.
+    size_t needsPadding = (structOffset % (size_t)DL.getTypeAllocSize(ArgType));
+    if (needsPadding) {
+      DataLen_CT += (unsigned)needsPadding;
+      structOffset += needsPadding;
+      ArgTypes.push_back(Int32Ty); // should assert that needsPadding == 4 here
+    }
+
+    ArgTypes.push_back(ArgType);
+    DataLen_CT += ((int)DL.getTypeAllocSize(ArgType));
+    structOffset += (size_t)DL.getTypeAllocSize(ArgType);
+  }
+
+  // ---  Generate call to printf_alloc to get pointer to data structure  ---
+  if (hasVarStrings)
+    TotalVarStrsLength = Builder.CreateAdd(
+        TotalVarStrsLength,
+        llvm::ConstantInt::get(Int32Ty, AllStringsLen_CT + DataLen_CT),
+        "total_buffer_size");
+  llvm::Value *BufferLen =
+      hasVarStrings
+          ? TotalVarStrsLength
+          : llvm::ConstantInt::get(Int32Ty, AllStringsLen_CT + DataLen_CT);
+
+  llvm::Value *DataStructPtr = Builder.CreateCall(
+      GetVargsFnAllocDeclaration(CGM, GPUAllocateName), {BufferLen});
+
+  // cast the generic return pointer to be a struct in device global memory
+  llvm::StructType *DataStructTy =
+      llvm::StructType::create(ArgTypes, "varfn_args_store");
+  unsigned AS = getContext().getTargetAddressSpace(LangAS::cuda_device);
+  llvm::Value *BufferPtr = Builder.CreatePointerCast(
+      DataStructPtr, llvm::PointerType::get(DataStructTy, AS),
+      "varfn_args_store_casted");
+
+  // ---  Header of struct contains length and NumArgs ---
+  llvm::Value *DataLenField = llvm::ConstantInt::get(Int32Ty, DataLen_CT);
+  llvm::Value *P = Builder.CreateStructGEP(DataStructTy, BufferPtr, 0);
+  Builder.CreateAlignedStore(
+      DataLenField, P, DL.getPrefTypeAlign(DataLenField->getType()));
+  llvm::Value *NumArgsField = llvm::ConstantInt::get(Int32Ty, NumArgs);
+  P = Builder.CreateStructGEP(DataStructTy, BufferPtr, 1);
+  Builder.CreateAlignedStore(
+      NumArgsField, P, DL.getPrefTypeAlign(NumArgsField->getType()));
+
+  // ---  2nd Pass: create array of 4-byte keys to describe each arg
+
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Type *ty = Args[I].getRValue(*this).getScalarVal()->getType();
+    llvm::Type::TypeID argtypeid =
+        Args[I].getRValue(*this).getScalarVal()->getType()->getTypeID();
+
+    // Get type size in bits. Usually 64 or 32.
+    uint32_t numbits = 0;
+    if (isString(E->getArg(I)->IgnoreParenCasts()->getType().getTypePtr()))
+      // The llvm typeID for string is pointer.  Since pointer numbits is 0,
+      // we set numbits to 1 to distinguish pointer type ID as string pointer.
+      numbits = 1;
+    else
+      numbits = ty->getScalarSizeInBits();
+    // Create a key that combines llvm typeID and size
+    llvm::Value *Key =
+        llvm::ConstantInt::get(Int32Ty, PACK_TY_BITLEN(argtypeid, numbits));
+    P = Builder.CreateStructGEP(DataStructTy, BufferPtr, I + 2);
+    Builder.CreateAlignedStore(Key, P, DL.getPrefTypeAlign(Key->getType()));
+  }
+
+  // ---  3rd Pass: Store thread-specfic data values for each arg ---
+
+  unsigned varstring_index = 0;
+  unsigned structIndex = 2 + NumArgs;
+  structOffset = 4 * structIndex;
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Value *Arg;
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        Arg = VarStrLengths[varstring_index];
+        varstring_index++;
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        int ArgStrLen = (int)ArgString.size() + 1;
+        // Change Arg from a char pointer to the integer string length
+        Arg = llvm::ConstantInt::get(Int32Ty, ArgStrLen);
+      }
+    } else {
+      Arg = Args[I].getKnownRValue().getScalarVal();
+    }
+    size_t structElementSize = (size_t)DL.getTypeAllocSize(Arg->getType());
+    size_t needsPadding = (structOffset % structElementSize);
+    if (needsPadding) {
+      // Skip over dummy fields in struct to align
+      structOffset += needsPadding; // should assert needsPadding == 4
+      structIndex++;
+    }
+    P = Builder.CreateStructGEP(DataStructTy, BufferPtr, structIndex);
+    Builder.CreateAlignedStore(Arg, P, DL.getPrefTypeAlign(Arg->getType()));
+    structOffset += structElementSize;
+    structIndex++;
+  }
+
+  // ---  4th Pass: memcpy all strings after the data values ---
+
+  // bitcast the struct in device global memory as a char buffer
+  Address BufferPtrByteAddr = Address(
+      Builder.CreatePointerCast(BufferPtr, llvm::PointerType::get(Int8Ty, AS)),
+      Int8Ty, CharUnits::fromQuantity(1));
+  // BufferPtrByteAddr is a pointer to where we want to write the next string
+  BufferPtrByteAddr = Builder.CreateConstInBoundsByteGEP(
+      BufferPtrByteAddr, CharUnits::fromQuantity(DataLen_CT));
+  varstring_index = 0;
+  for (unsigned I = 0; I < NumArgs; ++I) {
+    llvm::Value *Arg = Args[I].getKnownRValue().getScalarVal();
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        llvm::Value *varStrLength = VarStrLengths[varstring_index];
+        varstring_index++;
+        Address SrcAddr = Address(Arg, Int8Ty, CharUnits::fromQuantity(1));
+        Builder.CreateMemCpy(BufferPtrByteAddr, SrcAddr, varStrLength);
+        // update BufferPtrByteAddr for next string memcpy
+        llvm::Value *PtrAsInt = BufferPtrByteAddr.getPointer();
+        BufferPtrByteAddr = Address(
+            Builder.CreateGEP(Int8Ty,
+		    PtrAsInt, ArrayRef<llvm::Value*>(varStrLength)),
+            Int8Ty, CharUnits::fromQuantity(1));
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        int ArgStrLen = (int)ArgString.size() + 1;
+        Address SrcAddr = CGM.GetAddrOfConstantStringFromLiteral(SL);
+        Builder.CreateMemCpy(BufferPtrByteAddr, SrcAddr, ArgStrLen);
+        // update BufferPtrByteAddr for next memcpy
+        BufferPtrByteAddr = Builder.CreateConstInBoundsByteGEP(
+            BufferPtrByteAddr, CharUnits::fromQuantity(ArgStrLen));
+      }
+    }
+  }
+  return RValue::get(Builder.CreateCall(
+      hostexecVargsReturnsFnDeclaration(CGM, E->getType(), GPUStubFunctionName),
+      {DataStructPtr, BufferLen}));
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/CodeGen/CodeGenFunction.h llvm-project/clang/lib/CodeGen/CodeGenFunction.h
--- llvm-project.upstream/clang/lib/CodeGen/CodeGenFunction.h	2023-03-07 14:55:06.423746793 -0500
+++ llvm-project/clang/lib/CodeGen/CodeGenFunction.h	2023-03-28 15:19:22.758748031 -0400
@@ -4136,6 +4136,13 @@
   RValue EmitNVPTXDevicePrintfCallExpr(const CallExpr *E);
   RValue EmitAMDGPUDevicePrintfCallExpr(const CallExpr *E);
   RValue EmitOpenMPDevicePrintfCallExpr(const CallExpr *E);
+  std::vector<std::string> HostexecFns{
+      "printf",        "fprintf",         "hostexec",
+      "hostexec_uint", "hostexec_uint64", "hostexec_int",
+      "hostexec_long", "hostexec_float",  "hostexec_double"};
+  RValue EmitHostexecAllocAndExecFns(const CallExpr *E, 
+		  const char *allocate_name, const char *execute_name);
+
 
   RValue EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,
                          const CallExpr *E, ReturnValueSlot ReturnValue);
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp llvm-project/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp
--- llvm-project.upstream/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp	2023-02-10 14:55:56.548718783 -0500
+++ llvm-project/clang/lib/Driver/ToolChains/AMDGPUOpenMP.cpp	2023-03-28 15:19:22.762748013 -0400
@@ -54,6 +54,20 @@
   CC1Args.push_back(DriverArgs.MakeArgStringRef(GPUArch));
   CC1Args.push_back("-fcuda-is-device");
 
+  auto ABIVer = DeviceLibABIVersion::fromCodeObjectVersion(
+      getAMDGPUCodeObjectVersion(getDriver(), DriverArgs)).toString();
+  // link hostexec support if available for this GPU
+  StringRef libhostexec( ABIVer.compare("4")
+    ? DriverArgs.MakeArgString( getDriver().Dir + 
+      "/../lib/libdevice/libhostexec-amdgcn-" + GPUArch + ".bc")
+    : DriverArgs.MakeArgString( getDriver().Dir + 
+      "/../lib/libdevice/libhostexec400-amdgcn-" + GPUArch + ".bc")
+  );
+  if (llvm::sys::fs::exists(libhostexec)) {
+    CC1Args.push_back("-mlink-builtin-bitcode");
+    CC1Args.push_back(DriverArgs.MakeArgStringRef(libhostexec));
+  }
+
   if (DriverArgs.hasArg(options::OPT_nogpulib))
     return;
 
diff -Naur -x .git -x __pycache__ llvm-project.upstream/clang/lib/Driver/ToolChains/Clang.cpp llvm-project/clang/lib/Driver/ToolChains/Clang.cpp
--- llvm-project.upstream/clang/lib/Driver/ToolChains/Clang.cpp	2023-03-23 09:58:44.281817156 -0400
+++ llvm-project/clang/lib/Driver/ToolChains/Clang.cpp	2023-03-28 13:24:57.267847685 -0400
@@ -8156,7 +8156,10 @@
   assert(Input.isFilename() && "Invalid input.");
   CmdArgs.push_back(Input.getFilename());
 
-  const char *Exec = getToolChain().getDriver().getClangProgramPath();
+  // Get "clang" specifically, because the driver name might be set to flang-new
+  // instead, which does not support -cc1as but does also not require custom
+  // handling of this action
+  const char *Exec = Args.MakeArgString(getToolChain().GetProgramPath("clang"));
   if (D.CC1Main && !D.CCGenDiagnostics) {
     // Invoke cc1as directly in this process.
     C.addCommand(std::make_unique<CC1Command>(
diff -Naur -x .git -x __pycache__ llvm-project.upstream/flang/include/flang/Optimizer/Transforms/Passes.h llvm-project/flang/include/flang/Optimizer/Transforms/Passes.h
--- llvm-project.upstream/flang/include/flang/Optimizer/Transforms/Passes.h	2023-03-07 14:55:06.511746488 -0500
+++ llvm-project/flang/include/flang/Optimizer/Transforms/Passes.h	2023-03-28 13:24:57.267847685 -0400
@@ -20,6 +20,7 @@
 class Operation;
 class Pass;
 class Region;
+class ModuleOp;
 } // namespace mlir
 
 namespace fir {
@@ -70,6 +71,8 @@
 std::unique_ptr<mlir::Pass>
 createAlgebraicSimplificationPass(const mlir::GreedyRewriteConfig &config);
 std::unique_ptr<mlir::Pass> createPolymorphicOpConversionPass();
+std::unique_ptr<mlir::OperationPass<mlir::ModuleOp>>
+createCaptureImplicitlyDeclareTargetPass();
 
 // declarative passes
 #define GEN_PASS_REGISTRATION
diff -Naur -x .git -x __pycache__ llvm-project.upstream/flang/include/flang/Optimizer/Transforms/Passes.td llvm-project/flang/include/flang/Optimizer/Transforms/Passes.td
--- llvm-project.upstream/flang/include/flang/Optimizer/Transforms/Passes.td	2023-03-07 14:55:06.511746488 -0500
+++ llvm-project/flang/include/flang/Optimizer/Transforms/Passes.td	2023-03-28 13:24:57.267847685 -0400
@@ -284,5 +284,11 @@
   ];
 }
 
-  
+def CaptureImplicitlyDeclareTargetPass
+    : Pass<"capture-implicitly-declare-target", "mlir::ModuleOp"> {
+  let summary = "Marks all functions referenced by a declare target function as declare target";
+  let constructor = "::fir::createCaptureImplicitlyDeclareTargetPass()";
+  let dependentDialects = ["mlir::omp::OpenMPDialect"];
+}
+
 #endif // FLANG_OPTIMIZER_TRANSFORMS_PASSES
diff -Naur -x .git -x __pycache__ llvm-project.upstream/flang/lib/Frontend/FrontendActions.cpp llvm-project/flang/lib/Frontend/FrontendActions.cpp
--- llvm-project.upstream/flang/lib/Frontend/FrontendActions.cpp	2023-03-28 12:58:55.206855582 -0400
+++ llvm-project/flang/lib/Frontend/FrontendActions.cpp	2023-03-28 13:24:58.519841656 -0400
@@ -23,6 +23,7 @@
 #include "flang/Optimizer/Dialect/Support/KindMapping.h"
 #include "flang/Optimizer/Support/InitFIR.h"
 #include "flang/Optimizer/Support/Utils.h"
+#include "flang/Optimizer/Transforms/Passes.h"
 #include "flang/Parser/dump-parse-tree.h"
 #include "flang/Parser/parsing.h"
 #include "flang/Parser/provenance.h"
@@ -242,6 +243,12 @@
   pm.enableVerifier(/*verifyPasses=*/true);
   pm.addPass(std::make_unique<Fortran::lower::VerifierPass>());
 
+  // Add OpenMP-related passes
+  if (ci.getInvocation().getFrontendOpts().features.IsEnabled(
+          Fortran::common::LanguageFeature::OpenMP)) {
+    pm.addPass(fir::createCaptureImplicitlyDeclareTargetPass());
+  }
+
   if (mlir::failed(pm.run(*mlirModule))) {
     unsigned diagID = ci.getDiagnostics().getCustomDiagID(
         clang::DiagnosticsEngine::Error,
diff -Naur -x .git -x __pycache__ llvm-project.upstream/flang/lib/Lower/OpenMP.cpp llvm-project/flang/lib/Lower/OpenMP.cpp
--- llvm-project.upstream/flang/lib/Lower/OpenMP.cpp	2023-03-28 09:58:57.874056834 -0400
+++ llvm-project/flang/lib/Lower/OpenMP.cpp	2023-03-28 14:13:05.951753159 -0400
@@ -1122,6 +1122,13 @@
                    &opClauseList);
   } else if (blockDirective.v == llvm::omp::OMPD_target_data) {
     createTargetDataOp(converter, opClauseList, blockDirective.v, &eval);
+  } else if (blockDirective.v == llvm::omp::OMPD_target) {
+    // @@@Jan: Just testing to create a TargetOp
+    auto targetOp = firOpBuilder.create<mlir::omp::TargetOp>(
+        currentLocation, /*if_clause*/ mlir::Value(), /*device*/ mlir::Value(),
+        /*thread_limit*/ mlir::Value(),
+        /*nowait*/ nullptr);
+    createBodyOfOp(targetOp, converter, currentLocation, eval, &opClauseList);
   } else {
     TODO(converter.getCurrentLocation(), "Unhandled block directive");
   }
@@ -2184,6 +2191,143 @@
   converter.bindSymbol(sym, symThreadprivateExv);
 }
 
+void handleDeclareTarget(Fortran::lower::AbstractConverter &converter,
+                         Fortran::lower::pft::Evaluation &eval,
+                         const Fortran::parser::OpenMPDeclareTargetConstruct
+                             &declareTargetConstruct) {
+  std::vector<Fortran::semantics::Symbol> symbols;
+  auto findFuncAndVarSyms = [&](const Fortran::parser::OmpObjectList &objList) {
+    for (const auto &ompObject : objList.v) {
+      Fortran::common::visit(
+          Fortran::common::visitors{
+              [&](const Fortran::parser::Designator &designator) {
+                if (const Fortran::parser::Name *name =
+                        getDesignatorNameIfDataRef(designator)) {
+                  symbols.push_back(*name->symbol);
+                }
+              },
+              [&](const Fortran::parser::Name &name) {
+                symbols.push_back(*name.symbol);
+              }},
+          ompObject.u);
+    }
+  };
+
+  const auto &spec{std::get<Fortran::parser::OmpDeclareTargetSpecifier>(
+      declareTargetConstruct.t)};
+  auto mod = converter.getFirOpBuilder().getModule();
+  bool isOpenMPDevice = false;
+  if (auto offloadMod =
+          dyn_cast<mlir::omp::OffloadModuleInterface>(mod.getOperation())) {
+    isOpenMPDevice = offloadMod.getIsDevice();
+  }
+
+  // The default capture type
+  auto deviceType = Fortran::parser::OmpDeviceTypeClause::Type::Any;
+
+  if (const auto *objectList{
+          Fortran::parser::Unwrap<Fortran::parser::OmpObjectList>(spec.u)}) {
+    // Case: declare target(func, var1, var2)
+    findFuncAndVarSyms(*objectList);
+  } else if (const auto *clauseList{
+                 Fortran::parser::Unwrap<Fortran::parser::OmpClauseList>(
+                     spec.u)}) {
+    if (clauseList->v.empty()) {
+      // Case: declare target, implicit capture of function
+      symbols.push_back(eval.getOwningProcedure()->getSubprogramSymbol());
+    }
+
+    for (const auto &clause : clauseList->v) {
+      if (const auto *toClause{
+              std::get_if<Fortran::parser::OmpClause::To>(&clause.u)}) {
+        // Case: declare target to(func, var1, var2)...
+        findFuncAndVarSyms(toClause->v);
+      } else if (const auto *linkClause{
+                     std::get_if<Fortran::parser::OmpClause::Link>(
+                         &clause.u)}) {
+        // Case: declare target link(var1, var2)...
+        findFuncAndVarSyms(linkClause->v);
+      } else if (const auto *deviceClause{
+                     std::get_if<Fortran::parser::OmpClause::DeviceType>(
+                         &clause.u)}) {
+        // Case: declare target ... device_type(any | host | nohost)
+        deviceType = deviceClause->v.v;
+      }
+    }
+  }
+  // TODO for func:
+  // 1) handle link: done, can't use with function
+  // 2) handle to: done
+  // 3) the default case where neither are specified: done
+  // 4) nested implicit functions
+
+  // TODO for data:
+  // 1) lots... need to make test case first.
+
+  // might have to do the implicit capture further in during rewrite?
+  // Or if there is an end of module action
+  // or earlier during parsing...
+  // auto markAllFuncs = [&](mlir::func::FuncOp fOp) {
+  //   for (auto block = fOp.getBody().getBlocks().begin();
+  //        block != fOp.getBody().getBlocks().end(); ++block) {
+  //     llvm::errs() << "iterate on body \n";
+  //     for (auto op = block->begin(); op != block->end(); ++op) {
+  //       llvm::errs() << "iterate on op \n";
+  //       op->dump();
+  //       // probably needs to be a fir.CallOp, and then find the FuncOp
+  //       if (auto funcOp = mlir::dyn_cast<mlir::func::FuncOp>(op)) {
+  //         // markAllFuncs on func
+  //         // check if attr exists, if not apply it.
+  //         llvm::errs() << "markAllFuncs: " << funcOp->getName() << "\n";
+  //       }
+  //     }
+  //   }
+  // };
+
+  // mod.dump();
+
+  for (auto sym : symbols) {
+    auto *op = mod.lookupSymbol(converter.mangleName(sym));
+
+    // find any functions that are implicitly captured by this
+    // declare target and mark them with declare_target_type.
+    //
+    // This may be better to do at the parser/semantic level
+
+    // could be done inside of Bridge.cpp lowerFunc or lowerModule
+    // if (auto funcOp = mlir::dyn_cast<mlir::func::FuncOp>(op))
+    // markAllFuncs(funcOp);
+
+    // delete function early if we know it is going to be discared, if
+    // it is device_type any we keep it. This feels a little
+    // inconsistent as we can only remove things we know are unneeded
+    // at this stage, so we'll still end up with a module of mixed
+    // functions with some needing removal at a later stage in either
+    // case.
+    if ((deviceType == Fortran::parser::OmpDeviceTypeClause::Type::Nohost &&
+         !isOpenMPDevice) ||
+        (deviceType == Fortran::parser::OmpDeviceTypeClause::Type::Host &&
+         isOpenMPDevice)) {
+      op->dropAllUses();
+      op->dropAllReferences();
+      op->dropAllDefinedValueUses();
+      op->remove();
+    } else {
+      // Method 1: Remove function here if not desired and add adhoc
+      // attribute to the MLIR Funcs for special handling later
+      if (deviceType == Fortran::parser::OmpDeviceTypeClause::Type::Nohost) {
+        mlir::omp::OpenMPDialect::setDeclareTarget(op, "nohost");
+      } else if (deviceType ==
+                 Fortran::parser::OmpDeviceTypeClause::Type::Host) {
+        mlir::omp::OpenMPDialect::setDeclareTarget(op, "host");
+      } else if (deviceType ==
+                 Fortran::parser::OmpDeviceTypeClause::Type::Any) {
+        mlir::omp::OpenMPDialect::setDeclareTarget(op, "any");
+      }
+    }
+  }
+}
+
 void Fortran::lower::genOpenMPDeclarativeConstruct(
     Fortran::lower::AbstractConverter &converter,
     Fortran::lower::pft::Evaluation &eval,
@@ -2206,8 +2350,7 @@
           },
           [&](const Fortran::parser::OpenMPDeclareTargetConstruct
                   &declareTargetConstruct) {
-            TODO(converter.getCurrentLocation(),
-                 "OpenMPDeclareTargetConstruct");
+            handleDeclareTarget(converter, eval, declareTargetConstruct);
           },
           [&](const Fortran::parser::OpenMPRequiresConstruct
                   &requiresConstruct) {
diff -Naur -x .git -x __pycache__ llvm-project.upstream/flang/lib/Optimizer/Transforms/CMakeLists.txt llvm-project/flang/lib/Optimizer/Transforms/CMakeLists.txt
--- llvm-project.upstream/flang/lib/Optimizer/Transforms/CMakeLists.txt	2023-03-10 07:54:57.233325414 -0500
+++ llvm-project/flang/lib/Optimizer/Transforms/CMakeLists.txt	2023-03-28 13:24:57.271847666 -0400
@@ -15,7 +15,8 @@
   SimplifyIntrinsics.cpp
   AddDebugFoundation.cpp
   PolymorphicOpConversion.cpp
-
+  OMPCaptureImplicitDeclTar.cpp
+  
   DEPENDS
   FIRDialect
   FIROptTransformsPassIncGen
diff -Naur -x .git -x __pycache__ llvm-project.upstream/flang/lib/Optimizer/Transforms/OMPCaptureImplicitDeclTar.cpp llvm-project/flang/lib/Optimizer/Transforms/OMPCaptureImplicitDeclTar.cpp
--- llvm-project.upstream/flang/lib/Optimizer/Transforms/OMPCaptureImplicitDeclTar.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/flang/lib/Optimizer/Transforms/OMPCaptureImplicitDeclTar.cpp	2023-03-28 13:24:57.271847666 -0400
@@ -0,0 +1,59 @@
+#include "flang/Optimizer/Dialect/FIRDialect.h"
+#include "flang/Optimizer/Dialect/FIROps.h"
+#include "flang/Optimizer/Dialect/FIRType.h"
+#include "flang/Optimizer/Transforms/Passes.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/LLVMIR/LLVMDialect.h"
+#include "mlir/Dialect/OpenMP/OpenMPDialect.h"
+#include "mlir/IR/BuiltinDialect.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/Pass/Pass.h"
+#include <mlir/IR/BuiltinOps.h>
+#include <mlir/IR/SymbolTable.h>
+#include <mlir/Support/LLVM.h>
+
+namespace fir {
+#define GEN_PASS_DEF_CAPTUREIMPLICITLYDECLARETARGETPASS
+#include "flang/Optimizer/Transforms/Passes.h.inc"
+} // namespace fir
+
+namespace {
+// TODO: Handle case where a function is marked twice by declare target and it's
+// two different target types
+class CaptureImplicitlyDeclareTargetPass
+    : public fir::impl::CaptureImplicitlyDeclareTargetPassBase<
+          CaptureImplicitlyDeclareTargetPass> {
+
+  // TODO: deal with finding the same function twice, with different device_type
+  // should be combined into Any, or Any should supersede what was before
+  void markNestedFuncs(mlir::func::FuncOp functionOp, mlir::ModuleOp moduleOp) {
+    llvm::errs() << "CurrentFuncName: " << functionOp.getName() << "\n";
+    for (auto callOp : functionOp.getOps<fir::CallOp>()) {
+      if (auto currFOp = moduleOp.lookupSymbol<mlir::func::FuncOp>(
+              callOp.getCallee().value())) {
+        mlir::omp::OpenMPDialect::setDeclareTarget(
+            currFOp,
+            mlir::omp::OpenMPDialect::getDeclareTargetDeviceType(functionOp));
+        markNestedFuncs(currFOp, moduleOp);
+      }
+    }
+  }
+
+  void runOnOperation() override {
+    mlir::ModuleOp moduleOp = getOperation();
+    for (auto functionOp : moduleOp.getOps<mlir::func::FuncOp>()) {
+      if (mlir::omp::OpenMPDialect::isDeclareTarget(functionOp)) {
+        markNestedFuncs(functionOp, moduleOp);
+      }
+    }
+  }
+};
+
+} // namespace
+
+namespace fir {
+std::unique_ptr<mlir::OperationPass<mlir::ModuleOp>>
+createCaptureImplicitlyDeclareTargetPass() {
+  return std::make_unique<CaptureImplicitlyDeclareTargetPass>();
+}
+} // namespace fir
diff -Naur -x .git -x __pycache__ llvm-project.upstream/mlir/include/mlir/Dialect/OpenMP/OpenMPOps.td llvm-project/mlir/include/mlir/Dialect/OpenMP/OpenMPOps.td
--- llvm-project.upstream/mlir/include/mlir/Dialect/OpenMP/OpenMPOps.td	2023-03-28 12:58:55.246855414 -0400
+++ llvm-project/mlir/include/mlir/Dialect/OpenMP/OpenMPOps.td	2023-03-28 13:29:15.530627637 -0400
@@ -28,6 +28,13 @@
   let cppNamespace = "::mlir::omp";
   let dependentDialects = ["::mlir::LLVM::LLVMDialect"];
   let useDefaultAttributePrinterParser = 1;
+
+  let extraClassDeclaration = [{
+    // @@@AG: Make this into a well defined attribute if we maintain its use
+    static void setDeclareTarget(Operation *func, StringRef deviceType);
+    static bool isDeclareTarget(Operation *func);
+    static StringRef getDeclareTargetDeviceType(Operation *func);
+  }];
 }
 
 // OmpCommon requires definition of OpenACC_Dialect.
diff -Naur -x .git -x __pycache__ llvm-project.upstream/mlir/lib/Dialect/OpenMP/IR/OpenMPDialect.cpp llvm-project/mlir/lib/Dialect/OpenMP/IR/OpenMPDialect.cpp
--- llvm-project.upstream/mlir/lib/Dialect/OpenMP/IR/OpenMPDialect.cpp	2023-03-28 12:58:55.250855397 -0400
+++ llvm-project/mlir/lib/Dialect/OpenMP/IR/OpenMPDialect.cpp	2023-03-28 13:31:33.677990348 -0400
@@ -1421,6 +1421,27 @@
   return success();
 }
 
+//===----------------------------------------------------------------------===//
+// OpenMPDialect helper functions
+//===----------------------------------------------------------------------===//
+
+void OpenMPDialect::setDeclareTarget(Operation *func, StringRef deviceType) {
+  func->setAttr("omp.declare_target_type",
+                mlir::StringAttr::get(func->getContext(), deviceType));
+}
+
+bool OpenMPDialect::isDeclareTarget(Operation *func) {
+  return func->hasAttr("omp.declare_target_type");
+}
+
+StringRef OpenMPDialect::getDeclareTargetDeviceType(Operation *func) {
+  if (mlir::Attribute declTar = func->getAttr("omp.declare_target_type")) {
+    if (declTar.isa<mlir::StringAttr>())
+      return declTar.cast<mlir::StringAttr>().getValue();
+  }
+  return {};
+}
+
 #define GET_ATTRDEF_CLASSES
 #include "mlir/Dialect/OpenMP/OpenMPOpsAttributes.cpp.inc"
 
diff -Naur -x .git -x __pycache__ llvm-project.upstream/mlir/lib/Target/LLVMIR/CMakeLists.txt llvm-project/mlir/lib/Target/LLVMIR/CMakeLists.txt
--- llvm-project.upstream/mlir/lib/Target/LLVMIR/CMakeLists.txt	2023-03-23 09:58:45.765808485 -0400
+++ llvm-project/mlir/lib/Target/LLVMIR/CMakeLists.txt	2023-03-28 13:24:57.307847493 -0400
@@ -38,6 +38,7 @@
   MLIRLLVMDialect
   MLIRLLVMIRTransforms
   MLIRTranslateLib
+  MLIROpenMPDialect
   )
 
 add_mlir_translation_library(MLIRToLLVMIRTranslationRegistration
@@ -55,6 +56,7 @@
   MLIROpenACCToLLVMIRTranslation
   MLIROpenMPToLLVMIRTranslation
   MLIRROCDLToLLVMIRTranslation
+  MLIROpenMPDialect
   )
 
 add_mlir_translation_library(MLIRTargetLLVMIRImport
@@ -81,4 +83,5 @@
 
   LINK_LIBS PUBLIC
   MLIRLLVMIRToLLVMTranslation
+  MLIROpenMPDialect
   )
diff -Naur -x .git -x __pycache__ llvm-project.upstream/mlir/lib/Target/LLVMIR/Dialect/OpenMP/OpenMPToLLVMIRTranslation.cpp llvm-project/mlir/lib/Target/LLVMIR/Dialect/OpenMP/OpenMPToLLVMIRTranslation.cpp
--- llvm-project.upstream/mlir/lib/Target/LLVMIR/Dialect/OpenMP/OpenMPToLLVMIRTranslation.cpp	2023-03-23 09:58:45.769808461 -0400
+++ llvm-project/mlir/lib/Target/LLVMIR/Dialect/OpenMP/OpenMPToLLVMIRTranslation.cpp	2023-03-28 15:17:21.519296439 -0400
@@ -1659,6 +1659,19 @@
       .Case<omp::DataOp, omp::EnterDataOp, omp::ExitDataOp>([&](auto op) {
         return convertOmpTargetData(op, builder, moduleTranslation);
       })
+      .Case([&](omp::TargetOp) {
+        bool isDevice = false;
+        if (auto offloadMod = dyn_cast<mlir::omp::OffloadModuleInterface>(
+                op->getParentOfType<mlir::ModuleOp>().getOperation())) {
+          isDevice = offloadMod.getIsDevice();
+        }
+
+        printf("======== TargetOp detected with isDevice=%d\n", isDevice);
+        op->dump();
+        // Placeholder for Jan's convertOmpTarget(*op, builder,
+        // moduleTranslation);
+        return success();
+      })
       .Default([&](Operation *inst) {
         return inst->emitError("unsupported OpenMP operation: ")
                << inst->getName();
diff -Naur -x .git -x __pycache__ llvm-project.upstream/mlir/lib/Target/LLVMIR/ModuleTranslation.cpp llvm-project/mlir/lib/Target/LLVMIR/ModuleTranslation.cpp
--- llvm-project.upstream/mlir/lib/Target/LLVMIR/ModuleTranslation.cpp	2023-03-23 11:54:40.683236144 -0400
+++ llvm-project/mlir/lib/Target/LLVMIR/ModuleTranslation.cpp	2023-03-28 14:13:05.955753143 -0400
@@ -994,11 +994,29 @@
 
 LogicalResult ModuleTranslation::convertFunctions() {
   // Convert functions.
+
+  bool isDevice = false;
+  if (auto offloadMod =
+          dyn_cast<mlir::omp::OffloadModuleInterface>(mlirModule)) {
+    isDevice = offloadMod.getIsDevice();
+  }
+
   for (auto function : getModuleBody(mlirModule).getOps<LLVMFuncOp>()) {
     // Ignore external functions.
     if (function.isExternal())
       continue;
 
+    // FIXME: Must convert declare target functions on device pass.
+    //        We need a way to identify that function was defined or
+    //        declared inside declare target.
+    //        For now, we assume no declare target functions.
+    //        Target regions will have there own kernels generated.
+    printf("  FFFF Function name %s\n", function.getName().str().c_str());
+    bool isDeclareTargetFunction =
+        mlir::omp::OpenMPDialect::isDeclareTarget(function);
+    if (isDevice && !isDeclareTargetFunction)
+      continue;
+
     if (failed(convertOneFunction(function)))
       return failure();
   }
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/CMakeLists.txt llvm-project/openmp/libomptarget/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/CMakeLists.txt	2023-02-27 09:21:39.107743468 -0500
+++ llvm-project/openmp/libomptarget/CMakeLists.txt	2023-03-28 15:19:22.762748013 -0400
@@ -108,6 +108,7 @@
 add_subdirectory(plugins)
 add_subdirectory(plugins-nextgen)
 add_subdirectory(DeviceRTL)
+add_subdirectory(hostexec)
 add_subdirectory(tools)
 
 # Add tests.
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/CMakeLists.txt llvm-project/openmp/libomptarget/hostexec/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/hostexec/CMakeLists.txt	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/CMakeLists.txt	2023-03-23 23:14:46.914316803 -0400
@@ -0,0 +1,224 @@
+##===----------------------------------------------------------------------===##
+#
+# Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+# See https://llvm.org/LICENSE.txt for license information.
+# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+#
+##===----------------------------------------------------------------------===##
+#
+# Build hostexec host and device  support
+#
+##===----------------------------------------------------------------------===##
+
+cmake_minimum_required(VERSION 3.0 FATAL_ERROR)
+
+if("${CMAKE_SOURCE_DIR}" STREQUAL "${CMAKE_CURRENT_SOURCE_DIR}")
+  message(FATAL_ERROR "Direct configuration not supported, please use parent directory!")
+endif()
+
+find_package(hsa-runtime64 1.2.0 QUIET HINTS ${CMAKE_INSTALL_PREFIX} PATHS /opt/rocm/hsa)
+if (hsa-runtime64_FOUND)
+   list(APPEND HOSTRPC_ARCHS "amdgcn")
+   add_library(amdgcn_hostexec_services STATIC 
+      services/amdgcn_hostexec.cpp 
+      services/execute_service.cpp 
+      services/devsanitizer.cpp 
+      services/amdgcn_urilocator.cpp 
+   )
+   target_include_directories(
+      amdgcn_hostexec_services
+      PRIVATE
+      ${CMAKE_CURRENT_SOURCE_DIR}/services
+      ${LIBOMPTARGET_INCLUDE_DIR}
+   )
+   target_include_directories(
+      amdgcn_hostexec_services
+      PUBLIC
+      ${hsa-runtime64_DIR}/../../../include)
+   target_link_libraries(amdgcn_hostexec_services hsa-runtime64::hsa-runtime64)
+   if(SANITIZER_AMDGPU)
+      add_definitions(-DSANITIZER_AMDGPU=1)
+      set(ASAN_LIB ${LLVM_LIBRARY_DIR}/clang/${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}/lib/linux/libclang_rt.asan-x86_64.so)
+      target_link_libraries(amdgcn_hostexec_services ${ASAN_LIB})
+   endif()
+   set_property(TARGET amdgcn_hostexec_services PROPERTY POSITION_INDEPENDENT_CODE ON)
+   set(LIBOMPTARGET_SYSTEM_TARGETS "${LIBOMPTARGET_SYSTEM_TARGETS} amdgcn_hostexec_services" PARENT_SCOPE)
+else()
+   libomptarget_say("Not building hostexec for AMDGCN because hsa-runtime64 not found")
+endif()
+
+if (LIBOMPTARGET_DEP_CUDA_FOUND)
+   list(APPEND HOSTRPC_ARCHS "nvptx")
+   add_library(nvptx_hostexec_services STATIC 
+      services/execute_service.cpp
+   )
+   target_include_directories(
+      nvptx_hostexec_services
+      PRIVATE
+      ${CMAKE_CURRENT_SOURCE_DIR}/services
+      ${LIBOMPTARGET_INCLUDE_DIR}
+   )
+   set_property(TARGET nvptx_hostexec_services PROPERTY POSITION_INDEPENDENT_CODE ON)
+   set(LIBOMPTARGET_SYSTEM_TARGETS "${LIBOMPTARGET_SYSTEM_TARGETS} nvptx_hostexec_services" PARENT_SCOPE)
+else()
+   libomptarget_say("Not building hostexec for NVPTX because cuda not found")
+endif()
+
+if(NOT HOSTRPC_ARCHS)
+   return()
+endif()
+
+if (LLVM_DIR)
+  message("   -- Building hostexec with LLVM ${LLVM_PACKAGE_VERSION} found with CLANG_TOOL ${CLANG_TOOL}")
+  find_program(CLANG_TOOL clang PATHS ${LLVM_TOOLS_BINARY_DIR} NO_DEFAULT_PATH)
+  find_program(PACKAGER_TOOL clang-offload-packager PATHS ${LLVM_TOOLS_BINARY_DIR} NO_DEFAULT_PATH)
+  find_program(LINK_TOOL llvm-link PATHS ${LLVM_TOOLS_BINARY_DIR} NO_DEFAULT_PATH)
+else()
+  message("   ERROR: NO LLVM FOUND! Not building hostexec.")
+  return()
+endif()
+
+set(amdgpu_mcpus gfx700 gfx701 gfx801 gfx803 gfx900 gfx902 gfx906 gfx908 gfx90a gfx90c gfx940 gfx1010 gfx1030 gfx1031 gfx1032 gfx1033 gfx1034 gfx1035 gfx1036 gfx1100 gfx1101 gfx1102 gfx1103)
+if (DEFINED LIBOMPTARGET_AMDGCN_GFXLIST)
+  set(amdgpu_mcpus ${LIBOMPTARGET_AMDGCN_GFXLIST})
+endif()
+
+set(all_capabilities 35 37 50 52 53 60 61 62 70 72 75 80 86 89 90)
+set(LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES ${all_capabilities} CACHE STRING
+  "List of CUDA Compute Capabilities to be used to compile the NVPTX DeviceRTL.")
+string(TOLOWER ${LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES} LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES)
+if (LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES STREQUAL "all")
+  set(nvptx_sm_list ${all_capabilities})
+elseif(LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES STREQUAL "auto")
+  if (NOT LIBOMPTARGET_DEP_CUDA_FOUND)
+    libomptarget_error_say("[NVPTX] Cannot auto detect compute capability as CUDA not found.")
+  endif()
+  set(nvptx_sm_list ${LIBOMPTARGET_DEP_CUDA_ARCH})
+else()
+  string(REPLACE "," ";" nvptx_sm_list "${LIBOMPTARGET_NVPTX_COMPUTE_CAPABILITIES}")
+endif()
+foreach(sm ${nvptx_sm_list})
+   list(APPEND nvptx_mcpus "sm_${sm}")
+endforeach()
+
+set(ocl_atomics_cl_filename ${CMAKE_CURRENT_SOURCE_DIR}/src/oclAtomics.cl)
+set(invoke_cpp_file_name ${CMAKE_CURRENT_SOURCE_DIR}/src/hostexec_invoke.cpp)
+set(hostexec_stubs_filename ${CMAKE_CURRENT_SOURCE_DIR}/src/hostexec_stubs.cpp)
+set(h_file           ${CMAKE_CURRENT_SOURCE_DIR}/src/hostexec.h)
+set(internal_h_file  ${CMAKE_CURRENT_SOURCE_DIR}/src/hostexec_internal.h)
+
+foreach(archname ${HOSTRPC_ARCHS})
+   if (${archname} STREQUAL "amdgcn")
+      set(triple "amdgcn-amd-amdhsa")
+      set(mcpus ${amdgpu_mcpus})
+   endif()
+   if (${archname} STREQUAL "nvptx")
+      set(triple "nvptx64-nvidia-cuda")
+      set(mcpus ${nvptx_mcpus})
+   endif()
+
+
+   if (${archname} STREQUAL "amdgcn")
+      set(opencl_cmd ${CLANG_TOOL}
+       -fvisibility=default
+       -c -emit-llvm
+       -DCL_VERSION_2_0=200 -D__OPENCL_C_VERSION__=200
+       -Dcl_khr_fp64 -Dcl_khr_fp16
+       -Dcl_khr_subgroups -Dcl_khr_int64_base_atomics -Dcl_khr_int64_extended_atomics
+       -x cl -Xclang -cl-std=CL2.0 -Xclang -finclude-default-header
+       -target amdgcn-amd-amdhsa )
+      set(ocl_atomics_cl_bc "ocl_atomics_${archname}.bc")
+      add_custom_target(${ocl_atomics_cl_bc}
+      COMMAND ${opencl_cmd} ${ocl_atomics_cl_filename} -o ${ocl_atomics_cl_bc}
+      DEPENDS ${ocl_atomics_cl_filename})
+   endif()
+
+   foreach(mcpu ${mcpus})
+      if (${archname} STREQUAL "amdgcn")
+         set(openmp_device_args 
+	      -I../../runtime/src  # to pickup omp.h, we may need a dependency
+	      -fopenmp -fopenmp-cuda-mode -mllvm -openmp-opt-disable 
+	      -std=c++17 -fvisibility=hidden 
+	      -fopenmp-targets=${triple} -Xopenmp-target=${triple} -march=${mcpu}
+	      -c -emit-llvm --offload-device-only )
+      endif()
+      if (${archname} STREQUAL "nvptx")
+         set(openmp_device_args 
+	      -I../../runtime/src  # to pickup omp.h, we may need a dependency
+	      -fopenmp -fopenmp-cuda-mode -mllvm -openmp-opt-disable 
+	      -std=c++17 -fvisibility=hidden 
+	      -fopenmp-targets=${triple} -Xopenmp-target=${triple} -march=${mcpu}
+	      --cuda-feature=+ptx61,+${mcpu}
+	      -c -emit-llvm --offload-device-only -nocudalib -nogpulib 
+	      -Wno-unknown-cuda-version)
+      endif()
+
+      set(bc_filename "hostexec-stubs-${archname}-${mcpu}.bc")
+      add_custom_target(${bc_filename}
+         COMMAND ${CLANG_TOOL} ${openmp_device_args} ${hostexec_stubs_filename} -o ${bc_filename}
+         DEPENDS ${hostexec_stubs_filename} ${h_file} ${internal_h_file}
+         COMMENT "Built file ${bc_filename}")
+
+      set(hostexec_invoke_cpp_bc "hostexec-invoke-${archname}-${mcpu}.bc")
+      add_custom_target(${hostexec_invoke_cpp_bc}
+         COMMAND ${CLANG_TOOL} ${openmp_device_args} ${invoke_cpp_file_name} -o ${hostexec_invoke_cpp_bc}
+	 DEPENDS ${invoke_cpp_file_name}
+         COMMENT "Building bc file for hostexec_invoke: ${hostexec_invoke_cpp_bc}")
+
+      if (${archname} STREQUAL "amdgcn")
+
+         #  amdgcn needs to compile and link in ocl_atomics
+         set(invoke_resolved_bc "invoked_resolved-${archname}-${mcpu}.bc")
+         add_custom_target(${invoke_resolved_bc}
+            COMMAND ${LINK_TOOL} ${hostexec_invoke_cpp_bc} --internalize --only-needed ${ocl_atomics_cl_bc} -o ${invoke_resolved_bc}
+	    DEPENDS ${hostexec_invoke_cpp_bc} ${ocl_atomics_cl_bc}
+            COMMENT "Building invoke resolved bc ${invoke_resolved_bc}")
+          add_dependencies(${invoke_resolved_bc} ${hostexec_invoke_cpp_bc} ${ocl_atomics_cl_bc})
+
+         #  Construction of 400 files to support libhostexec400* is only needed for old plugin
+         #  The new plugin does not use implicit kernel args which are different between amdgcn 4 and amdgcn 5 abis
+         set(hostexec_invoke400_cpp_bc "hostexec-invoke400-${archname}-${mcpu}.bc")
+         add_custom_target(${hostexec_invoke400_cpp_bc}
+	      COMMAND ${CLANG_TOOL} ${openmp_device_args} -D__oclc_ABI_version=400 ${invoke_cpp_file_name} -o ${hostexec_invoke400_cpp_bc}
+	 DEPENDS ${invoke_cpp_file_name}
+         COMMENT "Building bc file for hostexec_invoke400: ${hostexec_invoke400_cpp_bc}")
+         set(invoke_resolved400_bc "invoked_resolved400-${archname}-${mcpu}.bc")
+         add_custom_target(${invoke_resolved400_bc}
+            COMMAND ${LINK_TOOL} ${hostexec_invoke400_cpp_bc} --internalize --only-needed ${ocl_atomics_cl_bc} -o ${invoke_resolved400_bc}
+	    DEPENDS ${hostexec_invoke400_cpp_bc} ${ocl_atomics_cl_bc}
+            COMMENT "Building invoke resolved400 bc ${invoke_resolved400_bc}")
+          add_dependencies(${invoke_resolved400_bc} ${hostexec_invoke400_cpp_bc} ${ocl_atomics_cl_bc})
+         set(libhostexec400-bc "libhostexec400-${archname}-${mcpu}.bc")
+         add_custom_target(${libhostexec400-bc}
+            COMMAND ${LINK_TOOL} ${bc_filename} --internalize --only-needed ${invoke_resolved400_bc} -o ${libhostexec400-bc}
+	    DEPENDS ${invoke_resolved400_bc} ${bc_filename} 
+            COMMENT "Building hostexec400 file ${libhostexec400-bc}")
+         add_dependencies(${libhostexec400-bc} ${bc_filename} ${invoke_resolved400_bc})
+      else()
+          # for nvptx we dont need ocl_atomics
+         set(invoke_resolved_bc ${hostexec_invoke_cpp_bc})
+      endif()
+
+      set(libhostexec-bc "libhostexec-${archname}-${mcpu}.bc")
+      add_custom_target(${libhostexec-bc}
+         COMMAND ${LINK_TOOL} ${bc_filename} --internalize --only-needed ${invoke_resolved_bc} -o ${libhostexec-bc}
+	 DEPENDS ${invoke_resolved_bc} ${bc_filename} 
+         COMMENT "Building hostexec file ${libhostexec-bc}")
+      add_dependencies(${libhostexec-bc} ${bc_filename} ${invoke_resolved_bc})
+
+      if (${archname} STREQUAL "amdgcn")
+         add_dependencies(amdgcn_hostexec_services ${libhostexec-bc})
+         add_dependencies(amdgcn_hostexec_services ${libhostexec400-bc})
+      endif()
+      if (${archname} STREQUAL "nvptx")
+         add_dependencies(nvptx_hostexec_services ${libhostexec-bc})
+      endif()
+      install(FILES ${CMAKE_CURRENT_BINARY_DIR}/${libhostexec-bc} DESTINATION ${DEVEL_PACKAGE}lib/libdevice)
+      if (${archname} STREQUAL "amdgcn")
+         install(FILES ${CMAKE_CURRENT_BINARY_DIR}/${libhostexec400-bc} DESTINATION ${DEVEL_PACKAGE}lib/libdevice)
+      endif()
+   endforeach() # end for each mcpu
+
+endforeach() # end foreach archs
+set(CLANG_VERSION_MAJOR 17)
+install(FILES ${h_file} DESTINATION lib/clang/${CLANG_VERSION_MAJOR}/include)
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/services/amdgcn_hostexec.cpp llvm-project/openmp/libomptarget/hostexec/services/amdgcn_hostexec.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/services/amdgcn_hostexec.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/services/amdgcn_hostexec.cpp	2023-03-24 08:11:25.385981562 -0400
@@ -0,0 +1,547 @@
+//===---- amdgcn_hostrpc.cpp - Services thread management  ----------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This contains code for the services thread for the hostrpc system using
+// amdgcn hsa. This nvptx cuda variant of this process is in development.
+//
+//===----------------------------------------------------------------------===//
+
+#include "../src/hostexec_internal.h"
+#include "execute_service.h"
+#include "urilocator.h"
+#include <atomic>
+#include <cstring>
+#include <functional>
+#include <hsa/hsa.h>
+#include <hsa/hsa_ext_amd.h>
+#include <iostream>
+#include <list>
+#include <mutex>
+#include <thread>
+
+/** Opaque wrapper for signal */
+typedef struct {
+  uint64_t handle;
+} signal_t;
+
+/** Field offsets in the packet control field */
+typedef enum {
+  CONTROL_OFFSET_READY_FLAG = 0,
+  CONTROL_OFFSET_RESERVED0 = 1,
+} control_offset_t;
+
+/** Field widths in the packet control field */
+typedef enum {
+  CONTROL_WIDTH_READY_FLAG = 1,
+  CONTROL_WIDTH_RESERVED0 = 31,
+} control_width_t;
+
+/** Packet header */
+typedef struct {
+  /** Tagged pointer to the next packet in an intrusive stack */
+  uint64_t next;
+  /** Bitmask that represents payload slots with valid data */
+  uint64_t activemask;
+  /** Service ID requested by the wave */
+  uint32_t service;
+  /** Control bits.
+   *  \li \c READY flag is bit 0. Indicates packet awaiting a host response.
+   */
+  uint32_t control;
+} header_t;
+
+/** \brief Hostcall state.
+ *
+ *  Holds the state of hostcalls being requested by all kernels that
+ *  share the same hostcall state. There is usually one buffer per
+ *  device queue.
+ */
+typedef struct {
+  /** Array of 2^index_size packet headers */
+  header_t *headers;
+  /** Array of 2^index_size packet payloads */
+  payload_t *payloads;
+  /** Signal used by kernels to indicate new work */
+  signal_t doorbell;
+  /** Stack of free packets */
+  uint64_t free_stack;
+  /** Stack of ready packets */
+  uint64_t ready_stack;
+  /** Number of LSBs in the tagged pointer can index into the packet arrays */
+  uint32_t index_size;
+  /** Device ID */
+  uint32_t device_id;
+} buffer_t;
+
+enum { SIGNAL_INIT = UINT64_MAX, SIGNAL_DONE = UINT64_MAX - 1 };
+
+static uint32_t get_buffer_alignment() { return alignof(payload_t); }
+
+static uint32_t set_control_field(uint32_t control, uint8_t offset,
+                                  uint8_t width, uint32_t value) {
+  uint32_t mask = ~(((1 << width) - 1) << offset);
+  control &= mask;
+  return control | (value << offset);
+}
+
+static uint32_t reset_ready_flag(uint32_t control) {
+  return set_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                           CONTROL_WIDTH_READY_FLAG, 0);
+}
+
+static uint64_t get_ptr_index(uint64_t ptr, uint32_t index_size) {
+  return ptr & ((1UL << index_size) - 1);
+}
+
+static uintptr_t align_to(uintptr_t value, uint32_t alignment) {
+  if (value % alignment == 0)
+    return value;
+  return value - (value % alignment) + alignment;
+}
+
+static uintptr_t get_header_start() {
+  return align_to(sizeof(buffer_t), alignof(header_t));
+}
+
+static uintptr_t get_payload_start(uint32_t num_packets) {
+  auto header_start = get_header_start();
+  auto header_end = header_start + sizeof(header_t) * num_packets;
+  return align_to(header_end, alignof(payload_t));
+}
+
+static size_t get_buffer_size(uint32_t num_packets) {
+  size_t buffer_size = get_payload_start(num_packets);
+  buffer_size += num_packets * sizeof(payload_t);
+  return buffer_size;
+}
+static uint64_t grab_ready_stack(buffer_t *buffer) {
+  return __atomic_exchange_n(&buffer->ready_stack, 0,
+                             std::memory_order_acquire);
+}
+static header_t *get_header(buffer_t *buffer, ulong ptr) {
+  return buffer->headers + get_ptr_index(ptr, buffer->index_size);
+}
+static payload_t *get_payload(buffer_t *buffer, ulong ptr) {
+  return buffer->payloads + get_ptr_index(ptr, buffer->index_size);
+}
+
+static signal_t create_signal() {
+  hsa_signal_t hs;
+  hsa_status_t status = hsa_signal_create(SIGNAL_INIT, 0, NULL, &hs);
+  if (status != HSA_STATUS_SUCCESS)
+    return {0};
+  return {hs.handle};
+}
+
+static hsa_amd_memory_pool_t static_host_memory_pool;
+static hsa_amd_memory_pool_t static_device_memory_pools[8];
+static hsa_agent_t static_hsa_agents[8];
+
+void save_hsa_statics(uint32_t device_id, hsa_amd_memory_pool_t HostMemoryPool,
+                      hsa_amd_memory_pool_t DevMemoryPool,
+                      hsa_agent_t hsa_agent) {
+  static_host_memory_pool = HostMemoryPool;
+  static_device_memory_pools[device_id] = DevMemoryPool;
+  static_hsa_agents[device_id] = hsa_agent;
+}
+
+// ====== START of helper functions for execute_service ======
+service_rc host_device_mem_free(void *ptr) {
+  hsa_status_t err = hsa_amd_memory_pool_free(ptr);
+  if (err == HSA_STATUS_SUCCESS)
+    return _RC_SUCCESS;
+  else
+    return _RC_ERROR_MEMFREE;
+}
+
+service_rc host_malloc(void **ptr, size_t size, uint32_t devid) {
+  hsa_amd_memory_pool_t MemoryPool = static_host_memory_pool;
+  hsa_agent_t agent = static_hsa_agents[devid];
+  hsa_status_t err = hsa_amd_memory_pool_allocate(MemoryPool, size, 0, ptr);
+  if (err == HSA_STATUS_SUCCESS)
+    err = hsa_amd_agents_allow_access(1, &agent, NULL, *ptr);
+  if (err != HSA_STATUS_SUCCESS)
+    thread_abort(_RC_ERROR_HSAFAIL);
+  return _RC_SUCCESS;
+}
+
+service_rc device_malloc(void **mem, size_t size, uint32_t devid) {
+  hsa_amd_memory_pool_t MemoryPool = static_device_memory_pools[devid];
+  hsa_status_t err = hsa_amd_memory_pool_allocate(MemoryPool, size, 0, mem);
+  if (err != HSA_STATUS_SUCCESS)
+    thread_abort(_RC_ERROR_HSAFAIL);
+  return _RC_SUCCESS;
+}
+
+void thread_abort(service_rc rc) {
+  fprintf(stderr,"hostrpc thread_abort called with code %d\n", rc);
+  abort();
+}
+// ====== END helper functions for execute_service ======
+
+/** \brief Locked reference to critical data.
+ *
+ *         Simpler version of the LockedAccessor in HIP sources.
+ *
+ *         Protects access to the member _data with a lock acquired on
+ *         contruction/destruction. T must contain a _mutex field
+ *         which meets the BasicLockable requirements (lock/unlock)
+ */
+template <typename T> struct locked_accessor_t {
+  locked_accessor_t(T &criticalData) : _criticalData(&criticalData) {
+    _criticalData->_mutex.lock();
+  };
+  ~locked_accessor_t() { _criticalData->_mutex.unlock(); }
+  // Syntactic sugar so -> can be used to get the underlying type.
+  T *operator->() { return _criticalData; };
+
+private:
+  T *_criticalData;
+};
+struct record_t {
+  bool discarded;
+};
+struct critical_data_t {
+  std::unordered_map<buffer_t *, record_t> buffers;
+  std::mutex _mutex;
+};
+typedef locked_accessor_t<critical_data_t> locked_critical_data_t;
+
+typedef struct {
+  hsa_queue_t *hsa_q;
+  buffer_t *hcb;
+  uint32_t devid;
+} hsaq_buf_entry_t;
+
+extern "C" void handler_SERVICE_SANITIZER(payload_t *packt_payload,
+                                          uint64_t activemask,
+                                          uint32_t gpu_device,
+                                          UriLocator *uri_locator);
+
+static bool static_version_was_checked = false;
+struct consumer_t {
+private:
+  signal_t doorbell;
+  std::thread thread;
+  critical_data_t critical_data;
+  UriLocator *urilocator;
+  consumer_t(signal_t _doorbell) : doorbell(_doorbell) {}
+  // Table of hsa_q's and their associated buffer_t's
+  std::list<hsaq_buf_entry_t *> hsaq_bufs;
+
+public:
+  static consumer_t *create_consumer();
+
+  hsaq_buf_entry_t *add_hsaq_buf_entry(buffer_t *hcb, hsa_queue_t *hsa_q,
+                                       uint32_t devid) {
+    hsaq_buf_entry_t *new_hsaq_buf = new hsaq_buf_entry_t;
+    new_hsaq_buf->hcb = hcb;
+    new_hsaq_buf->devid = devid;
+    new_hsaq_buf->hsa_q = hsa_q;
+    hsaq_bufs.push_back(new_hsaq_buf);
+    return new_hsaq_buf;
+  }
+
+  hsaq_buf_entry_t *find_hsaq_buf_entry(hsa_queue_t *hsa_q) {
+    for (auto hsaq_buf : hsaq_bufs) {
+      if (hsaq_buf->hsa_q == hsa_q)
+        return hsaq_buf;
+    }
+    return NULL;
+  }
+
+  service_rc check_version(uint device_vrm) const {
+    if (device_vrm == (unsigned int)HOSTEXEC_VRM)
+      return _RC_SUCCESS;
+    uint device_version_release = device_vrm >> 6;
+    if (device_version_release != HOSTEXEC_VERSION_RELEASE) {
+      fprintf(stderr,"ERROR Incompatible device and host release\n     Device "
+             "release(%d)\n     Host release(%d)\n",
+             device_version_release, HOSTEXEC_VERSION_RELEASE);
+      return _RC_ERROR_WRONGVERSION;
+    }
+    if (device_vrm > HOSTEXEC_VRM) {
+      fprintf(stderr,"ERROR Incompatible device and host version\n      Device "
+             "version(%d)\n     Host version(%d)\n",
+             device_vrm, HOSTEXEC_VERSION_RELEASE);
+      fprintf(stderr,"         Upgrade libomptarget runtime on your system.\n");
+      return _RC_ERROR_OLDHOSTVERSIONMOD;
+    }
+    if (device_vrm < HOSTEXEC_VRM) {
+      unsigned int host_ver = ((unsigned int)HOSTEXEC_VRM) >> 12;
+      unsigned int host_rel = (((unsigned int)HOSTEXEC_VRM) << 20) >> 26;
+      unsigned int host_mod = (((unsigned int)HOSTEXEC_VRM) << 26) >> 26;
+      unsigned int dev_ver = ((unsigned int)device_vrm) >> 12;
+      unsigned int dev_rel = (((unsigned int)device_vrm) << 20) >> 26;
+      unsigned int dev_mod = (((unsigned int)device_vrm) << 26) >> 26;
+      fprintf(stderr,
+          "WARNING:  Device mod version < host mod version \n          Device "
+          "version: %d.%d.%d\n          Host version:   %d.%d.%d\n",
+          dev_ver, dev_rel, dev_mod, host_ver, host_rel, host_mod);
+      fprintf(stderr,
+        "          Consider rebuild binary with more recent compiler.\n");
+    }
+    return _RC_SUCCESS;
+  }
+
+  void process_packets(buffer_t *buffer, uint64_t ready_stack) const {
+    // This function is always called from consume_packets, which owns
+    // the lock for the critical data.
+
+    // Each wave can submit at most one packet at a time, and all
+    // waves independently push ready packets. The stack of packets at
+    // this point cannot contain multiple packets from the same wave,
+    // so consuming ready packets in a latest-first order does not
+    // affect any wave.
+    for (decltype(ready_stack) iter = ready_stack, next = 0; iter;
+         iter = next) {
+
+      // Remember the next packet pointer. The current packet will
+      // get reused from the free stack after we process it.
+      auto header = get_header(buffer, iter);
+      next = header->next;
+
+      auto payload = get_payload(buffer, iter);
+      uint64_t activemask = header->activemask;
+
+      // split the 32-bit service number into service_id and VRM to be checked
+      // if device hostrpc or stubs are ahead of this host runtime.
+      uint service_id = (header->service << 16) >> 16;
+      if (!static_version_was_checked) {
+        uint device_vrm = ((uint)(header->service) >> 16);
+        service_rc err = check_version(device_vrm);
+        if (err != _RC_SUCCESS)
+          thread_abort(err);
+        static_version_was_checked = true;
+      }
+
+      if (service_id == HOSTEXEC_SID_SANITIZER) {
+        handler_SERVICE_SANITIZER(payload, activemask, buffer->device_id,
+                                  urilocator);
+      } else {
+        // Serialize calls to execute_service for each active lane
+        // TODO: One could use ffs to skip inactive lanes faster.
+        for (uint32_t wi = 0; wi != 64; ++wi) {
+          uint64_t flag = activemask & ((uint64_t)1 << wi);
+          if (flag == 0)
+            continue;
+          execute_service(service_id, buffer->device_id, payload->slots[wi]);
+        }
+      }
+      __atomic_store_n(&header->control, reset_ready_flag(header->control),
+                       std::memory_order_release);
+    }
+  }
+
+  // FIXME: This cannot be const because it locks critical data.
+  // A lock-free implementaiton might make that possible.
+  void consume_packets() {
+    /* TODO: The consumer iterates over all registered buffers in an
+       unspecified order, and for each buffer, processes packets also
+       in an unspecified order. This may need a more efficient
+       strategy based on the turnaround time for the services
+       requested by all these packets.
+     */
+    uint64_t signal_value = SIGNAL_INIT;
+    uint64_t timeout = 1024 * 1024;
+
+    while (true) {
+      hsa_signal_t hs{doorbell.handle};
+      signal_value =
+          hsa_signal_wait_scacquire(hs, HSA_SIGNAL_CONDITION_NE, signal_value,
+                                    timeout, HSA_WAIT_STATE_BLOCKED);
+      if (signal_value == SIGNAL_DONE) {
+        return;
+      }
+
+      locked_critical_data_t data(critical_data);
+
+      for (auto ii = data->buffers.begin(), ie = data->buffers.end(); ii != ie;
+           /* don't increment here */) {
+        auto record = ii->second;
+        if (record.discarded) {
+          ii = data->buffers.erase(ii);
+          continue;
+        }
+
+        buffer_t *buffer = ii->first;
+        uint64_t F = grab_ready_stack(buffer);
+        if (F)
+          process_packets(buffer, F);
+        ++ii;
+      }
+    }
+    return;
+  }
+
+  service_rc launch_service_thread() {
+    if (thread.joinable())
+      return _RC_ERROR_CONSUMER_ACTIVE;
+    thread = std::thread(&consumer_t::consume_packets, this);
+    if (!thread.joinable())
+      return _RC_ERROR_CONSUMER_LAUNCH_FAILED;
+    return _RC_SUCCESS;
+  }
+
+  service_rc terminate_service_thread() {
+    if (!thread.joinable())
+      return _RC_ERROR_CONSUMER_INACTIVE;
+    hsa_signal_t signal = {doorbell.handle};
+    hsa_signal_store_screlease(signal, SIGNAL_DONE);
+    thread.join();
+    return _RC_SUCCESS;
+  }
+
+  void register_buffer(void *b) {
+    locked_critical_data_t data(critical_data);
+    auto buffer = reinterpret_cast<buffer_t *>(b);
+    auto &record = data->buffers[buffer];
+    record.discarded = false;
+    buffer->doorbell = doorbell;
+    urilocator = new UriLocator();
+  }
+
+  service_rc deregister_buffer(void *b) {
+    locked_critical_data_t data(critical_data);
+    auto buffer = reinterpret_cast<buffer_t *>(b);
+    if (data->buffers.count(buffer) == 0)
+      return _RC_ERROR_INVALID_REQUEST;
+    auto &record = data->buffers[buffer];
+    if (record.discarded)
+      return _RC_ERROR_INVALID_REQUEST;
+    record.discarded = true;
+    return _RC_SUCCESS;
+  }
+
+  // destructor triggered by delete static_consumer_ptr in hostrpc_terminate().
+  ~consumer_t() {
+    for (auto hsaq_buf : hsaq_bufs) {
+      if (hsaq_buf) {
+        deregister_buffer(hsaq_buf->hcb);
+        delete hsaq_buf;
+      }
+    }
+    hsaq_bufs.clear();
+    terminate_service_thread();
+    delete urilocator;
+    critical_data.buffers.clear();
+    hsa_signal_t hs{doorbell.handle};
+    hsa_signal_destroy(hs);
+  }
+
+  buffer_t *create_buffer_t(uint32_t num_packets, uint32_t devid) {
+    if (num_packets == 0) {
+      fprintf(stderr,"hostrpc create_buffer-t num_packets cannot be zero.\n");
+      thread_abort(_RC_ERROR_ZEROPACKETS);
+    }
+    size_t size = get_buffer_size(num_packets);
+    uint32_t align = get_buffer_alignment();
+    void *newbuffer = NULL;
+    service_rc err = host_malloc(&newbuffer, size + align, devid);
+    if (!newbuffer || (err != _RC_SUCCESS)) {
+      fprintf(stderr,"hostrpc call to host_malloc failed \n");
+      thread_abort(err);
+    }
+
+    if ((uintptr_t)newbuffer % get_buffer_alignment() != 0) {
+      fprintf(stderr,"ERROR: incorrect alignment \n");
+      thread_abort(_RC_ERROR_ALIGNMENT);
+    }
+
+    //  Initialize the buffer_t
+    buffer_t *hb = (buffer_t *)newbuffer;
+
+    hb->headers = (header_t *)((uint8_t *)hb + get_header_start());
+    hb->payloads =
+        (payload_t *)((uint8_t *)hb + get_payload_start(num_packets));
+
+    uint32_t index_size = 1;
+    if (num_packets > 2)
+      index_size = 32 - __builtin_clz(num_packets);
+    hb->index_size = index_size;
+    hb->headers[0].next = 0;
+
+    uint64_t next = 1UL << index_size;
+    for (uint32_t ii = 1; ii != num_packets; ++ii) {
+      hb->headers[ii].next = next;
+      next = ii;
+    }
+    hb->free_stack = next;
+    hb->ready_stack = 0;
+    hb->device_id = devid;
+    return hb;
+  }
+
+}; // end of class/struct consumer_t
+
+consumer_t *consumer_t::create_consumer() {
+  signal_t doorbell = create_signal();
+  if (doorbell.handle == 0) {
+    return nullptr;
+  }
+  return new consumer_t(doorbell);
+}
+
+// Currently, a single instance of consumer_t is created and saved statically.
+// This instance starts a single service thread for ALL devices.
+static consumer_t *static_consumer_ptr = NULL;
+
+// This is the main hostrpc function called by the amdgpu plugin when
+// launching a kernel on a designated hsa_queue_t. This function should only
+// be called if any kernel in the device image requires hostrpc services.
+extern "C" unsigned long
+hostrpc_assign_buffer(hsa_agent_t agent, hsa_queue_t *this_Q,
+                      uint32_t device_id, hsa_amd_memory_pool_t HostMemoryPool,
+                      hsa_amd_memory_pool_t DevMemoryPool) {
+  // Create and launch the services thread
+  if (!static_consumer_ptr) {
+    static_consumer_ptr = consumer_t::create_consumer();
+    service_rc err = static_consumer_ptr->launch_service_thread();
+    if (err != _RC_SUCCESS)
+      thread_abort(err);
+  }
+
+  // quick return to kernel launch if this hsa q is being reused
+  hsaq_buf_entry_t *hsaq_buf = static_consumer_ptr->find_hsaq_buf_entry(this_Q);
+  if (hsaq_buf)
+    return (unsigned long)hsaq_buf->hcb;
+
+  // Helper functions for execute_service need these hsa values saved
+  save_hsa_statics(device_id, HostMemoryPool, DevMemoryPool, agent);
+
+  // Get values needed to determine buffer size
+  uint32_t numCu;
+  hsa_agent_get_info(
+      agent, (hsa_agent_info_t)HSA_AMD_AGENT_INFO_COMPUTE_UNIT_COUNT, &numCu);
+  // ErrorCheck(Could not get number of cus, err);
+  uint32_t waverPerCu;
+  hsa_agent_get_info(agent,
+                     (hsa_agent_info_t)HSA_AMD_AGENT_INFO_MAX_WAVES_PER_CU,
+                     &waverPerCu);
+  // ErrorCheck(Could not get number of waves per cu, err);
+  unsigned int minpackets = numCu * waverPerCu;
+
+  //  Create and initialize the new buffer to return to kernel launch
+  buffer_t *hcb = static_consumer_ptr->create_buffer_t(minpackets, device_id);
+
+  // Register the buffer for the consumer thread
+  static_consumer_ptr->register_buffer(hcb);
+
+  // Cache in hsaq_bufs for reuse
+  hsaq_buf = static_consumer_ptr->add_hsaq_buf_entry(hcb, this_Q, device_id);
+  return (unsigned long)hcb;
+}
+
+extern "C" hsa_status_t hostrpc_terminate() {
+  if (static_consumer_ptr) {
+    // The consumer_t destructor takes care of all memory returns
+    delete static_consumer_ptr;
+    static_consumer_ptr = NULL;
+  }
+  return HSA_STATUS_SUCCESS;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/services/amdgcn_urilocator.cpp llvm-project/openmp/libomptarget/hostexec/services/amdgcn_urilocator.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/services/amdgcn_urilocator.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/services/amdgcn_urilocator.cpp	2023-03-15 22:36:45.000000000 -0400
@@ -0,0 +1,228 @@
+//===---- amdgcn_urilocator.cpp - services support for urilocator  --------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This contains code to support hsa UriLocator in hostrpc.
+//
+//===----------------------------------------------------------------------===//
+
+/* Copyright (c) 2023 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+ */
+
+#include "urilocator.h"
+#include <cstdlib>
+#include <fcntl.h>
+#include <sstream>
+#include <sys/stat.h>
+#include <unistd.h>
+
+static bool GetFileHandle(const char *fname, int *fd_ptr, size_t *sz_ptr) {
+  if ((fd_ptr == nullptr) || (sz_ptr == nullptr)) {
+    return false;
+  }
+
+  // open system function call, return false on fail
+  struct stat stat_buf;
+  *fd_ptr = open(fname, O_RDONLY);
+  if (*fd_ptr < 0) {
+    return false;
+  }
+
+  // Retrieve stat info and size
+  if (fstat(*fd_ptr, &stat_buf) != 0) {
+    close(*fd_ptr);
+    return false;
+  }
+
+  *sz_ptr = stat_buf.st_size;
+
+  return true;
+}
+
+hsa_status_t UriLocator::createUriRangeTable() {
+
+  auto execCb = [](hsa_executable_t exec, void *data) -> hsa_status_t {
+    int execState = 0;
+    hsa_status_t status;
+    status =
+        hsa_executable_get_info(exec, HSA_EXECUTABLE_INFO_STATE, &execState);
+    if (status != HSA_STATUS_SUCCESS)
+      return status;
+    if (execState != HSA_EXECUTABLE_STATE_FROZEN)
+      return status;
+
+    auto loadedCodeObjectCb = [](hsa_executable_t exec,
+                                 hsa_loaded_code_object_t lcobj,
+                                 void *data) -> hsa_status_t {
+      hsa_status_t result;
+      uint64_t loadBAddr = 0, loadSize = 0;
+      uint32_t uriLen = 0;
+      int64_t delta = 0;
+      uint64_t *argsCb = static_cast<uint64_t *>(data);
+      hsa_ven_amd_loader_1_03_pfn_t *fnTab =
+          reinterpret_cast<hsa_ven_amd_loader_1_03_pfn_t *>(argsCb[0]);
+      std::vector<UriRange> *rangeTab =
+          reinterpret_cast<std::vector<UriRange> *>(argsCb[1]);
+
+      if (!fnTab->hsa_ven_amd_loader_loaded_code_object_get_info)
+        return HSA_STATUS_ERROR;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_BASE,
+          (void *)&loadBAddr);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_SIZE,
+          (void *)&loadSize);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_URI_LENGTH,
+          (void *)&uriLen);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_DELTA,
+          (void *)&delta);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      char *uri = new char[uriLen + 1];
+      uri[uriLen] = '\0';
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_URI, (void *)uri);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      rangeTab->push_back(UriRange{loadBAddr, loadBAddr + loadSize - 1, delta,
+                                   std::string{uri, uriLen + 1}});
+      delete[] uri;
+      return HSA_STATUS_SUCCESS;
+    };
+
+    uint64_t *args = static_cast<uint64_t *>(data);
+    hsa_ven_amd_loader_1_03_pfn_t *fnExtTab =
+        reinterpret_cast<hsa_ven_amd_loader_1_03_pfn_t *>(args[0]);
+    return fnExtTab->hsa_ven_amd_loader_executable_iterate_loaded_code_objects(
+        exec, loadedCodeObjectCb, data);
+  };
+
+  if (!fn_table_.hsa_ven_amd_loader_iterate_executables)
+    return HSA_STATUS_ERROR;
+
+  uint64_t callbackArgs[2] = {(uint64_t)&fn_table_, (uint64_t)&rangeTab_};
+  return fn_table_.hsa_ven_amd_loader_iterate_executables(execCb,
+                                                          (void *)callbackArgs);
+}
+
+// Encoding of uniform-resource-identifier(URI) is detailed in
+// https://llvm.org/docs/AMDGPUUsage.html#loaded-code-object-path-uniform-resource-identifier-uri
+// The below code currently extracts the uri of loaded code object using
+// file-uri.
+std::pair<uint64_t, uint64_t> UriLocator::decodeUriAndGetFd(UriInfo &uri,
+                                                            int *uri_fd) {
+
+  std::ostringstream ss;
+  char cur;
+  uint64_t offset = 0, size = 0;
+  if (uri.uriPath.size() == 0)
+    return {0, 0};
+  auto pos = uri.uriPath.find("//");
+  if (pos == std::string::npos || uri.uriPath.substr(0, pos) != "file:") {
+    uri.uriPath = "";
+    return {0, 0};
+  }
+  auto rspos = uri.uriPath.find('#');
+  if (rspos != std::string::npos) {
+    // parse range specifier
+    std::string offprefix = "offset=", sizeprefix = "size=";
+    auto sbeg = uri.uriPath.find('&', rspos);
+    auto offbeg = rspos + offprefix.size() + 1;
+    std::string offstr = uri.uriPath.substr(offbeg, sbeg - offbeg);
+    auto sizebeg = sbeg + sizeprefix.size() + 1;
+    std::string sizestr =
+        uri.uriPath.substr(sizebeg, uri.uriPath.size() - sizebeg);
+    offset = std::stoull(offstr, nullptr, 0);
+    size = std::stoull(sizestr, nullptr, 0);
+    rspos -= 1;
+  } else {
+    rspos = uri.uriPath.size() - 1;
+  }
+  pos += 2;
+  // decode filepath
+  for (auto i = pos; i <= rspos;) {
+    cur = uri.uriPath[i];
+    if (isalnum(cur) || cur == '/' || cur == '-' || cur == '_' || cur == '.' ||
+        cur == '~') {
+      ss << cur;
+      i++;
+    } else {
+      // characters prefix with '%' char
+      char tbits = uri.uriPath[i + 1], lbits = uri.uriPath[i + 2];
+      uint8_t t = (tbits < 58) ? (tbits - 48) : ((tbits - 65) + 10);
+      uint8_t l = (lbits < 58) ? (lbits - 48) : ((lbits - 65) + 10);
+      ss << (char)(((0b00000000 | t) << 4) | l);
+      i += 3;
+    }
+  }
+  uri.uriPath = ss.str();
+  size_t fd_size;
+  GetFileHandle(uri.uriPath.c_str(), uri_fd, &fd_size);
+  // As per URI locator syntax, range_specifier is optional
+  // if range_specifier is absent return total size of the file
+  // and set offset to begin at 0.
+  if (size == 0)
+    size = fd_size;
+  return {offset, size};
+}
+
+UriLocator::UriInfo UriLocator::lookUpUri(uint64_t device_pc) {
+  UriInfo errorstate{"", 0};
+
+  if (!init_) {
+
+    hsa_status_t result;
+    result = hsa_system_get_major_extension_table(
+        HSA_EXTENSION_AMD_LOADER, 1, sizeof(fn_table_), &fn_table_);
+    if (result != HSA_STATUS_SUCCESS)
+      return errorstate;
+    result = createUriRangeTable();
+    if (result != HSA_STATUS_SUCCESS) {
+      rangeTab_.clear();
+      return errorstate;
+    }
+    init_ = true;
+  }
+
+  for (auto &seg : rangeTab_)
+    if (seg.startAddr_ <= device_pc && device_pc <= seg.endAddr_)
+      return UriInfo{seg.Uri_.c_str(), seg.elfDelta_};
+
+  return errorstate;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/services/devsanitizer.cpp llvm-project/openmp/libomptarget/hostexec/services/devsanitizer.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/services/devsanitizer.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/services/devsanitizer.cpp	2023-03-15 22:36:45.000000000 -0400
@@ -0,0 +1,143 @@
+//===---- devsanitizer.cpp: Definition of handler for Sanitizer Service ---===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+/*  Copyright (c) 2023 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+ */
+
+#include "execute_service.h"
+#include "urilocator.h"
+#include <algorithm>
+#include <assert.h>   //to exp
+#include <inttypes.h> //to exp
+#include <string>
+#include <tuple>
+#include <vector>
+
+// Address sanitizer runtime entry-function to report the invalid device memory
+// access this will be defined in llvm-project/compiler-rt/lib/asan, and will
+// have effect only when compiler-rt is build for AMDGPU. Note: This API is
+// runtime interface of asan library and only defined for linux os.
+extern "C" void __asan_report_nonself_error(
+    uint64_t *callstack, uint32_t n_callstack, uint64_t *addr, uint32_t naddr,
+    uint64_t *entity_ids, uint32_t n_entities, bool is_write,
+    uint32_t access_size, bool is_abort, const char *name, int64_t vma_adjust,
+    int fd, uint64_t file_extent_size, uint64_t file_extent_start = 0);
+
+namespace {
+extern "C" void handler_SERVICE_SANITIZER(payload_t *packt_payload,
+                                          uint64_t activemask,
+                                          uint32_t gpu_device,
+                                          UriLocator *uri_locator) {
+  // An address results in invalid access in each active lane
+  uint64_t device_failing_addresses[64];
+  // An array of identifications of entities requesting a report.
+  // index 0       - contains device id
+  // index 1,2,3   - contains wg_idx, wg_idy, wg_idz respectively.
+  // index 4 to 67 - contains reporting wave ids in a wave-front.
+  uint64_t entity_id[68], callstack[1];
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  uint32_t n_activelanes = __builtin_popcountl(activemask);
+  uint64_t access_info = 0, access_size = 0;
+  bool is_abort = true;
+#endif
+#endif
+  entity_id[0] = gpu_device;
+
+  assert(packt_payload != nullptr && "packet payload is null?");
+
+  int indx = 0, en_idx = 1;
+  bool first_workitem = false;
+  for (uint32_t wi = 0; wi != 64; ++wi) {
+    uint64_t flag = activemask & ((uint64_t)1 << wi);
+    if (flag == 0)
+      continue;
+
+    auto data_slot = packt_payload->slots[wi];
+    // encoding of packet payload arguments is
+    // defined in device-libs/asanrtl/src/report.cl
+    if (!first_workitem) {
+      device_failing_addresses[indx] = data_slot[0];
+      callstack[0] = data_slot[1];
+      entity_id[en_idx] = data_slot[2];
+      entity_id[++en_idx] = data_slot[3];
+      entity_id[++en_idx] = data_slot[4];
+      entity_id[++en_idx] = data_slot[5];
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+      access_info = data_slot[6];
+      access_size = data_slot[7];
+#endif
+#endif
+      first_workitem = true;
+    } else {
+      device_failing_addresses[indx] = data_slot[0];
+      entity_id[en_idx] = data_slot[5];
+    }
+    indx++;
+    en_idx++;
+  }
+
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  bool is_write = false;
+  if (access_info & 0xFFFFFFFF00000000)
+    is_abort = false;
+  if (access_info & 1)
+    is_write = true;
+#endif
+#endif
+
+  std::string fileuri;
+  uint64_t size = 0, offset = 0;
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  int64_t loadAddrAdjust = 0;
+#endif
+#endif
+  int uri_fd = -1;
+
+  if (uri_locator) {
+    UriLocator::UriInfo fileuri_info = uri_locator->lookUpUri(callstack[0]);
+    std::tie(offset, size) =
+        uri_locator->decodeUriAndGetFd(fileuri_info, &uri_fd);
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+    loadAddrAdjust = fileuri_info.loadAddressDiff;
+#endif
+#endif
+  }
+
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  __asan_report_nonself_error(
+      callstack, 1, device_failing_addresses, n_activelanes, entity_id,
+      n_activelanes + 4, is_write, access_size, is_abort,
+      /*thread key*/ "amdgpu", loadAddrAdjust, uri_fd, size, offset);
+#endif
+#endif
+}
+} // end anonymous namespace
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/services/execute_service.cpp llvm-project/openmp/libomptarget/hostexec/services/execute_service.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/services/execute_service.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/services/execute_service.cpp	2023-03-28 17:56:16.618083145 -0400
@@ -0,0 +1,953 @@
+//===---- execute_service.cpp - support for hostrpc services --------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains architecture independed host code for hostrpc services
+// Calls to hostrpc_execute are serialized by the thread and packet manager.
+// For printf and fprintf, this code reconstructs the host variable argument
+// ABI to support alls to vprintf and vfprintf.  This is facilitated by
+// a robust buffer packaging scheme defined in Clang codegen. The same buffer
+// packaging scheme is used for hostexec fuctions.  Hostexec functions support
+// the launching of host variadic functions from the GPU.
+//
+//===----------------------------------------------------------------------===//
+
+/* MIT License
+
+Copyright © 2023 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is furnished
+to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+*/
+
+#include "execute_service.h"
+#include "../src/hostexec_internal.h"
+#include <assert.h>
+#include <cstring>
+#include <ctype.h>
+#include <stdarg.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <tuple>
+
+// MAXVARGS applies to non-printf vargs functions only.
+#define MAXVARGS 32
+// NUMFPREGS and FPREGSZ are part of x86 vargs ABI that
+// is recreated with the printf support.
+#define NUMFPREGS 8
+#define FPREGSZ 16
+
+typedef int uint128_t __attribute__((mode(TI)));
+struct hostrpc_pfIntRegs {
+  uint64_t rdi, rsi, rdx, rcx, r8, r9;
+};
+typedef struct hostrpc_pfIntRegs hostrpc_pfIntRegs_t; // size = 48 bytes
+
+struct hostrpc_pfRegSaveArea {
+  hostrpc_pfIntRegs_t iregs;
+  uint128_t freg[NUMFPREGS];
+};
+typedef struct hostrpc_pfRegSaveArea
+    hostrpc_pfRegSaveArea_t; // size = 304 bytes
+
+struct hostrpc_ValistExt {
+  uint32_t gp_offset;      /* offset to next available gpr in reg_save_area */
+  uint32_t fp_offset;      /* offset to next available fpr in reg_save_area */
+  void *overflow_arg_area; /* args that are passed on the stack */
+  hostrpc_pfRegSaveArea_t *reg_save_area; /* int and fp registers */
+  size_t overflow_size;
+} __attribute__((packed));
+typedef struct hostrpc_ValistExt hostrpc_ValistExt_t;
+
+/// Prototype for host fallback functions
+// typedef uint32_t hostexec_uint_t(void *, ...);
+// typedef uint64_t hostexec_uint64_t(void *, ...);
+// typedef double   hostexec_double_t(void *, ...);
+
+static service_rc hostrpc_printf(char *buf, size_t bufsz, uint32_t *rc);
+static service_rc hostrpc_fprintf(char *buf, size_t bufsz, uint32_t *rc);
+
+template <typename T, typename FT>
+static service_rc hostexec_service(char *buf, size_t bufsz, T *rc);
+
+static void handler_SERVICE_PRINTF(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  uint uint_value;
+  service_rc rc = hostrpc_printf(device_buffer, bufsz, &uint_value);
+  payload[0] = (uint64_t)uint_value; // what the printf returns
+  payload[1] = (uint64_t)rc;         // Any errors in the service function
+  service_rc rcmem = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)rcmem;
+}
+static void handler_SERVICE_FPRINTF(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  uint uint_value;
+  service_rc rc = hostrpc_fprintf(device_buffer, bufsz, &uint_value);
+  payload[0] = (uint64_t)uint_value; // what the printf returns
+  payload[1] = (uint64_t)rc;         // Any errors in the service function
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)err;
+}
+
+template <typename T, typename TF>
+static void handler_SERVICE_VARFN(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  payload[0] = 0; // zero 64 bits
+  service_rc rc = hostexec_service<T, TF>(device_buffer, bufsz, (T *)payload);
+  // payload[0] has the return value
+  // payload[1] reserved for 128 bit values such as double complex
+  // payload[2-3] reserved for 256 bit return values
+  payload[1] = (uint64_t)rc; // any errors in the service function
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)err; // any errors on memory free
+}
+
+static void handler_SERVICE_HOST_MALLOC(uint32_t device_id,
+                                          uint64_t *payload) {
+  void *ptr = NULL;
+  // CPU device ID 0 is the fine grain memory
+  size_t sz = (size_t)payload[0];
+  service_rc err = host_malloc(&ptr, sz, device_id);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+
+//  SERVICE_MALLOC & SERVICE_FREE are for allocating a heap of device memory
+//  only used by the device to be used for device side malloc and free.
+//  This is called by __ockl_devmem_request. For allocating memory visible
+//  to both host and device user SERVICE_HOST_MALLOC. The corresponding
+//  vargs function will release this
+static void handler_SERVICE_MALLOC(uint32_t device_id, uint64_t *payload) {
+  void *ptr = NULL;
+  size_t sz = (size_t)payload[0];
+  service_rc err = device_malloc(&ptr, sz, device_id);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+
+#if 0
+void fort_ptr_assign_i8(void *arg0, void *arg1, void *arg2, void *arg3, void *arg4) {
+  printf("\n\n ERROR: hostrpc service FTNASSIGN is not functional\n\n");
+};
+service_rc FtnAssignWrapper(void *arg0, void *arg1, void *arg2, void *arg3, void *arg4) {
+  fort_ptr_assign_i8(arg0, arg1, arg2, arg3, arg4);
+  return HSA_STATUS_SUCCESS;
+}
+
+service_rc ftn_assign_wrapper(void *arg0, void *arg1, void *arg2, void *arg3,
+                                void *arg4) {
+  return FtnAssignWrapper(arg0, arg1, arg2, arg3, arg4);
+}
+
+static void handler_SERVICE_FTNASSIGN(uint32_t device_id,
+                                              uint64_t *payload) {
+  void *ptr = NULL;
+  service_rc err = ftn_assign_wrapper((void *)payload[0], (void *)payload[1],
+                                        (void *)payload[2], (void *)payload[3],
+                                        (void *)payload[4]);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+#endif
+
+static void handler_SERVICE_FREE(uint32_t device_id, uint64_t *payload) {
+  char *device_buffer = (char *)payload[0];
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[0] = (uint64_t)err;
+}
+
+// The consumer thread will serialize each active lane and call execute_service
+// for the service request. These services are intended to be architecturally
+// independent.
+void execute_service(uint32_t service_id, uint32_t device_id,
+                     uint64_t *payload) {
+  switch (service_id) {
+  case HOSTEXEC_SID_PRINTF:
+    handler_SERVICE_PRINTF(device_id, payload);
+    break;
+  case HOSTEXEC_SID_FPRINTF:
+    handler_SERVICE_FPRINTF(device_id, payload);
+    break;
+  case HOSTEXEC_SID_VOID:
+    // Cannot return a void in template so just use uint64_t
+    handler_SERVICE_VARFN<uint64_t, hostexec_uint64_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_UINT:
+    handler_SERVICE_VARFN<uint, hostexec_uint_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_UINT64:
+    handler_SERVICE_VARFN<uint64_t, hostexec_uint64_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_DOUBLE:
+    handler_SERVICE_VARFN<double, hostexec_double_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_INT:
+    handler_SERVICE_VARFN<int, hostexec_int_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_LONG:
+    handler_SERVICE_VARFN<long, hostexec_long_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_FLOAT:
+    handler_SERVICE_VARFN<float, hostexec_float_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_HOST_MALLOC:
+    handler_SERVICE_HOST_MALLOC(device_id, payload);
+    break;
+  case HOSTEXEC_SID_DEVICE_MALLOC:
+    handler_SERVICE_MALLOC(device_id, payload);
+    break;
+    //  case HOSTEXEC_SID_FTNASSIGN:
+    //    handler_SERVICE_FTNASSIGN(device_id, payload);
+    //    break;
+  case HOSTEXEC_SID_FREE:
+    handler_SERVICE_FREE(device_id, payload);
+    break;
+  default:
+    fprintf(stderr,"ERROR: hostrpc got a bad service id:%d\n", service_id);
+    thread_abort(_RC_INVALIDSERVICE_ERROR);
+  }
+}
+
+// Support for hostrpc_printf service
+
+// Handle overflow when building the va_list for vprintf
+static service_rc hostrpc_pfGetOverflow(hostrpc_ValistExt_t *valist,
+                                        size_t needsize) {
+  if (needsize < valist->overflow_size)
+    return _RC_SUCCESS;
+
+  // Make the overflow area bigger
+  size_t stacksize;
+  void *newstack;
+  if (valist->overflow_size == 0) {
+    // Make initial save area big to reduce mallocs
+    stacksize = (FPREGSZ * NUMFPREGS) * 2;
+    if (needsize > stacksize)
+      stacksize = needsize; // maybe a big string
+  } else {
+    // Initial save area not big enough, double it
+    stacksize = valist->overflow_size * 2;
+  }
+  if (!(newstack = malloc(stacksize))) {
+    return _RC_STATUS_ERROR;
+  }
+  memset(newstack, 0, stacksize);
+  if (valist->overflow_size) {
+    memcpy(newstack, valist->overflow_arg_area, valist->overflow_size);
+    free(valist->overflow_arg_area);
+  }
+  valist->overflow_arg_area = newstack;
+  valist->overflow_size = stacksize;
+  return _RC_SUCCESS;
+}
+
+// Add an integer to the va_list for vprintf
+static service_rc hostrpc_pfAddInteger(hostrpc_ValistExt_t *valist, char *val,
+                                       size_t valsize, size_t *stacksize) {
+  uint64_t ival;
+  switch (valsize) {
+  case 1:
+    ival = *(uint8_t *)val;
+    break;
+  case 2:
+    ival = *(uint32_t *)val;
+    break;
+  case 4:
+    ival = (*(uint32_t *)val);
+    break;
+  case 8:
+    ival = *(uint64_t *)val;
+    break;
+  default: {
+    return _RC_STATUS_ERROR;
+  }
+  }
+  //  Always copy 8 bytes, sizeof(ival)
+  if ((valist->gp_offset + sizeof(ival)) <= sizeof(hostrpc_pfIntRegs_t)) {
+    memcpy(((char *)valist->reg_save_area + valist->gp_offset), &ival,
+           sizeof(ival));
+    valist->gp_offset += sizeof(ival);
+    return _RC_SUCCESS;
+  }
+  // Ensure valist overflow area is big enough
+  size_t needsize = (size_t)*stacksize + sizeof(ival);
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  // Copy to overflow
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, &ival,
+         sizeof(ival));
+
+  *stacksize += sizeof(ival);
+  return _RC_SUCCESS;
+}
+
+// Add a String argument when building va_list for vprintf
+static service_rc hostrpc_pfAddString(hostrpc_ValistExt_t *valist, char *val,
+                                      size_t strsz, size_t *stacksize) {
+  size_t valsize =
+      sizeof(char *); // ABI captures pointer to string,  not string
+  if ((valist->gp_offset + valsize) <= sizeof(hostrpc_pfIntRegs_t)) {
+    memcpy(((char *)valist->reg_save_area + valist->gp_offset), val, valsize);
+    valist->gp_offset += valsize;
+    return _RC_SUCCESS;
+  }
+  size_t needsize = (size_t)*stacksize + valsize;
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, val,
+         valsize);
+  *stacksize += valsize;
+  return _RC_SUCCESS;
+}
+
+// Add a floating point value when building va_list for vprintf
+static service_rc hostrpc_pfAddFloat(hostrpc_ValistExt_t *valist, char *numdata,
+                                     size_t valsize, size_t *stacksize) {
+  // FIXME, we can used load because doubles are now aligned
+  double dval;
+  if (valsize == 4) {
+    float fval;
+    memcpy(&fval, numdata, 4);
+    dval = (double)fval; // Extend single to double per abi
+  } else if (valsize == 8) {
+    memcpy(&dval, numdata, 8);
+  } else {
+    return _RC_STATUS_ERROR;
+  }
+  if ((valist->fp_offset + FPREGSZ) <= sizeof(hostrpc_pfRegSaveArea_t)) {
+    memcpy(((char *)valist->reg_save_area + (size_t)(valist->fp_offset)), &dval,
+           sizeof(double));
+    valist->fp_offset += FPREGSZ;
+    return _RC_SUCCESS;
+  }
+  size_t needsize = (size_t)*stacksize + sizeof(double);
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, &dval,
+         sizeof(double));
+  // move only by the size of the double (8 bytes)
+  *stacksize += sizeof(double);
+  return _RC_SUCCESS;
+}
+
+// We would like to get llvm typeID enum from Type.h. e.g.
+// #include "../../../../../llvm/include/llvm/IR/Type.h"
+// But we cannot include LLVM headers in a runtime function.
+// So we a have a manual copy of llvm TypeID enum from Type.h
+enum TypeID {
+  // PrimitiveTypes
+  HalfTyID = 0,  ///< 16-bit floating point type
+  BFloatTyID,    ///< 16-bit floating point type (7-bit significand)
+  FloatTyID,     ///< 32-bit floating point type
+  DoubleTyID,    ///< 64-bit floating point type
+  X86_FP80TyID,  ///< 80-bit floating point type (X87)
+  FP128TyID,     ///< 128-bit floating point type (112-bit significand)
+  PPC_FP128TyID, ///< 128-bit floating point type (two 64-bits, PowerPC)
+  VoidTyID,      ///< type with no size
+  LabelTyID,     ///< Labels
+  MetadataTyID,  ///< Metadata
+  X86_MMXTyID,   ///< MMX vectors (64 bits, X86 specific)
+  X86_AMXTyID,   ///< AMX vectors (8192 bits, X86 specific)
+  TokenTyID,     ///< Tokens
+
+  // Derived types... see DerivedTypes.h file.
+  IntegerTyID,        ///< Arbitrary bit width integers
+  FunctionTyID,       ///< Functions
+  PointerTyID,        ///< Pointers
+  StructTyID,         ///< Structures
+  ArrayTyID,          ///< Arrays
+  FixedVectorTyID,    ///< Fixed width SIMD vector type
+  ScalableVectorTyID, ///< Scalable SIMD vector type
+  TypedPointerTyID,   ///< Typed pointer used by some GPU targets
+  TargetExtTyID,      ///< Target extension type
+};
+
+// Build an extended va_list for vprintf by unpacking the buffer
+static service_rc hostrpc_pfBuildValist(hostrpc_ValistExt_t *valist,
+                                        int NumArgs, char *keyptr,
+                                        char *dataptr, char *strptr,
+                                        size_t *data_not_used) {
+  hostrpc_pfRegSaveArea_t *regs;
+  size_t regs_size = sizeof(*regs);
+  regs = (hostrpc_pfRegSaveArea_t *)malloc(regs_size);
+  if (!regs)
+    return _RC_STATUS_ERROR;
+  memset(regs, 0, regs_size);
+  *valist = (hostrpc_ValistExt_t){
+      .gp_offset = 0,
+      .fp_offset = 0,
+      .overflow_arg_area = NULL,
+      .reg_save_area = regs,
+      .overflow_size = 0,
+  };
+
+  size_t num_bytes;
+  size_t bytes_consumed;
+  size_t strsz;
+  size_t fillerNeeded;
+
+  size_t stacksize = 0;
+
+  for (int argnum = 0; argnum < NumArgs; argnum++) {
+    num_bytes = 0;
+    strsz = 0;
+    unsigned int key = *(unsigned int *)keyptr;
+    unsigned int llvmID = key >> 16;
+    unsigned int numbits = (key << 16) >> 16;
+
+    switch (llvmID) {
+    case FloatTyID:  ///<  2: 32-bit floating point type
+    case DoubleTyID: ///<  3: 64-bit floating point type
+    case FP128TyID:  ///<  5: 128-bit floating point type (112-bit mantissa)
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+      if (valist->fp_offset == 0)
+        valist->fp_offset = sizeof(hostrpc_pfIntRegs_t);
+      if (hostrpc_pfAddFloat(valist, dataptr, num_bytes, &stacksize))
+        return _RC_ADDFLOAT_ERROR;
+      break;
+
+    case IntegerTyID: ///< 11: Arbitrary bit width integers
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+      if (hostrpc_pfAddInteger(valist, dataptr, num_bytes, &stacksize))
+        return _RC_ADDINT_ERROR;
+      break;
+
+    case PointerTyID:     ///< 15: Pointers
+      if (numbits == 1) { // This is a pointer to string
+        num_bytes = 4;
+        bytes_consumed = num_bytes;
+        strsz = (size_t) * (unsigned int *)dataptr;
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        if (hostrpc_pfAddString(valist, (char *)&strptr, strsz, &stacksize))
+          return _RC_ADDSTRING_ERROR;
+      } else {
+        num_bytes = 8;
+        bytes_consumed = num_bytes;
+        fillerNeeded = ((size_t)dataptr) % num_bytes;
+        if (fillerNeeded) {
+          dataptr += fillerNeeded; // dataptr is now aligned
+          bytes_consumed += fillerNeeded;
+        }
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        if (hostrpc_pfAddInteger(valist, dataptr, num_bytes, &stacksize))
+          return _RC_ADDINT_ERROR;
+      }
+      break;
+
+    case HalfTyID:           ///<  1: 16-bit floating point type
+    case ArrayTyID:          ///< 14: Arrays
+    case StructTyID:         ///< 13: Structures
+    case FunctionTyID:       ///< 12: Functions
+    case TokenTyID:          ///< 10: Tokens
+    case X86_MMXTyID:        ///<  9: MMX vectors (64 bits, X86 specific)
+    case MetadataTyID:       ///<  8: Metadata
+    case LabelTyID:          ///<  7: Labels
+    case PPC_FP128TyID:      ///<  6: 128-bit floating point type (two 64-bits,
+                             ///<  PowerPC)
+    case X86_FP80TyID:       ///<  4: 80-bit floating point type (X87)
+    case FixedVectorTyID:    ///< 16: Fixed width SIMD vector type
+    case ScalableVectorTyID: ///< 17: Scalable SIMD vector type
+    case TypedPointerTyID:   ///< Typed pointer used by some GPU targets
+    case TargetExtTyID:      ///< Target extension type
+    case VoidTyID:
+      return _RC_UNSUPPORTED_ID_ERROR;
+      break;
+    default:
+      return _RC_INVALID_ID_ERROR;
+    }
+
+    dataptr += num_bytes;
+    strptr += strsz;
+    *data_not_used -= bytes_consumed;
+    keyptr += 4;
+  }
+  return _RC_SUCCESS;
+} // end hostrpc_pfBuildValist
+
+/*
+ *  The buffer to pack arguments for all vargs functions has thes 4 sections:
+ *  1. Header        datalen 4 bytes
+ *                   numargs 4 bytes
+ *  2. Keys          A 4-byte key for each arg including string args
+ *                   Each 4-byte key contains llvmID and numbits to
+ *                   describe the datatype.
+ *  3. args_data     Ths data values for each argument.
+ *                   Each arg is aligned according to its size.
+ *                   If the field is a string
+ *                   the dataptr contains the string length.
+ *  4. strings_data  Exection time string values
+ */
+static service_rc hostrpc_fprintf(char *buf, size_t bufsz, uint *rc) {
+
+  // FIXME: Put the collection of these 6 values in a function
+  //        All service routines that use vargs will need these values.
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  // Skip past the file pointer and format string argument
+  size_t fillerNeeded = ((size_t)dataptr) % 8;
+  if (fillerNeeded)
+    dataptr += fillerNeeded; // dataptr is now aligned on 8 byte
+  // Cannot convert directly to FILE*, so convert to 8-byte size_t first
+  FILE *fileptr = (FILE *)*((size_t *)dataptr);
+  dataptr += sizeof(FILE *); // skip past file ptr
+  NumArgs = NumArgs - 2;
+  keyptr += 8; // All keys are 4 bytes
+  size_t strsz = (size_t) * (unsigned int *)dataptr;
+  dataptr += 4; //  for strings the data value is the size, not a key
+  char *fmtstr = strptr;
+  strptr += strsz;
+  data_not_used -= (sizeof(FILE *) + 4); // 12
+
+  hostrpc_ValistExt_t valist;
+  va_list *real_va_list;
+  real_va_list = (va_list *)&valist;
+
+  if (hostrpc_pfBuildValist(&valist, NumArgs, keyptr, dataptr, strptr,
+                            &data_not_used) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Roll back offsets and save stack pointer
+  valist.gp_offset = 0;
+  valist.fp_offset = sizeof(hostrpc_pfIntRegs_t);
+  void *save_stack = valist.overflow_arg_area;
+
+  *rc = vfprintf(fileptr, fmtstr, *real_va_list);
+
+  if (valist.reg_save_area)
+    free(valist.reg_save_area);
+  if (save_stack)
+    free(save_stack);
+
+  return _RC_SUCCESS;
+}
+//  This the main service routine for printf
+static service_rc hostrpc_printf(char *buf, size_t bufsz, uint *rc) {
+  if (bufsz == 0)
+    return _RC_SUCCESS;
+
+  // Get 6 values needed to unpack the buffer
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  if (NumArgs <= 0)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Skip past the format string argument
+  char *fmtstr = strptr;
+  NumArgs--;
+  keyptr += 4;
+  size_t strsz = (size_t) * (unsigned int *)dataptr;
+  dataptr += 4; // for strings the data value is the size, not a real pointer
+  strptr += strsz;
+  data_not_used -= 4;
+
+  hostrpc_ValistExt_t valist;
+  va_list *real_va_list;
+  real_va_list = (va_list *)&valist;
+
+  if (hostrpc_pfBuildValist(&valist, NumArgs, keyptr, dataptr, strptr,
+                            &data_not_used) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Roll back offsets and save stack pointer for
+  valist.gp_offset = 0;
+  valist.fp_offset = sizeof(hostrpc_pfIntRegs_t);
+  void *save_stack = valist.overflow_arg_area;
+
+  *rc = vprintf(fmtstr, *real_va_list);
+
+  if (valist.reg_save_area)
+    free(valist.reg_save_area);
+  if (save_stack)
+    free(save_stack);
+
+  return _RC_SUCCESS;
+}
+
+//---------------- Support for hostexec_* service ---------------------
+//
+
+// These are the helper functions for hostexec_<TYPE>_ functions
+static uint64_t getuint32(char *val) {
+  uint32_t i32 = *(uint32_t *)val;
+  return (uint64_t)i32;
+}
+static uint64_t getuint64(char *val) { return *(uint64_t *)val; }
+
+static void *getfnptr(char *val) {
+  uint64_t ival = *(uint64_t *)val;
+  return (void *)ival;
+}
+
+// build argument array
+static service_rc hostrpc_build_vargs_array(int NumArgs, char *keyptr,
+                                            char *dataptr, char *strptr,
+                                            size_t *data_not_used,
+                                            uint64_t *a[MAXVARGS]) {
+  size_t num_bytes;
+  size_t bytes_consumed;
+  size_t strsz;
+  size_t fillerNeeded;
+
+  uint argcount = 0;
+
+  for (int argnum = 0; argnum < NumArgs; argnum++) {
+    num_bytes = 0;
+    strsz = 0;
+    unsigned int key = *(unsigned int *)keyptr;
+    unsigned int llvmID = key >> 16;
+    unsigned int numbits = (key << 16) >> 16;
+
+    switch (llvmID) {
+    case FloatTyID:  ///<  2: 32-bit floating point type
+    case DoubleTyID: ///<  3: 64-bit floating point type
+    case FP128TyID:  ///<  5: 128-bit floating point type (112-bit mantissa)
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+
+      if (num_bytes == 4)
+        a[argcount] = (uint64_t *)getuint32(dataptr);
+      else
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+
+      break;
+
+    case IntegerTyID: ///< 11: Arbitrary bit width integers
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+
+      if (num_bytes == 4)
+        a[argcount] = (uint64_t *)getuint32(dataptr);
+      else
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+
+      break;
+
+    case PointerTyID:     ///< 15: Pointers
+      if (numbits == 1) { // This is a pointer to string
+        num_bytes = 4;
+        bytes_consumed = num_bytes;
+        strsz = (size_t) * (unsigned int *)dataptr;
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        a[argcount] = (uint64_t *)((char *)strptr);
+
+      } else {
+        num_bytes = 8;
+        bytes_consumed = num_bytes;
+        fillerNeeded = ((size_t)dataptr) % num_bytes;
+        if (fillerNeeded) {
+          dataptr += fillerNeeded; // dataptr is now aligned
+          bytes_consumed += fillerNeeded;
+        }
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+      }
+      break;
+
+    case HalfTyID:           ///<  1: 16-bit floating point type
+    case ArrayTyID:          ///< 14: Arrays
+    case StructTyID:         ///< 13: Structures
+    case FunctionTyID:       ///< 12: Functions
+    case TokenTyID:          ///< 10: Tokens
+    case X86_MMXTyID:        ///<  9: MMX vectors (64 bits, X86 specific)
+    case MetadataTyID:       ///<  8: Metadata
+    case LabelTyID:          ///<  7: Labels
+    case PPC_FP128TyID:      ///<  6: 128-bit floating point type (two 64-bits,
+                             ///<  PowerPC)
+    case X86_FP80TyID:       ///<  4: 80-bit floating point type (X87)
+    case FixedVectorTyID:    ///< 16: Fixed width SIMD vector type
+    case ScalableVectorTyID: ///< 17: Scalable SIMD vector type
+    case TypedPointerTyID:   ///< Typed pointer used by some GPU targets
+    case TargetExtTyID:      ///< Target extension type
+    case VoidTyID:
+      return _RC_UNSUPPORTED_ID_ERROR;
+      break;
+    default:
+      return _RC_INVALID_ID_ERROR;
+    }
+
+    // Move to next argument
+    dataptr += num_bytes;
+    strptr += strsz;
+    *data_not_used -= bytes_consumed;
+    keyptr += 4;
+    argcount++;
+  }
+  return _RC_SUCCESS;
+}
+
+// Make the vargs function call to the function pointer fnptr
+// by casting fnptr to vfnptr. Return uint32_t
+template <typename T, typename FT>
+static service_rc hostrpc_call_fnptr(uint32_t NumArgs, void *fnptr,
+                                     uint64_t *a[MAXVARGS], T *rv) {
+  //
+  // Currently users are instructed that the first arg must be reserved
+  // for device side to store function pointer. Removing this requirement
+  // is much more difficult that it appears.  One change of many is to
+  // remove fnptr in the call sites below. 2nd is to change the host 
+  // side macro in hostexec.h to remove the fn arg. This results in the
+  // symbol for the variadic function being undefined at GPU link time.
+  // This is because device compilation must ignore variadic function
+  // definitions.
+  //
+  // This is a major design decision which would change the test case.
+  //
+  FT *vfnptr = (FT *)fnptr;
+
+  switch (NumArgs) {
+  case 1:
+    *rv = vfnptr(fnptr, a[0]);
+    break;
+  case 2:
+    *rv = vfnptr(fnptr, a[0], a[1]);
+    break;
+  case 3:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2]);
+    break;
+  case 4:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3]);
+    break;
+  case 5:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4]);
+    break;
+  case 6:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5]);
+    break;
+  case 7:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6]);
+    break;
+  case 8:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7]);
+    break;
+  case 9:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8]);
+    break;
+  case 10:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9]);
+    break;
+  case 11:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10]);
+    break;
+  case 12:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11]);
+    break;
+  case 13:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12]);
+    break;
+  case 14:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13]);
+    break;
+  case 15:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14]);
+    break;
+  case 16:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15]);
+    break;
+  case 17:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16]);
+    break;
+  case 18:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17]);
+    break;
+  case 19:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18]);
+    break;
+  case 20:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19]);
+    break;
+  case 21:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20]);
+    break;
+  case 22:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21]);
+    break;
+  case 23:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22]);
+    break;
+  case 24:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23]);
+    break;
+  case 25:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24]);
+    break;
+  case 26:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25]);
+    break;
+  case 27:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26]);
+    break;
+  case 28:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27]);
+    break;
+  case 29:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28]);
+    break;
+  case 30:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29]);
+    break;
+  case 31:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29], a[30]);
+    break;
+  case 32:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29], a[30], a[31]);
+    break;
+  default:
+    return _RC_EXCEED_MAXVARGS_ERROR;
+  }
+  return _RC_SUCCESS;
+}
+
+template <typename T, typename FT>
+static service_rc hostexec_service(char *buf, size_t bufsz, T *return_value) {
+  if (bufsz == 0)
+    return _RC_SUCCESS;
+
+  // Get 6 values needed to unpack the buffer
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  // skip the function pointer arg including any align buffer
+  if (((size_t)dataptr) % (size_t)8) {
+    dataptr += 4;
+    data_not_used -= 4;
+  }
+  void *fnptr = getfnptr(dataptr);
+  NumArgs--;
+  keyptr += 4;
+  dataptr += 8;
+  data_not_used -= 4;
+
+  if (NumArgs <= 0)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  uint64_t *a[MAXVARGS];
+  if (hostrpc_build_vargs_array(NumArgs, keyptr, dataptr, strptr,
+                                &data_not_used, a) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  if (hostrpc_call_fnptr<T, FT>(NumArgs, fnptr, a, return_value) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  return _RC_SUCCESS;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/services/execute_service.h llvm-project/openmp/libomptarget/hostexec/services/execute_service.h
--- llvm-project.upstream/openmp/libomptarget/hostexec/services/execute_service.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/services/execute_service.h	2023-03-17 13:00:11.771248305 -0400
@@ -0,0 +1,56 @@
+//===---- execute_service.h - header file for execute_service -------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef EXECUTE_SERVICE_H
+#define EXECUTE_SERVICE_H
+
+#include <cstdlib>
+#include <stdint.h>
+
+// Error return codes for service handler functions
+typedef enum service_rc {
+  _RC_SUCCESS = 0,
+  _RC_STATUS_UNKNOWN = 1,
+  _RC_STATUS_ERROR = 2,
+  _RC_STATUS_TERMINATE = 3,
+  _RC_DATA_USED_ERROR = 4,
+  _RC_ADDINT_ERROR = 5,
+  _RC_ADDFLOAT_ERROR = 6,
+  _RC_ADDSTRING_ERROR = 7,
+  _RC_UNSUPPORTED_ID_ERROR = 8,
+  _RC_INVALID_ID_ERROR = 9,
+  _RC_ERROR_INVALID_REQUEST = 10,
+  _RC_EXCEED_MAXVARGS_ERROR = 11,
+  _RC_INVALIDSERVICE_ERROR = 12,
+  _RC_ERROR_MEMFREE = 13,
+  _RC_ERROR_CONSUMER_ACTIVE = 14,
+  _RC_ERROR_CONSUMER_INACTIVE = 15,
+  _RC_ERROR_CONSUMER_LAUNCH_FAILED = 16,
+  _RC_ERROR_SERVICE_UNKNOWN = 17,
+  _RC_ERROR_INCORRECT_ALIGNMENT = 18,
+  _RC_ERROR_NULLPTR = 19,
+  _RC_ERROR_WRONGVERSION = 20,
+  _RC_ERROR_OLDHOSTVERSIONMOD = 21,
+  _RC_ERROR_HSAFAIL = 22,
+  _RC_ERROR_ZEROPACKETS= 23,
+  _RC_ERROR_ALIGNMENT= 24,
+} service_rc;
+
+// helper functions defined in <arch>-hostrpc.cpp used by execute_service
+service_rc host_malloc(void **mem, size_t size, uint32_t device_id);
+service_rc device_malloc(void **mem, size_t size, uint32_t device_id);
+service_rc host_device_mem_free(void *mem);
+void thread_abort(service_rc);
+
+typedef struct {
+  uint64_t slots[64][8];
+} payload_t;
+
+void execute_service(uint32_t service_id, uint32_t devid, uint64_t *payload);
+
+#endif
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/services/urilocator.h llvm-project/openmp/libomptarget/hostexec/services/urilocator.h
--- llvm-project.upstream/openmp/libomptarget/hostexec/services/urilocator.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/services/urilocator.h	2023-03-15 22:36:45.000000000 -0400
@@ -0,0 +1,63 @@
+//===--- UriLocator.h: Schema of URI Locator  -----------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+/* Copyright (c) 2023 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+*/
+
+#ifndef URILOCATOR_H
+#define URILOCATOR_H
+#include "hsa/hsa_ven_amd_loader.h"
+#include <string>
+#include <vector>
+
+class UriLocator {
+
+public:
+  struct UriInfo {
+    std::string uriPath;
+    int64_t loadAddressDiff;
+  };
+
+  struct UriRange {
+    uint64_t startAddr_, endAddr_;
+    int64_t elfDelta_;
+    std::string Uri_;
+  };
+
+  bool init_ = false;
+  std::vector<UriRange> rangeTab_;
+  hsa_ven_amd_loader_1_03_pfn_t fn_table_;
+
+  hsa_status_t createUriRangeTable();
+
+  ~UriLocator() {}
+
+  UriInfo lookUpUri(uint64_t device_pc);
+  std::pair<uint64_t, uint64_t> decodeUriAndGetFd(UriInfo &uri_path,
+                                                  int *uri_fd);
+};
+
+#endif
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec.h llvm-project/openmp/libomptarget/hostexec/src/hostexec.h
--- llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/src/hostexec.h	2023-03-28 17:51:45.431849040 -0400
@@ -0,0 +1,49 @@
+//
+// hostexec.h: Headers for hostexec device stubs
+//
+#ifndef __HOSTEXEC_H__
+#define __HOSTEXEC_H__
+
+#if defined(__cplusplus)
+#define EXTERN extern "C"
+#else
+#define EXTERN extern
+#endif
+
+#include <stdint.h>
+#include <stdio.h>
+
+typedef void hostexec_t(void *, ...);
+typedef uint32_t hostexec_uint_t(void *, ...);
+typedef uint64_t hostexec_uint64_t(void *, ...);
+typedef double hostexec_double_t(void *, ...);
+typedef float hostexec_float_t(void *, ...);
+typedef int hostexec_int_t(void*,...);
+typedef long hostexec_long_t(void *, ...);
+
+#if defined(__NVPTX__) || defined(__AMDGCN__)
+
+// Device interfaces for user-callable hostexec functions
+EXTERN void hostexec(void *fnptr, ...);
+EXTERN uint32_t hostexec_uint(void *fnptr, ...);
+EXTERN uint64_t hostexec_uint64(void *fnptr, ...);
+EXTERN double hostexec_double(void *fnptr, ...);
+EXTERN float hostexec_float(void *fnptr, ...);
+EXTERN int hostexec_int(void *fnptr, ...);
+EXTERN long hostexec_long(void *fnptr, ...);
+
+#else
+
+//  On host pass, simply drop the hostexec wrapper. Technically,
+//  host passes should not see these hostexec functions
+#define hostexec_uint(fn, ...) fn(fn,__VA_ARGS__)
+#define hostexec_uint64(fn, ...) fn(fn,__VA_ARGS__)
+#define hostexec_double(fn, ...) fn(fn,__VA_ARGS__)
+#define hostexec_float(fn, ...) fn(fn,__VA_ARGS__)
+#define hostexec_int(fn, ...) fn(fn,__VA_ARGS__)
+#define hostexec_long(fn, ...) fn(fn,__VA_ARGS__)
+#define hostexec(fn, ...) fn(fn,__VA_ARGS__)
+
+#endif
+
+#endif // __HOSTEXEC_H__
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec_internal.h llvm-project/openmp/libomptarget/hostexec/src/hostexec_internal.h
--- llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec_internal.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/src/hostexec_internal.h	2023-03-24 09:09:06.936939660 -0400
@@ -0,0 +1,112 @@
+#ifndef __HOSTEXEC_INTERNAL_H__
+#define __HOSTEXEC_INTERNAL_H__
+
+/*
+ *   hostexec_internal.h:
+
+MIT License
+
+Copyright © 2020 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is furnished
+to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+*/
+
+#if defined(__cplusplus)
+#define EXTERN extern "C"
+#else
+#define EXTERN extern
+#endif
+
+#define NOINLINE __attribute__((noinline))
+
+#include "hostexec.h"
+#include <stdint.h>
+#include <stdio.h>
+
+//  These are the interfaces for the device stubs emitted 
+//  by EmitHostexecAllocAndExecFns in CGGPUBuiltin.cpp 
+EXTERN char *printf_allocate(uint32_t bufsz);
+EXTERN int printf_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *fprintf_allocate(uint32_t bufsz);
+EXTERN int fprintf_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_allocate(uint32_t bufsz);
+EXTERN void hostexec_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_uint_allocate(uint32_t bufsz);
+EXTERN uint32_t hostexec_uint_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_uint64_allocate(uint32_t bufsz);
+EXTERN uint64_t hostexec_uint64_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_double_allocate(uint32_t bufsz);
+EXTERN double hostexec_double_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_int_allocate(uint32_t bufsz);
+EXTERN int hostexec_int_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_long_allocate(uint32_t bufsz);
+EXTERN long hostexec_long_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_float_allocate(uint32_t bufsz);
+EXTERN float hostexec_float_execute(char *bufptr, uint32_t bufsz);
+
+// This device runtime utility function is needed for variable length strings.
+EXTERN uint32_t __strlen_max(char *instr, uint32_t maxstrlen);
+
+// The version release and patch level (VRM) are embedded in every packet with
+// the service id (sid) and checked by the host runtime on the first packet.
+// The runtime fails if VR is different or device VRM > host VRM.
+// Runtime warns if device VRM < host VRM i.e old compiler with newer runtime.
+// See check_version in services/<arch>_hostexec.cpp.
+#define HOSTEXEC_VERSION 0
+#define HOSTEXEC_RELEASE 1
+#define HOSTEXEC_PATCH 0
+// HOSTEXEC_VRM uses 2 bytes allowing 64 patches, 64 releases, 15 versions
+#define HOSTEXEC_VRM ((HOSTEXEC_VERSION * 4096) + \
+		(HOSTEXEC_RELEASE * 64) + HOSTEXEC_PATCH) 
+#define HOSTEXEC_VERSION_RELEASE ((HOSTEXEC_VERSION * 64) + HOSTEXEC_RELEASE)
+
+// This macro packs VRM and sid id into the 1st 4 bytes of the packet.
+#define PACK_VERS(x) ((uint32_t)HOSTEXEC_VRM << 16) | ((uint32_t)x)
+
+// The host runtime for host services is linked statically into the 
+// libomptarget plugin i.e.  libomptarget.rtl.amdgcn or libomptarget.rtl.cuda 
+// Typically these are part of the compiler installation so VRM checking
+// would not be necessary. However, compiled applications that dynamically
+// link to libomptarget and the plugin may might get an old runtime.
+// so VRM checking is 
+//
+// Please update at least the patch level when adding a new service id (sid)
+// below. This ensures that applications that use a new device stub do not
+// try to use backlevel host runtimes that do not have a valid VRM.
+enum hostexec_sid {
+  HOSTEXEC_SID_UNUSED,
+  HOSTEXEC_SID_TERMINATE,
+  HOSTEXEC_SID_DEVICE_MALLOC, // Device global memory
+  HOSTEXEC_SID_HOST_MALLOC,   // shared or managed memory
+  HOSTEXEC_SID_FREE,
+  HOSTEXEC_SID_PRINTF,
+  HOSTEXEC_SID_FPRINTF,
+  HOSTEXEC_SID_FTNASSIGN,
+  HOSTEXEC_SID_SANITIZER,
+  HOSTEXEC_SID_UINT,
+  HOSTEXEC_SID_UINT64,
+  HOSTEXEC_SID_DOUBLE,
+  HOSTEXEC_SID_INT,
+  HOSTEXEC_SID_LONG,
+  HOSTEXEC_SID_FLOAT,
+  HOSTEXEC_SID_VOID,
+};
+
+#endif // __HOSTEXEC_INTERNAL_H__
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec_invoke.cpp llvm-project/openmp/libomptarget/hostexec/src/hostexec_invoke.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec_invoke.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/src/hostexec_invoke.cpp	2023-03-23 22:51:37.492730628 -0400
@@ -0,0 +1,502 @@
+
+#include <stdint.h>
+
+#define GLOB_ATTR __attribute__((address_space(1)))
+#define __static_inl static __attribute__((flatten, always_inline))
+#define __inl __attribute__((flatten, always_inline))
+
+// mem order codes: A=acquire, X=relaxed, R=release
+
+// headers for amdgcn opencl atomics
+extern "C" __inl uint64_t oclAtomic64Load_A(GLOB_ATTR uint64_t *Address);
+extern "C" __inl uint64_t oclAtomic64Load_X(GLOB_ATTR uint64_t *Address);
+extern "C" __inl uint32_t oclAtomic32Load_A(GLOB_ATTR const uint32_t *Address);
+extern "C" __inl bool oclAtomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                                  uint64_t new_ptr);
+extern "C" __inl bool oclAtomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                                  uint64_t new_ptr);
+
+// headers for cuda nvptx atomics
+extern "C" __attribute__((nothrow)) unsigned long long
+__ullAtomicAdd_system(unsigned long long *address, unsigned long long val);
+extern "C" __attribute__((nothrow)) unsigned long long
+__ullAtomicCAS_system(unsigned long long int *address,
+                      unsigned long long int compare,
+                      unsigned long long int val);
+// headers for builtins
+int __builtin_popcountl(unsigned long);
+int __builtin_popcount(unsigned);
+int __builtin_amdgcn_readfirstlane(int);
+unsigned long __builtin_amdgcn_read_exec();
+unsigned int __builtin_amdgcn_mbcnt_hi(unsigned int, unsigned int);
+unsigned int __builtin_amdgcn_mbcnt_lo(unsigned int, unsigned int);
+int __nvvm_read_ptx_sreg_tid_x();
+
+// We need __ockl_hsa_signal_add and __ockl_lane_u32 from ockl
+typedef uint64_t hsa_signal_value_t;
+typedef uint64_t hsa_signal_t;
+// typedef struct hsa_signal_s {
+// uint64_t handle;
+//} hsa_signal_t;
+typedef enum __ockl_memory_order_e {
+  __ockl_memory_order_relaxed = __ATOMIC_RELAXED,
+  __ockl_memory_order_acquire = __ATOMIC_ACQUIRE,
+  __ockl_memory_order_release = __ATOMIC_RELEASE,
+  __ockl_memory_order_acq_rel = __ATOMIC_ACQ_REL,
+  __ockl_memory_order_seq_cst = __ATOMIC_SEQ_CST,
+} __ockl_memory_order;
+extern "C" void __ockl_hsa_signal_add(hsa_signal_t signal,
+                                      hsa_signal_value_t value,
+                                      __ockl_memory_order mo);
+extern "C" uint32_t __ockl_lane_u32();
+
+#pragma omp begin declare target device_type(nohost)
+
+typedef enum { STATUS_SUCCESS, STATUS_BUSY } status_t;
+
+typedef enum {
+  CONTROL_OFFSET_READY_FLAG = 0,
+  CONTROL_OFFSET_RESERVED0 = 1,
+} control_offset_t;
+
+typedef enum {
+  CONTROL_WIDTH_READY_FLAG = 1,
+  CONTROL_WIDTH_RESERVED0 = 31,
+} control_width_t;
+
+typedef uint64_t LaneMask_t;
+
+typedef struct {
+  uint64_t next;
+  LaneMask_t activemask;
+  uint32_t service;
+  uint32_t control;
+} header_t;
+
+typedef struct {
+  // 64 slots of 8 uint64_ts each (4KB/payload)
+  uint64_t slots[64][8];
+} payload_t;
+
+typedef struct {
+  GLOB_ATTR header_t *headers;
+  GLOB_ATTR payload_t *payloads;
+  hsa_signal_t doorbell;
+  uint64_t free_stack;
+  uint64_t ready_stack;
+  uint32_t index_size;
+  uint32_t device_id;
+} buffer_t;
+
+namespace impl {
+
+// These functions have arch-specific variants
+__inl void deviceSleepHostWait();
+//  These still dont have nvptx variants
+__inl void send_signal(hsa_signal_t signal);
+__inl uint32_t first_lane_id(uint32_t val);
+__inl uint32_t lane_id();
+__inl uint64_t get_mask();
+__inl uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address);
+__inl uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address);
+__inl uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address);
+__inl bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                    uint64_t new_ptr);
+__inl bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                    uint64_t new_ptr);
+__inl void write_needs_host_services_symbol();
+
+#pragma omp begin declare variant match(device = {arch(amdgcn)})
+
+__static_inl void write_needs_host_services_symbol() {
+  // The global variable "__needs_host_services" is used to detect that
+  // host services are required. If hostexec_invoke is not called, the symbol
+  // will not be present and the runtime can avoid allocating and initialising
+  // service_thread_buf.
+  __asm__( ".type __needs_host_services,@object\n\t"
+          ".global __needs_host_services\n\t"
+          ".comm __needs_host_services,4" ::
+              :);
+}
+
+__static_inl uint32_t lane_id() {
+  return __builtin_amdgcn_mbcnt_hi(~0u, __builtin_amdgcn_mbcnt_lo(~0u, 0u));
+};
+
+__static_inl uint32_t first_lane_id(uint32_t me) {
+  return __builtin_amdgcn_readfirstlane(me);
+}
+__static_inl uint64_t get_mask() { return __builtin_amdgcn_read_exec(); }
+__static_inl void send_signal(hsa_signal_t signal) {
+  __ockl_hsa_signal_add(signal, 1, __ockl_memory_order_release);
+}
+__static_inl void deviceSleepHostWait() { __builtin_amdgcn_s_sleep(1); }
+__static_inl uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address) {
+  return oclAtomic64Load_A(Address);
+}
+__static_inl uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address) {
+  return oclAtomic64Load_X(Address);
+}
+__static_inl uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address) {
+  return oclAtomic32Load_A(Address);
+}
+__static_inl bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                           uint64_t new_ptr) {
+  return oclAtomic64CAS_AX(Address, e_Val, new_ptr);
+}
+__static_inl bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                           uint64_t new_ptr) {
+  return oclAtomic64CAS_RX(Address, e_Val, new_ptr);
+}
+
+#pragma omp end declare variant
+
+#pragma omp begin declare variant match(                                       \
+    device = {arch(nvptx, nvptx64)}, implementation = {extension(match_any)})
+
+__static_inl void write_needs_host_services_symbol() {
+  // The global variable "__needs_host_services" is used to detect that
+  // host services are required. If hostexec_invoke is not called, the symbol
+  // will not be present and the runtime can avoid allocating and initialising
+  // service_thread_buf.
+  __asm__(".global .align 4 .u32 __needs_host_services = 1;");
+}
+__static_inl inline void send_signal(hsa_signal_t signal) {
+  __ullAtomicAdd_system((unsigned long long *)signal, 1);
+}
+
+__static_inl void deviceSleepHostWait() {
+  int32_t start = __nvvm_read_ptx_sreg_clock();
+  for (;;) {
+    if ((__nvvm_read_ptx_sreg_clock() - start) >= 1000)
+      break;
+  }
+}
+
+__static_inl uint64_t get_mask() {
+  unsigned int Mask;
+  asm("activemask.b32 %0;" : "=r"(Mask));
+  uint64_t mask64 = (uint64_t)Mask;
+  return mask64;
+}
+
+//  FIXME: nvptx needs to use lane_id somehow here
+__static_inl uint32_t first_lane_id(unsigned int lane_id) {
+  unsigned int mask = (unsigned int)get_mask();
+  if (mask == 0)
+    return 0;
+  unsigned int pos = 0;
+  unsigned int m = 1;
+  while (!(mask & m)) {
+    m = m << 1;
+    pos++;
+  }
+  return pos;
+};
+
+__static_inl uint32_t lane_id() {
+  return (uint32_t)(__nvvm_read_ptx_sreg_tid_x() & 31);
+};
+
+__static_inl uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address) {
+  unsigned long long result =
+      __ullAtomicAdd_system((unsigned long long *)Address, 0);
+  return (uint64_t)result;
+}
+__static_inl uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address) {
+  unsigned long long result =
+      __ullAtomicAdd_system((unsigned long long *)Address, 0);
+  return (uint64_t)result;
+}
+__static_inl uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address) {
+  return __uAtomicAdd((uint32_t *)Address, 0);
+}
+__static_inl bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                           uint64_t new_ptr) {
+  unsigned long long result = __ullAtomicCAS_system(
+      (unsigned long long *)Address, (unsigned long long)*e_Val,
+      (unsigned long long)new_ptr);
+  return (bool)result;
+}
+__static_inl bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                           uint64_t new_ptr) {
+  unsigned long long result = __ullAtomicCAS_system(
+      (unsigned long long *)Address, (unsigned long long)*e_Val,
+      (unsigned long long)new_ptr);
+  return (bool)result;
+}
+
+#pragma omp end declare variant
+
+} // end namespace impl
+
+
+__static_inl uint64_t get_ptr_index(uint64_t ptr, uint32_t index_size) {
+  return ptr & (((uint64_t)1 << index_size) - 1);
+}
+
+__static_inl GLOB_ATTR header_t *get_header(GLOB_ATTR buffer_t *buffer,
+                                      uint64_t ptr) {
+  return buffer->headers + get_ptr_index(ptr, buffer->index_size);
+}
+
+__static_inl GLOB_ATTR payload_t *get_payload(GLOB_ATTR buffer_t *buffer,
+                                        uint64_t ptr) {
+  return buffer->payloads + get_ptr_index(ptr, buffer->index_size);
+}
+
+// get_control_field only used by get_ready_flag
+__static_inl uint32_t get_control_field(uint32_t control, uint32_t offset,
+                                  uint32_t width) {
+  return (control >> offset) & ((1 << width) - 1);
+}
+
+// get_ready_flag only called by lead lane of get_return_value
+//                on atomically loaded control field of packet header
+__static_inl uint32_t get_ready_flag(uint32_t control) {
+  return get_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                           CONTROL_WIDTH_READY_FLAG);
+}
+
+// set_control_field only used by set_ready_flag
+__static_inl uint32_t set_control_field(uint32_t control, uint32_t offset,
+                                  uint32_t width, uint32_t value) {
+  uint32_t mask = ~(((1 << width) - 1) << offset);
+  return (control & mask) | (value << offset);
+}
+
+__static_inl uint32_t set_ready_flag(uint32_t control) {
+  return set_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                           CONTROL_WIDTH_READY_FLAG, 1);
+}
+
+__static_inl uint64_t pop(GLOB_ATTR uint64_t *top, GLOB_ATTR buffer_t *buffer) {
+  uint64_t F = impl::atomic64Load_A(top);
+  // F is guaranteed to be non-zero, since there are at least as
+  // many packets as there are waves, and each wave can hold at most
+  // one packet.
+  while (true) {
+    GLOB_ATTR header_t *P = get_header(buffer, F);
+    uint64_t N = impl::atomic64Load_X(&P->next);
+    if (impl::atomic64CAS_AX(top, &F, N))
+      break;
+    impl::deviceSleepHostWait();
+  }
+
+  return F;
+}
+
+/** \brief Use the first active lane to get a free packet and
+ *         broadcast to the whole wave.
+ */
+__static_inl uint64_t pop_free_stack(GLOB_ATTR buffer_t *buffer, uint32_t me,
+                               uint32_t low) {
+  uint64_t packet_ptr = 0;
+  if (me == low) {
+    packet_ptr = pop(&buffer->free_stack, buffer);
+  }
+
+  uint32_t ptr_lo = packet_ptr;
+  uint32_t ptr_hi = packet_ptr >> 32;
+  ptr_lo = impl::first_lane_id(ptr_lo);
+  ptr_hi = impl::first_lane_id(ptr_hi);
+
+  return ((uint64_t)ptr_hi << 32) | ptr_lo;
+}
+
+__static_inl void push(GLOB_ATTR uint64_t *top, uint64_t ptr,
+                 GLOB_ATTR buffer_t *buffer) {
+  uint64_t F = impl::atomic64Load_X(top);
+  GLOB_ATTR header_t *P = get_header(buffer, ptr);
+
+  while (true) {
+    P->next = F;
+    if (impl::atomic64CAS_RX(top, &F, ptr))
+      break;
+    impl::deviceSleepHostWait();
+  }
+}
+
+/** \brief Use the first active lane in a wave to submit a ready
+ *         packet and signal the host.
+ */
+__static_inl void push_ready_stack(GLOB_ATTR buffer_t *buffer, uint64_t ptr,
+                             uint32_t me, uint32_t low) {
+  if (me == low) {
+    push(&buffer->ready_stack, ptr, buffer);
+    impl::send_signal(buffer->doorbell);
+  }
+}
+
+__static_inl uint64_t inc_ptr_tag(uint64_t ptr, uint32_t index_size) {
+  // Unit step for the tag.
+  uint64_t inc = 1UL << index_size;
+  ptr += inc;
+  // When the tag for index 0 wraps, increment the tag.
+  return ptr == 0 ? inc : ptr;
+}
+
+/** \brief Return the packet after incrementing the ABA tag
+ */
+__static_inl void return_free_packet(GLOB_ATTR buffer_t *buffer, uint64_t ptr,
+                               uint32_t me, uint32_t low) {
+  if (me == low) {
+    ptr = inc_ptr_tag(ptr, buffer->index_size);
+    push(&buffer->free_stack, ptr, buffer);
+  }
+}
+
+void __static_inl fill_packet(GLOB_ATTR header_t *header,
+                        GLOB_ATTR payload_t *payload, uint32_t service_id,
+                        uint64_t arg0, uint64_t arg1, uint64_t arg2,
+                        uint64_t arg3, uint64_t arg4, uint64_t arg5,
+                        uint64_t arg6, uint64_t arg7, uint32_t me,
+                        uint32_t low) {
+  uint64_t active = impl::get_mask();
+  if (me == low) {
+    header->service = service_id;
+    header->activemask = active;
+    uint32_t control = set_ready_flag(0);
+    header->control = control;
+  }
+  GLOB_ATTR uint64_t *ptr = payload->slots[me];
+  ptr[0] = arg0;
+  ptr[1] = arg1;
+  ptr[2] = arg2;
+  ptr[3] = arg3;
+  ptr[4] = arg4;
+  ptr[5] = arg5;
+  ptr[6] = arg6;
+  ptr[7] = arg7;
+}
+
+//  result is 8*8=64 bytes per lane
+//  Total payload could be 64 lanes * 64 bytes = 4KB
+typedef struct {
+  uint64_t arg0;
+  uint64_t arg1;
+  uint64_t arg2;
+  uint64_t arg3;
+  uint64_t arg4;
+  uint64_t arg5;
+  uint64_t arg6;
+  uint64_t arg7;
+} hostexec_result_t;
+
+/** \brief Wait for the host response and return the first two uint64_t
+ *         entries per workitem.
+ *
+ *  After the packet is submitted in READY state, the wave spins until
+ *  the host changes the state to DONE. Each workitem reads the first
+ *  two uint64_t elements in its slot and returns this.
+ */
+__static_inl hostexec_result_t get_return_value(GLOB_ATTR header_t *header,
+                                         GLOB_ATTR payload_t *payload,
+                                         uint32_t me, uint32_t low) {
+  // The while loop needs to be executed by all active
+  // lanes. Otherwise, later reads from ptr are performed only by
+  // the first thread, while other threads reuse a value cached from
+  // previous operations. The use of readfirstlane in the while loop
+  // prevents this reordering.
+  //
+  // In the absence of the readfirstlane, only one thread has a
+  // sequenced-before relation from the atomic load on
+  // header->control to the ordinary loads on ptr. As a result, the
+  // compiler is free to reorder operations in such a way that the
+  // ordinary loads are performed only by the first thread. The use
+  // of readfirstlane provides a stronger code-motion barrier, and
+  // it effectively "spreads out" the sequenced-before relation to
+  // the ordinary stores in other threads too.
+  while (true) {
+    uint32_t ready_flag = 1;
+    if (me == low) {
+      uint32_t control =
+          impl::atomic32Load_A((GLOB_ATTR uint32_t *)&header->control);
+      ready_flag = get_ready_flag(control);
+    }
+    ready_flag = impl::first_lane_id(ready_flag);
+    if (ready_flag == 0)
+      break;
+    impl::deviceSleepHostWait();
+  }
+
+  GLOB_ATTR uint64_t *ptr = (GLOB_ATTR uint64_t *)(payload->slots + me);
+  hostexec_result_t retval;
+  retval.arg0 = *ptr++;
+  retval.arg1 = *ptr++;
+  retval.arg2 = *ptr++;
+  retval.arg3 = *ptr++;
+  retval.arg4 = *ptr++;
+  retval.arg5 = *ptr++;
+  retval.arg6 = *ptr++;
+  retval.arg7 = *ptr;
+
+  return retval;
+}
+
+#undef __static_inl
+
+/** \brief The implementation that should be hidden behind an ABI
+ *
+ *  The transaction is a wave-wide operation, where the service_id
+ *  must be uniform, but the parameters are different for each
+ *  workitem. Parameters from all active lanes are written into a
+ *  hostcall packet. The hostcall blocks until the host processes the
+ *  request, and returns the response it receiveds.
+ *
+ *  TODO: This function and everything above it should eventually move
+ *  to a separate library that is loaded by the language runtime. The
+ *  function itself will be exposed as an orindary function symbol to
+ *  be linked into kernel objects that are loaded after this library.
+ */
+
+//  service_thread_buf is a global constant symbol that contains the
+//  pointer to the buffer used by the service thread. This is written
+//  by nextgen plugin method writeGlobalToDevice only when the device
+//  image requires host services.
+//  This is the alternative to using reserved IMPLICIT kern arg hostcall
+//  because nvptx arch does not have implicit kern args.
+uint64_t [[clang::address_space(4)]] service_thread_buf
+    [[clang::loader_uninitialized]]
+    __attribute__((used, retain, weak, visibility("protected")));
+
+extern "C" __attribute__((noinline)) hostexec_result_t
+hostexec_invoke(const uint32_t service_id, uint64_t arg0, uint64_t arg1, uint64_t arg2,
+               uint64_t arg3, uint64_t arg4, uint64_t arg5, uint64_t arg6,
+               uint64_t arg7) {
+  impl::write_needs_host_services_symbol();
+  uint32_t me = impl::lane_id();
+  uint32_t low = impl::first_lane_id(me);
+
+  GLOB_ATTR buffer_t *buffer = (GLOB_ATTR buffer_t *)service_thread_buf;
+
+#ifdef __AMDGCN__
+  // Temporary hack to support the old amdgcn plugin. When running the
+  // old plugin, the global service_thread_buf will be null. So get value
+  // from implicit kernel args set by the old plugin. The location in
+  // the implicit kernel args depends on which abi version is in use.
+  if (!buffer){
+    const size_t* argptr = (const size_t *)__builtin_amdgcn_implicitarg_ptr();
+#ifndef __oclc_ABI_version
+#define __oclc_ABI_version 500
+#endif
+#if ( __oclc_ABI_version < 500 )
+    buffer = (GLOB_ATTR buffer_t *) argptr[3];
+#else
+    buffer = (GLOB_ATTR buffer_t *) argptr[10];
+#endif
+  }
+#endif
+
+  uint64_t packet_ptr = pop_free_stack(buffer, me, low);
+  GLOB_ATTR header_t *header = get_header(buffer, packet_ptr);
+  GLOB_ATTR payload_t *payload = get_payload(buffer, packet_ptr);
+  fill_packet(header, payload, service_id, arg0, arg1, arg2, arg3, arg4, arg5,
+              arg6, arg7, me, low);
+  push_ready_stack(buffer, packet_ptr, me, low);
+  hostexec_result_t retval = get_return_value(header, payload, me, low);
+  return_free_packet(buffer, packet_ptr, me, low);
+  return retval;
+}
+
+#pragma omp end declare target
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec_stubs.cpp llvm-project/openmp/libomptarget/hostexec/src/hostexec_stubs.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/src/hostexec_stubs.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/src/hostexec_stubs.cpp	2023-03-24 09:09:23.872865374 -0400
@@ -0,0 +1,224 @@
+///
+///  hostexec_stubs.cpp: definitions of device stubs
+///
+// GPUs typically do not support vargs style functions.  So to implement
+// printf or any vargs function as a hostexec service requires the compiler
+// to generate code to allocate a buffer, fill the buffer with the value of
+// each argument, and then call a stub to execute the service with a pointer to
+// the buffer. The clang compiler does this in the CGGPUBuiltin.cpp source.
+// Here we define printf_allocate and printf_execute device functions that are
+// generated by the clang compiler when it encounters a printf statement.
+// printf_allocate is implemented as a hostexec stub. We assume that the
+// host routine for printf_execute will free the buffer that was allocated
+// by printf_allocate.
+
+#include "hostexec_internal.h"
+#include <stdint.h>
+#include <stdio.h>
+
+#pragma omp declare target
+
+// #pragma omp begin declare variant match(device = {kind(gpu)})
+
+typedef struct hostexec_result_s {
+  uint64_t arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7;
+} hostexec_result_t;
+
+extern "C" hostexec_result_t hostexec_invoke(const uint32_t id, uint64_t arg0,
+                                           uint64_t arg1, uint64_t arg2,
+                                           uint64_t arg3, uint64_t arg4,
+                                           uint64_t arg5, uint64_t arg6,
+                                           uint64_t arg7);
+
+
+static __attribute__((flatten, always_inline)) hostexec_result_t
+hostexec_invoke_zeros(const uint32_t id, uint64_t arg0 = 0, uint64_t arg1 = 0,
+                     uint64_t arg2 = 0, uint64_t arg3 = 0, uint64_t arg4 = 0,
+                     uint64_t arg5 = 0, uint64_t arg6 = 0, uint64_t arg7 = 0) {
+  return hostexec_invoke(id, arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7);
+}
+
+extern "C" {
+// This definition of __ockl_devmem_request and __ockl_sanitizer_report needs to
+// override the weak symbol for __ockl_devmem_request and
+// __ockl_sanitizer_report in rocm device lib ockl.bc because ockl uses
+// hostcall but OpenMP uses hostexec.
+__attribute__((noinline)) uint64_t __ockl_devmem_request(uint64_t addr,
+                                                         uint64_t size) {
+  uint64_t arg0;
+  if (size) { // allocation request
+    arg0 = size;
+    hostexec_result_t result =
+        hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_DEVICE_MALLOC), arg0);
+    return result.arg1;
+  } else { // free request
+    arg0 = addr;
+    hostexec_result_t result =
+        hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_FREE), arg0);
+    return result.arg0;
+  }
+}
+
+void __ockl_sanitizer_report(uint64_t addr, uint64_t pc, uint64_t wgidx,
+                             uint64_t wgidy, uint64_t wgidz, uint64_t wave_id,
+                             uint64_t is_read, uint64_t access_size) {
+  hostexec_result_t result =
+      hostexec_invoke(PACK_VERS(HOSTEXEC_SID_SANITIZER), addr, pc, wgidx,
+                     wgidy, wgidz, wave_id, is_read, access_size);
+}
+void f90print_(char *s) { printf("%s\n", s); }
+void f90printi_(char *s, int *i) { printf("%s %d\n", s, *i); }
+void f90printl_(char *s, long *i) { printf("%s %ld\n", s, *i); }
+void f90printf_(char *s, float *f) { printf("%s %f\n", s, *f); }
+void f90printd_(char *s, double *d) { printf("%s %g\n", s, *d); }
+
+char *printf_allocate(uint32_t bufsz) {
+  uint64_t arg0 = (uint64_t)bufsz;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_HOST_MALLOC), arg0);
+  return (char *)result.arg1;
+}
+int printf_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_PRINTF), arg0, arg1);
+  return (int)result.arg0;
+}
+
+char *hostexec_allocate(uint32_t bufsz) {
+  uint64_t arg0 = (uint64_t)bufsz;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_HOST_MALLOC), arg0);
+  return (char *)result.arg1;
+}
+
+char *hostexec_uint_allocate(uint32_t bufsz) {
+  return hostexec_allocate(bufsz);
+}
+char *hostexec_uint64_allocate(uint32_t bufsz) {
+  return hostexec_allocate(bufsz);
+}
+char *hostexec_int_allocate(uint32_t bufsz) { return hostexec_allocate(bufsz); }
+char *hostexec_long_allocate(uint32_t bufsz) {
+  return hostexec_allocate(bufsz);
+}
+char *hostexec_double_allocate(uint32_t bufsz) {
+  return hostexec_allocate(bufsz);
+}
+char *hostexec_float_allocate(uint32_t bufsz) {
+  return hostexec_allocate(bufsz);
+}
+
+char *fprintf_allocate(uint32_t bufsz) {
+  uint64_t arg0 = (uint64_t)bufsz;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_HOST_MALLOC), arg0);
+  return (char *)result.arg1;
+}
+int fprintf_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_FPRINTF), arg0, arg1);
+  return (int)result.arg0;
+}
+
+#if 0
+uint64_t __tgt_fort_ptr_assn_i8(void *varg0, void *varg1, void *varg2,
+                                void *varg3, void *varg4) {
+  uint64_t arg0, arg1, arg2, arg3, arg4;
+  arg0 = (uint64_t)varg0;
+  arg1 = (uint64_t)varg1;
+  arg2 = (uint64_t)varg2;
+  arg3 = (uint64_t)varg3;
+  arg4 = (uint64_t)varg4;
+  hostexec_result_t result = hostexec_invoke_zeros(
+      PACK_VERS(HOSTEXEC_SID_FTNASSIGN), arg0, arg1, arg2, arg3, arg4);
+  return (uint64_t)result.arg0;
+}
+#endif
+
+void hostexec_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_VOID), arg0, arg1);
+  // hostexec() is a user-callable void function that returns nothing.
+  return;
+}
+uint32_t hostexec_uint_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_UINT), arg0, arg1);
+  return (uint32_t)result.arg0;
+}
+uint64_t hostexec_uint64_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_UINT64), arg0, arg1);
+  return (uint64_t)result.arg0;
+}
+double hostexec_double_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_DOUBLE), arg0, arg1);
+  union {
+    uint64_t val;
+    double dval;
+  } unionarg;
+  unionarg.val = result.arg0;
+  return unionarg.dval;
+}
+int hostexec_int_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_INT), arg0, arg1);
+  return (int)result.arg0;
+}
+float hostexec_float_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_FLOAT), arg0, arg1);
+  union {
+    float fval[2];
+    uint64_t val;
+  } unionarg;
+  unionarg.val = result.arg0;
+  return unionarg.fval[0];
+}
+long hostexec_long_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(PACK_VERS(HOSTEXEC_SID_LONG), arg0, arg1);
+  return (long)result.arg0;
+}
+
+// This function is used for printf arguments that are variable length strings
+// The clang compiler will generate calls to this only when a string length is
+// not a compile time constant.
+uint32_t __strlen_max(char *instr, uint32_t maxstrlen) {
+  for (uint32_t i = 0; i < maxstrlen; i++)
+    if (instr[i] == (char)0)
+      return (uint32_t)(i + 1);
+  return maxstrlen;
+}
+
+} // end extern "C"
+
+#pragma omp end declare target
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/src/oclAtomics.cl llvm-project/openmp/libomptarget/hostexec/src/oclAtomics.cl
--- llvm-project.upstream/openmp/libomptarget/hostexec/src/oclAtomics.cl	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/src/oclAtomics.cl	2023-03-23 21:30:25.855823718 -0400
@@ -0,0 +1,27 @@
+
+#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
+#pragma OPENCL EXTENSION cl_khr_int64_extended_atomics : enable
+
+#define __atomic_ulong atomic_ulong
+#define __inl __attribute__((flatten, always_inline))
+
+// mem order codes: A=acquire, X=relaxed, R=release
+extern __inl ulong oclAtomic64Load_A(__global __atomic_ulong * Address){
+  return  __opencl_atomic_load(Address, memory_order_acquire, memory_scope_all_svm_devices);
+}
+extern __inl ulong oclAtomic64Load_X(__global __atomic_ulong * Address){
+  return  __opencl_atomic_load(Address, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+extern __inl uint oclAtomic32Load_A(__global const atomic_uint * Address){
+  return  __opencl_atomic_load(Address, memory_order_acquire, memory_scope_all_svm_devices);
+}
+extern __inl int oclAtomic64CAS_AX(__global __atomic_ulong * Address,  ulong * e_val, ulong new_val) {
+   return __opencl_atomic_compare_exchange_strong( Address, e_val, new_val,
+     memory_order_acquire, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+extern __inl int oclAtomic64CAS_RX(__global __atomic_ulong * Address,  ulong * e_val, ulong new_val) {
+   return __opencl_atomic_compare_exchange_strong(Address, e_val, new_val,
+     memory_order_release, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+#undef __atomic_ulong
+#undef __inl
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/test/compile_execute_cmds llvm-project/openmp/libomptarget/hostexec/test/compile_execute_cmds
--- llvm-project.upstream/openmp/libomptarget/hostexec/test/compile_execute_cmds	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/test/compile_execute_cmds	2023-03-29 18:17:42.333220781 -0400
@@ -0,0 +1,50 @@
+
+AOMP=${AOMP:-/usr/lib/aomp}
+AOMP_GPU=${AOMP_GPU:-gfx908}
+export LD_LIBRARY_PATH=$AOMP/lib
+
+echo $AOMP/bin/clang -O3 -fopenmp --offload-arch=$AOMP_GPU hostexec_test.c -o hostexec_test
+$AOMP/bin/clang -O3 -fopenmp --offload-arch=$AOMP_GPU hostexec_test.c -o hostexec_test
+echo 
+echo ===== Running ./hostexec_test with offloading =====
+echo LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=MANDATORY ./hostexec_test
+LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=MANDATORY ./hostexec_test
+echo
+echo ===== Running ./hostexec_test with offloading disabled =====
+echo LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=DISABLED ./hostexec_test
+LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=DISABLED ./hostexec_test
+
+echo 
+echo $AOMP/bin/clang -O3 -fopenmp --offload-arch=$AOMP_GPU variant.c -o variant
+$AOMP/bin/clang -O3 -fopenmp --offload-arch=$AOMP_GPU variant.c -o variant
+echo 
+echo ===== Running ./variant with offloading =====
+echo LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=MANDATORY ./variant
+LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=MANDATORY ./variant
+echo
+echo ===== Running ./variant with offloading disabled =====
+echo LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=DISABLED ./variant
+LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=DISABLED ./variant
+
+echo
+# try to find openmpi
+MPI=~/local/openmpi
+[ ! -d $MPI ] && MPI=/opt/openmpi-4.1.5
+[ ! -d $MPI ] && MPI=/opt/openmpi-4.1.4
+[ ! -d $MPI ] && MPI=/usr/local/openmpi
+[ ! -d $MPI ] && MPI=/usr/lib/openmpi
+[ ! -d $MPI ] && MPI=/usr/lib/x86_64-linux-gnu/openmpi
+[ ! -d $MPI ] && exit 
+echo OMPI_CC=$AOMP/bin/clang++ $MPI/bin/mpicc -O2 -fopenmp -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=$AOMP_GPU mpi-in-target-region.cpp -I. -o mpi-in-target-region
+OMPI_CC=$AOMP/bin/clang++ $MPI/bin/mpicc -O2 -fopenmp -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=$AOMP_GPU mpi-in-target-region.cpp -I. -o mpi-in-target-region
+echo
+echo ===== Running ./mpi-in-target-region with offloading =====
+echo LIBOMPTARGET_KERNEL_TRACE=1 OMP_TARGET_OFFLOAD=MANDATORY ./variant
+echo LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MPI/lib:/usr/lib/x86_64-linux-gnu: $MPI/bin/mpirun -np 2 --mca btl_openib_warn_no_device_params_found 0 ./mpi-in-target-region
+LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$MPI/lib:/usr/lib/x86_64-linux-gnu: $MPI/bin/mpirun -np 2 --mca btl_openib_warn_no_device_params_found 0 ./mpi-in-target-region
+
+echo
+echo cleanup 
+rm ./variant
+rm ./hostexec_test
+rm ./mpi-in-target-region
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/test/hostexec_test.c llvm-project/openmp/libomptarget/hostexec/test/hostexec_test.c
--- llvm-project.upstream/openmp/libomptarget/hostexec/test/hostexec_test.c	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/test/hostexec_test.c	2023-03-27 20:49:29.770252359 -0400
@@ -0,0 +1,159 @@
+#include <hostexec.h>
+#include <omp.h>
+#include <stdarg.h>
+#include <stdio.h>
+
+int myintfn(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  int *a = va_arg(args, int *);
+  int i2 = va_arg(args, int);
+  int i3 = va_arg(args, int);
+  va_end(args);
+  int rv = i2 + i3;
+  printf("  INSIDE myintfn:  fnptr:%p  &a:%p int arg2:%d  int arg3:%d rv:%d \n",
+         fnptr, a, i2, i3, rv);
+  return rv;
+}
+double mydoublefn(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  int *a = va_arg(args, int *);
+  int i2 = va_arg(args, int);
+  int i3 = va_arg(args, int);
+  double rv = (double)(i2 + i3) * 1.1;
+  printf(
+      "  INSIDE mydoublefn:  fnptr:%p  &a:%p int arg2:%d  int arg3:%d rv:%f \n",
+      fnptr, a, i2, i3, rv);
+  va_end(args);
+  return rv;
+}
+long mylongfn(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  int *a = va_arg(args, int *);
+  int i2 = va_arg(args, int);
+  int i3 = va_arg(args, int);
+  long rv = -(long)(i2 + i3);
+  printf(
+      "  INSIDE mylongfn:  fnptr:%p  &a:%p int arg2:%d  int arg3:%d rv:%ld \n",
+      fnptr, a, i2, i3, rv);
+  va_end(args);
+  return rv;
+}
+float myfloatfn(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  int *a = va_arg(args, int *);
+  int i2 = va_arg(args, int);
+  int i3 = va_arg(args, int);
+  float rv = (float)(i2 + i3) * 1.1;
+  printf(
+      "  INSIDE myfloatfn:  fnptr:%p  &a:%p int arg2:%d  int arg3:%d rv:%f \n",
+      fnptr, a, i2, i3, rv);
+  va_end(args);
+  return rv;
+}
+void my4argvoidfn(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  int *a = va_arg(args, int *);
+  int i2 = va_arg(args, int);
+  int i3 = va_arg(args, int);
+  printf("  INSIDE my4argvoidfn:  fnptr:%p  &a:%p int arg2:%d  int arg3:%d  \n",
+         fnptr, a, i2, i3);
+  va_end(args);
+}
+
+int main() {
+  int N = 4;
+  int a[N];
+  int b[N];
+  for (int i = 0; i < N; i++) {
+    a[i] = 0;
+    b[i] = i;
+  }
+
+  printf("\nTesting native variadic function pointers fnptr:%p  &a:%p\n", myintfn, (void*) a);
+  uint sim1 = myintfn(myintfn, &a, 2, 3);
+  double sim1d = mydoublefn(mydoublefn, &a, 2, 3);
+  my4argvoidfn(my4argvoidfn, &a, 2, 3);
+  printf("Return values are %d  and %f \n", sim1, sim1d);
+
+  printf("\nTesting hostexec wrappers/macro on host (no target region)\n");
+  printf("  Results should be same as above  fn_ptr:%p &a:%p\n", myintfn, &a);
+  uint sim2 = hostexec_int(myintfn, &a, 2, 3);
+  double sim2d = hostexec_double(mydoublefn, &a, 2, (int)3);
+  hostexec(my4argvoidfn, &a, 2, 3);
+  printf("Return values are %d  and %f \n", sim2, sim2d);
+
+  // function pointers are not captured so convert to variables
+  hostexec_int_t *myintfn_var = myintfn;
+  hostexec_double_t *mydoublefn_var = mydoublefn;
+  hostexec_float_t *myfloatfn_var = myfloatfn;
+  hostexec_long_t *mylongfn_var = mylongfn;
+  hostexec_t *my4argvoidfn_var = my4argvoidfn;
+
+  printf("\nTesting hostexec variadic functions in simple target region \n");
+  printf("  Results should be same as above  fn_ptr:%p &a:%p\n", myintfn, &a);
+#pragma omp target map(to:a[0:N]) 
+  {
+  uint sim2 = hostexec_int(myintfn_var, &a, 2, 3);
+  double sim2d = hostexec_double(mydoublefn_var, &a, 2, (int)3);
+  hostexec(my4argvoidfn_var, &a, 2, 3);
+  printf("target printf: Return values are %d  and %f \n", sim2, sim2d);
+  }
+
+  int failcode = 0;
+  printf("\nTesting hostexec variadic functions in omp parallel target region \n");
+#pragma omp target parallel for map(from: a[0:N]) map(to:b[0:N]) map(tofrom : failcode) 
+  for (int j = 0; j < N; j++) {
+    a[j] = b[j];
+    uint rv = hostexec_int(myintfn_var, &a, j, a[j]);
+    double rvd = hostexec_double(mydoublefn_var, &a, j, a[j]);
+    float rvf = hostexec_float(myfloatfn_var, &a, j, a[j]);
+    long rvl = hostexec_long(mylongfn_var, &a, j, a[j]);
+    hostexec(my4argvoidfn_var, &a, j, a[j]);
+    printf( "target printf: t:%d of %d :: j:%d a[j]:%d return_vals int:%d "
+            "double:%f float:%f long:%ld \n",
+         omp_get_thread_num(), omp_get_num_threads(), j, a[j], rv, rvd, rvf, rvl);
+    if (rv != j + a[j]) {
+      failcode++;
+      fprintf(stderr, "hostexec_int failed\n");
+    }
+    if (rvd != (double)(j + a[j]) * 1.1) {
+      failcode++;
+      fprintf(stderr, "hostexec_double failed\n");
+    }
+    if (rvf - ((float)(j + a[j]) * 1.1) > 10e-8) {
+      failcode++;
+      fprintf(stderr, "hostexec_float failed rvf:%f answer:%f \n", rvf,
+              (float)(j + a[j]) * 1.1);
+    }
+    if (rvd - ((double)(j + a[j]) * 1.1) > 10e-15) {
+      failcode++;
+      fprintf(stderr, "hostexec_double failed rvd:%f answer:%f \n", rvd,
+              (double)(j + a[j]) * 1.1);
+    }
+    if (rvl != -(long)(j + a[j])) {
+      failcode++;
+      printf("hostexec_long FAILED %ld rvl:%ld\n", -(long)(j + a[j]), rvl);
+    }
+  }
+
+  // Check that b was copied into a
+  int rc = 0;
+  for (int i = 0; i < N; i++)
+    if (a[i] != b[i]) {
+      rc++;
+      printf("Wrong value: a[%d]=%d\n", i, a[i]);
+    }
+
+  if (!rc && !failcode) {
+    printf("Success\n");
+    return EXIT_SUCCESS;
+  } else {
+    printf("Failure %d\n", failcode);
+    return EXIT_FAILURE;
+  }
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi-in-target-region.cpp llvm-project/openmp/libomptarget/hostexec/test/mpi-in-target-region.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi-in-target-region.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/test/mpi-in-target-region.cpp	2023-03-30 16:08:18.550748956 -0400
@@ -0,0 +1,54 @@
+#include "mpi_supplemental.h"
+#include "omp.h"
+#include <mpi.h>
+#include <stdio.h>
+// #define VSIZE 256 * 8
+#define VSIZE 256
+
+int main(int argc, char *argv[]) {
+  _mpilib_set_device_globals();
+  int numranks, rank;
+  MPI_Init(&argc, &argv);
+  MPI_Comm_size(MPI_COMM_WORLD, &numranks);
+  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
+  printf("Number of Ranks= %d My rank= %d\n", numranks, rank);
+  MPI_Comm _mpi_comm = MPI_COMM_WORLD;
+  MPI_Datatype _mpi_int = MPI_INT;
+  int *send_recv_buffer = (int *)malloc(VSIZE * sizeof(int));
+  printf("buffer address is %p\n", send_recv_buffer);
+
+#pragma omp target teams distribute parallel for map(to                        \
+                                                     : send_recv_buffer        \
+                                                     [0:VSIZE])
+  for (int i = 0; i < VSIZE; i++) {
+    int rc = 0;
+    int TH = omp_get_thread_num();
+    int NTH = omp_get_num_threads();
+    int TM = omp_get_team_num();
+    int NTM = omp_get_num_teams();
+    int L = TH % 64;
+    if (rank == 0) {
+      send_recv_buffer[i] = -i;
+      if (L == 0 || L == 63) // Only print 1st and last lane
+        printf("P:%d TAG:%d sending %d with dev addr %p team:%d of %d  "
+               "thread:%d of %d  LANE:%d  WARP:%d \n",
+               rank, i, send_recv_buffer[i], &send_recv_buffer[i], TM, NTM, TH,
+               NTH, L, TH / 64);
+      MPI_Send(&send_recv_buffer[i], 1, _mpi_int, 1, i, _mpi_comm);
+    } else {
+      MPI_Recv(&send_recv_buffer[i], 1, _mpi_int, 0, i, _mpi_comm,
+               MPI_STATUS_IGNORE);
+      if (send_recv_buffer[i] != -i)
+        rc = 1;
+      if (rc != 0 || L == 0 || L == 63) // Only print 1st and last lane
+        printf("P:%d TAG:%d received %d with dev addr %p team:%d of %d  "
+               "thread:%d of %d  LANE:%d  WARP:%d rc:%d\n",
+               rank, i, send_recv_buffer[i], &send_recv_buffer[i], TM, NTM, TH,
+               NTH, L, TH / 64, rc);
+    }
+  } // end for loop and target region
+
+  MPI_Finalize();
+  //  fprintf(stderr,"rc = %d\n",rc);
+  return 0;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi-simple.cpp llvm-project/openmp/libomptarget/hostexec/test/mpi-simple.cpp
--- llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi-simple.cpp	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/test/mpi-simple.cpp	2023-03-30 13:22:05.968408550 -0400
@@ -0,0 +1,37 @@
+#include "mpi_supplemental.h"
+#include "omp.h"
+#include <mpi.h>
+#include <stdio.h>
+
+int main(int argc, char *argv[]) {
+  _mpilib_set_device_globals();
+  int numranks, rank;
+  MPI_Init(&argc, &argv);
+  MPI_Comm_size(MPI_COMM_WORLD, &numranks);
+  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
+  printf("Number of Ranks= %d My rank= %d\n", numranks, rank);
+  MPI_Comm _mpi_comm = MPI_COMM_WORLD;
+  MPI_Datatype _mpi_int = MPI_INT;
+  int send_recv_buffer[2];
+  printf("buffer address is %p\n", send_recv_buffer);
+
+#pragma omp target
+  {
+    if (rank == 0) {
+      send_recv_buffer[0] = -123;
+      printf("\nProcess %d sending  %d from dev address %p to process 1\n",
+             rank, send_recv_buffer[0], send_recv_buffer);
+      MPI_Send(&send_recv_buffer[0], 1, _mpi_int, 1, 0, _mpi_comm);
+    }
+    if (rank == 1) {
+      MPI_Recv(&send_recv_buffer[0], 1, _mpi_int, 0, 0, _mpi_comm,
+               MPI_STATUS_IGNORE);
+      printf("\nProcess %d received %d from dev address %p from process 0\n\n",
+             rank, send_recv_buffer[0], send_recv_buffer);
+    }
+  }
+
+  MPI_Finalize();
+
+  return 0;
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi_supplemental.h llvm-project/openmp/libomptarget/hostexec/test/mpi_supplemental.h
--- llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi_supplemental.h	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/test/mpi_supplemental.h	2023-03-30 13:11:14.330705619 -0400
@@ -0,0 +1,79 @@
+//
+// mpi_supplemental.h: Supplemental header to build device variants for specific
+//                     openmpi host function declarations using hostexec:
+//
+// int MPI_Send(const void *buf, int count, MPI_Datatype datatype, int dest,
+//              int tag, MPI_Comm comm);
+// int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source,
+//              int tag, MPI_Comm comm, MPI_Status *status);
+//
+// These functions can now be called from OpenMP target regions without
+// source code modifications.
+//
+#include <hostexec.h>
+#include <mpi.h>
+#include <stdarg.h>
+
+//  There are 4 parts to a supplemental header.
+
+// 1. Create variadic proxy functions
+extern int V_MPI_Send(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  int v1 = va_arg(args, int);
+  MPI_Datatype v2 = va_arg(args, MPI_Datatype);
+  int v3 = va_arg(args, int);
+  int v4 = va_arg(args, int);
+  MPI_Comm v5 = va_arg(args, MPI_Comm);
+  va_end(args);
+  int rval = MPI_Send(v0, v1, v2, v3, v4, v5);
+  return rval;
+}
+extern int V_MPI_Recv(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  int v1 = va_arg(args, int);
+  MPI_Datatype v2 = va_arg(args, MPI_Datatype);
+  int v3 = va_arg(args, int);
+  int v4 = va_arg(args, int);
+  MPI_Comm v5 = va_arg(args, MPI_Comm);
+  MPI_Status *v6 = va_arg(args, MPI_Status *);
+  va_end(args);
+  int rval = MPI_Recv(v0, v1, v2, v3, v4, v5, v6);
+  return rval;
+}
+
+#pragma omp begin declare target
+
+// 2. Create global variables to store pointers to variadic proxy functions.
+//    These must be inside a declare target.
+hostexec_int_t *V_MPI_Send_var;
+hostexec_int_t *V_MPI_Recv_var;
+
+// 3. Create the device variants that call hostexec.
+#pragma omp begin declare variant match(                                       \
+    device = {arch(amdgcn, nvptx, nvptx64)},                                   \
+    implementation = {extension(match_any)})
+int MPI_Send(const void *buf, int count, MPI_Datatype datatype, int dest,
+             int tag, MPI_Comm comm) {
+  return hostexec_int((void *)V_MPI_Send_var, buf, count, datatype, dest, tag,
+                      comm);
+}
+int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag,
+             MPI_Comm comm, MPI_Status *st) {
+  return hostexec_int((void *)V_MPI_Recv_var, buf, count, datatype, source, tag,
+                      comm, st);
+}
+#pragma omp end declare variant
+#pragma omp end declare target
+
+// 4.  Initialize pointers to host-only library functions on the device.
+//     These are host pointers stored as device globals which are passed
+//     to hostexec in the device variants above.
+void _mpilib_set_device_globals() {
+  V_MPI_Send_var = V_MPI_Send;
+  V_MPI_Recv_var = V_MPI_Recv;
+#pragma omp target update to(V_MPI_Send_var, V_MPI_Recv_var)
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi_test llvm-project/openmp/libomptarget/hostexec/test/mpi_test
--- llvm-project.upstream/openmp/libomptarget/hostexec/test/mpi_test	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/test/mpi_test	2023-03-30 14:42:25.281486575 -0400
@@ -0,0 +1,26 @@
+
+TRUNK=${TRUNK:-$HOME/rocm/trunk}
+GPU=${GPU:-gfx908}
+export LD_LIBRARY_PATH=$TRUNK/lib
+binary=mpi_in_target_region
+[ -f $binary ] && rm $binary
+
+# try to find openmpi
+MPI=~/local/openmpi
+[ ! -d $MPI ] && MPI=/opt/openmpi-4.1.5
+[ ! -d $MPI ] && MPI=/opt/openmpi-4.1.4
+[ ! -d $MPI ] && MPI=/usr/local/openmpi
+[ ! -d $MPI ] && MPI=/usr/lib/openmpi
+[ ! -d $MPI ] && MPI=/usr/lib/x86_64-linux-gnu/openmpi
+[ ! -d $MPI ] && exit 
+echo
+echo ===== Compiling mpi-in-target-region.cpp =====
+echo OMPI_CC=$TRUNK/bin/clang++ $MPI/bin/mpicc -O2 -fopenmp -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=$GPU mpi-in-target-region.cpp -I. -o $binary
+OMPI_CC=$TRUNK/bin/clang++ $MPI/bin/mpicc -O2 -fopenmp -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=$GPU mpi-in-target-region.cpp -I. -o $binary
+echo
+if [ -f $binary ] ; then 
+echo ===== Running ./$binary offloading =====
+echo
+LIBOMPTARGET_KERNEL_TRACE=1 LD_LIBRARY_PATH=$TRUNK/lib:$MPI/lib:/usr/lib/x86_64-linux-gnu $MPI/bin/mpirun -np 2 --mca btl_openib_warn_no_device_params_found 0 ./$binary
+fi
+
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/hostexec/test/variant.c llvm-project/openmp/libomptarget/hostexec/test/variant.c
--- llvm-project.upstream/openmp/libomptarget/hostexec/test/variant.c	1969-12-31 19:00:00.000000000 -0500
+++ llvm-project/openmp/libomptarget/hostexec/test/variant.c	2023-03-29 11:01:26.524277251 -0400
@@ -0,0 +1,107 @@
+#include <hostexec.h>
+#include <omp.h>
+#include <stdarg.h>
+#include <stdio.h>
+//#include <unistd.h>
+
+// In this test case, think of my_increment as some library function that may
+// only execute on the host. When my_increment is called from a target region,
+// the device variant of my_increment will use hostexec_int to run my_increment
+// on the host.
+
+extern int my_increment(int val) {
+  int rval = val + 1;
+  return rval;
+}
+
+//========== START of example supplemental library header ==========
+//
+//  To enable host-only execution inside an OpenMP target region of any
+//  library function without source modification in the target region,
+//  the library maintainer would create a "supplemental" header.
+//  This supplemental header has 4 parts for each host-only library function
+//  This is alternative to source modifications to use reverse offload.
+//  This supplemental header could be auto-generated from information found
+//  in the library function declaration.
+
+// 1. Create variadic proxy function for each host-only library function.
+//    This variadic proxy function builds a non-variadic call site on the
+//    host which results in a correct stack frame. The host service thread
+//    calls the variadic function via function pointer passed by hostexec.
+//    First, the args are unpacked suitable for a variadic function call. 
+//    The service thread cannot call a non-variadic library function directly 
+//    because we cannot recreate every possible function template.
+//    Having the library owner be responsible for the conversion from variadic
+//    to nonvariadic function provides the ability to use hostexec for
+//    any library function without changing the user code.
+extern int V_my_increment(void*fnptr, ...) { 
+  va_list args;
+  va_start(args, fnptr);
+  int v0 = va_arg(args, int);
+  va_end(args);
+  return my_increment(v0);  // The non-variadic call site
+}
+
+#pragma omp begin declare target 
+
+// 2. Create a global variable to store pointer to variadic proxy function
+hostexec_int_t * V_my_increment_var;
+
+// 3. Create the device variant of the library function that calls hostexec
+//    with the pointer to the variadic proxy function. 
+#pragma omp begin declare variant match(device = {arch(amdgcn, nvptx, nvptx64)})
+int my_increment(int val) {return hostexec_int(V_my_increment_var,val);}
+#pragma omp end declare variant  
+#pragma omp end declare target
+
+// 4.  Initialize pointers to host-only library functions on the device.
+//     These are host pointers stored as device globals passed to hostexec.
+void _mylib_set_device_globals(){
+  V_my_increment_var  = V_my_increment;
+  #pragma omp target update to(V_my_increment_var)
+}
+
+//========== END example supplemental library header ==========
+
+void vmul(int*a, int*b, int*c, int N){
+  #pragma omp target teams distribute parallel for map(to: a[0:N],b[0:N]) map(from:c[0:N]) 
+  for(int i=0;i<N;i++) {
+    c[i]=a[i]*b[i];
+    c[i] = my_increment(c[i]) ; // example call to library function.
+    c[i]--;  // adjust (decrement) to get the correct answer in this test case.
+  }
+}
+
+int main(){
+    // Initialize pointers to host-only library functions on device.
+    // FIXME: Get this done by initialization of libomptarget globals.
+    _mylib_set_device_globals(); 
+    
+    const int N = 10;    
+    int a[N],b[N],c[N],validate[N];
+    int flag=-1; // Mark Success
+    for(int i=0;i<N;i++) {
+        a[i]=i+1;
+        b[i]=i+2;
+        validate[i]=a[i]*b[i];
+    }
+
+    vmul(a,b,c,N);
+
+    for(int i=0;i<N;i++) {
+        if(c[i]!=validate[i]) {
+//          print 1st bad index
+            if( flag == -1 ) 
+              printf("First fail: c[%d](%d) != validate[%d](%d)\n",i,c[i],i,validate[i]);
+            flag = i;
+        }
+    }
+    if( flag == -1 ){
+        printf("Success\n");
+        return 0;
+    } else {
+        printf("Last fail: c[%d](%d) != validate[%d](%d)\n",flag,c[flag],flag,validate[flag]);
+        printf("Fail\n");
+        return 1;
+    }
+}
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt	2023-03-28 09:58:58.126055281 -0400
+++ llvm-project/openmp/libomptarget/plugins/amdgpu/CMakeLists.txt	2023-03-28 15:19:22.762748013 -0400
@@ -85,11 +85,14 @@
   elf_common
   ${LIBOMPTARGET_DEP_LIBRARIES}
   ${OPENMP_PTHREAD_LIB}
+  -Wl,--whole-archive amdgcn_hostexec_services -Wl,--no-whole-archive
   "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/../exports"
   ${LDFLAGS_UNDEFINED}
 
   NO_INSTALL_RPATH
 )
+add_dependencies(omptarget.rtl.amdgpu omptarget.devicertl.amdgpu )
+add_dependencies(omptarget.rtl.amdgpu amdgcn_hostexec_services)
 
 target_include_directories(
   omptarget.rtl.amdgpu
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp llvm-project/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp
--- llvm-project.upstream/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp	2023-02-27 09:21:39.111743452 -0500
+++ llvm-project/openmp/libomptarget/plugins/amdgpu/src/rtl.cpp	2023-03-28 15:19:22.762748013 -0400
@@ -53,16 +53,18 @@
 // implement a fallback for toolchains that do not yet have a hostrpc library.
 extern "C" {
 uint64_t hostrpc_assign_buffer(hsa_agent_t Agent, hsa_queue_t *ThisQ,
-                               uint32_t DeviceId);
-hsa_status_t hostrpc_init();
+                               uint32_t DeviceId,
+                               hsa_amd_memory_pool_t HostMemoryPool,
+                               hsa_amd_memory_pool_t DevMemoryPool);
 hsa_status_t hostrpc_terminate();
 
-__attribute__((weak)) hsa_status_t hostrpc_init() { return HSA_STATUS_SUCCESS; }
 __attribute__((weak)) hsa_status_t hostrpc_terminate() {
   return HSA_STATUS_SUCCESS;
 }
-__attribute__((weak)) uint64_t hostrpc_assign_buffer(hsa_agent_t, hsa_queue_t *,
-                                                     uint32_t DeviceId) {
+__attribute__((weak)) uint64_t
+hostrpc_assign_buffer(hsa_agent_t, hsa_queue_t *, uint32_t DeviceId,
+                      hsa_amd_memory_pool_t HostMemoryPool,
+                      hsa_amd_memory_pool_t DevMemoryPool) {
   DP("Warning: Attempting to assign hostrpc to device %u, but hostrpc library "
      "missing\n",
      DeviceId);
@@ -998,9 +1000,6 @@
       return;
     }
 
-    // Init hostcall soon after initializing hsa
-    hostrpc_init();
-
     Err = findAgents([&](hsa_device_type_t DeviceType, hsa_agent_t Agent) {
       if (DeviceType == HSA_DEVICE_TYPE_CPU) {
         CPUAgents.push_back(Agent);
@@ -1104,6 +1103,7 @@
     DeviceStateStore.clear();
     KernelArgPoolMap.clear();
     // Terminate hostrpc before finalizing hsa
+    DP("Terminating hostrpc service thread and buffer if allocated \n");
     hostrpc_terminate();
 
     hsa_status_t Err;
@@ -1503,7 +1503,9 @@
         static pthread_mutex_t HostcallInitLock = PTHREAD_MUTEX_INITIALIZER;
         pthread_mutex_lock(&HostcallInitLock);
         uint64_t Buffer = hostrpc_assign_buffer(
-            DeviceInfo().HSAAgents[DeviceId], Queue, DeviceId);
+            DeviceInfo().HSAAgents[DeviceId], Queue, DeviceId,
+            DeviceInfo().HostFineGrainedMemoryPool,
+            DeviceInfo().getDeviceMemoryPool(DeviceId));
         pthread_mutex_unlock(&HostcallInitLock);
         if (!Buffer) {
           DP("hostrpc_assign_buffer failed, gpu would dereference null and "
@@ -1531,6 +1533,10 @@
 
         // initialise pointer for implicit_argument_count == 0 ABI
         ImplArgs->HostcallPtr = Buffer;
+        DP("Hostrpc buffer allocated at %p and service thread started\n",
+           (void *)Buffer);
+      } else {
+        DP("No hostrpc buffer or service thread required\n");
       }
 
       Packet->kernarg_address = KernArg;
@@ -2140,7 +2146,7 @@
     hsa_status_t Err = moduleRegisterFromMemoryToPlace(
         KernelInfo, SymbolInfo, (void *)Image->ImageStart, ImgSize, DeviceId,
         [&](void *Data, size_t Size) {
-          if (imageContainsSymbol(Data, Size, "needs_hostcall_buffer")) {
+          if (imageContainsSymbol(Data, Size, "__needs_host_services")) {
             __atomic_store_n(&DeviceInfo().HostcallRequired, true,
                              __ATOMIC_RELEASE);
           }
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt	2023-02-27 09:21:59.251665276 -0500
+++ llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/CMakeLists.txt	2023-03-28 15:19:22.762748013 -0400
@@ -85,11 +85,13 @@
   PluginInterface
   ${LIBOMPTARGET_DEP_LIBRARIES}
   ${OPENMP_PTHREAD_LIB}
+  -Wl,--whole-archive amdgcn_hostexec_services -Wl,--no-whole-archive
   "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/../exports"
   ${LDFLAGS_UNDEFINED}
 
   NO_INSTALL_RPATH
 )
+add_dependencies(omptarget.rtl.amdgpu.nextgen amdgcn_hostexec_services)
 
 target_include_directories(
   omptarget.rtl.amdgpu.nextgen
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp
--- llvm-project.upstream/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp	2023-03-23 09:58:45.801808275 -0400
+++ llvm-project/openmp/libomptarget/plugins-nextgen/amdgpu/src/rtl.cpp	2023-03-28 15:19:22.762748013 -0400
@@ -58,6 +58,27 @@
 namespace target {
 namespace plugin {
 
+extern "C" {
+uint64_t hostrpc_assign_buffer(hsa_agent_t Agent, hsa_queue_t *ThisQ,
+                               uint32_t DeviceId,
+                               hsa_amd_memory_pool_t HostMemoryPool,
+                               hsa_amd_memory_pool_t DevMemoryPool);
+hsa_status_t hostrpc_terminate();
+__attribute__((weak)) hsa_status_t hostrpc_terminate() {
+  return HSA_STATUS_SUCCESS;
+}
+__attribute__((weak)) uint64_t
+hostrpc_assign_buffer(hsa_agent_t, hsa_queue_t *, uint32_t DeviceId,
+                      hsa_amd_memory_pool_t HostMemoryPool,
+                      hsa_amd_memory_pool_t DevMemoryPool) {
+  // FIXME:THIS SHOULD BE HARD FAIL
+  DP("Warning: Attempting to assign hostrpc to device %u, but hostrpc library "
+     "missing\n",
+     DeviceId);
+  return 0;
+}
+}
+
 /// Forward declarations for all specialized data structures.
 struct AMDGPUKernelTy;
 struct AMDGPUDeviceTy;
@@ -129,6 +150,11 @@
                        "Error in hsa_amd_agent_iterate_memory_pools: %s");
 }
 
+extern "C" uint64_t hostrpc_assign_buffer(hsa_agent_t Agent, hsa_queue_t *ThisQ,
+                                          uint32_t DeviceId,
+                                          hsa_amd_memory_pool_t HostMemoryPool,
+                                          hsa_amd_memory_pool_t DevMemoryPool);
+extern "C" hsa_status_t hostrpc_terminate();
 } // namespace utils
 
 /// Utility class representing generic resource references to AMDGPU resources.
@@ -384,6 +410,9 @@
     return It->second;
   }
 
+  /// Does device image contain Symbol
+  bool hasDeviceSymbol(GenericDeviceTy &Device, StringRef SymbolName) const;
+
 private:
   /// The exectuable loaded on the agent.
   hsa_executable_t Executable;
@@ -397,6 +426,7 @@
   /// Create an AMDGPU kernel with a name and an execution mode.
   AMDGPUKernelTy(const char *Name, OMPTgtExecModeFlags ExecutionMode)
       : GenericKernelTy(Name, ExecutionMode),
+        GlobalTy_device_st_buf("service_thread_buf", sizeof(uint64_t)),
         ImplicitArgsSize(sizeof(utils::AMDGPUImplicitArgsTy)) {}
 
   /// Initialize the AMDGPU kernel.
@@ -444,8 +474,20 @@
       INFO(OMP_INFOTYPE_PLUGIN_KERNEL, Device.getDeviceId(),
            "Could not read extra information for kernel %s.", getName());
 
+    needs_host_services =
+        AMDImage.hasDeviceSymbol(Device, "__needs_host_services");
+    if (needs_host_services) {
+      // GenericGlobalHandlerTy * GHandler = Plugin::createGlobalHandler();
+      if (auto Err = rpc_buf_handler->getGlobalMetadataFromDevice(
+              Device, AMDImage, GlobalTy_device_st_buf))
+        return Err;
+    }
+
     return Plugin::success();
   }
+  bool needs_host_services;
+  GlobalTy GlobalTy_device_st_buf;
+  GenericGlobalHandlerTy *rpc_buf_handler = Plugin::createGlobalHandler();
 
   /// Launch the AMDGPU kernel function.
   Error launchImpl(GenericDeviceTy &GenericDevice, uint32_t NumThreads,
@@ -637,6 +679,7 @@
     // Push the barrier with the lock acquired.
     return pushBarrierImpl(OutputSignal, InputSignal1, InputSignal2);
   }
+  hsa_queue_t *getHsaQueue() { return Queue; }
 
 private:
   /// Push a barrier packet that will wait up to two input signals. Assumes the
@@ -757,6 +800,8 @@
 /// devices. This class relies on signals to implement streams and define the
 /// dependencies between asynchronous operations.
 struct AMDGPUStreamTy {
+  AMDGPUQueueTy *getQueue() { return &Queue; };
+
 private:
   /// Utility struct holding arguments for async H2H memory copies.
   struct MemcpyArgsTy {
@@ -1424,6 +1469,9 @@
 
     return Plugin::success();
   }
+  AMDGPUMemoryPoolTy *getCoarseGrainedMemoryPool() {
+    return CoarseGrainedMemoryPools[0];
+  }
 
   /// Retrieve and construct all memory pools from the device agent(s).
   virtual Error retrieveAllMemoryPools() = 0;
@@ -2236,7 +2284,6 @@
 Expected<hsa_executable_symbol_t>
 AMDGPUDeviceImageTy::findDeviceSymbol(GenericDeviceTy &Device,
                                       StringRef SymbolName) const {
-
   AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(Device);
   hsa_agent_t Agent = AMDGPUDevice.getAgent();
 
@@ -2251,6 +2298,16 @@
   return Symbol;
 }
 
+bool AMDGPUDeviceImageTy::hasDeviceSymbol(GenericDeviceTy &Device,
+                                          StringRef SymbolName) const {
+  AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(Device);
+  hsa_agent_t Agent = AMDGPUDevice.getAgent();
+  hsa_executable_symbol_t Symbol;
+  hsa_status_t Status = hsa_executable_get_symbol_by_name(
+      Executable, SymbolName.data(), &Agent, &Symbol);
+  return (Status == HSA_STATUS_SUCCESS);
+}
+
 template <typename ResourceTy>
 Error AMDGPUResourceRef<ResourceTy>::create(GenericDeviceTy &Device) {
   if (Resource)
@@ -2418,6 +2475,7 @@
 
   /// Deinitialize the plugin.
   Error deinitImpl() override {
+    utils::hostrpc_terminate();
     // The HSA runtime was not initialized, so nothing from the plugin was
     // actually initialized.
     if (!Initialized)
@@ -2595,6 +2653,29 @@
 
   AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(GenericDevice);
   AMDGPUStreamTy &Stream = AMDGPUDevice.getStream(AsyncInfoWrapper);
+  if (needs_host_services) {
+    int32_t devid = AMDGPUDevice.getDeviceId();
+    hsa_amd_memory_pool_t host_mem_pool =
+        HostDevice.getFineGrainedMemoryPool().get();
+    hsa_amd_memory_pool_t device_mem_pool =
+        AMDGPUDevice.getCoarseGrainedMemoryPool()->get();
+    hsa_queue_t *hsa_queue = Stream.getQueue()->getHsaQueue();
+    uint64_t Buffer =
+        utils::hostrpc_assign_buffer(AMDGPUDevice.getAgent(), hsa_queue, devid,
+                                     host_mem_pool, device_mem_pool);
+    GlobalTy GlobalTy_host_st_buf("service_thread_buf", sizeof(uint64_t),
+                                  &Buffer);
+    if (auto Err = rpc_buf_handler->writeGlobalToDevice(
+            AMDGPUDevice, GlobalTy_host_st_buf, GlobalTy_device_st_buf)) {
+      DP("Missing symbol %s, continue execution anyway.\n",
+         GlobalTy_host_st_buf.getName().data());
+      consumeError(std::move(Err));
+    }
+    DP("Hostrpc buffer allocated at %p and service thread started\n",
+       (void *)Buffer);
+  } else {
+    DP("No hostrpc buffer or service thread required\n");
+  }
 
   // Push the kernel launch into the stream.
   return Stream.pushKernelLaunch(*this, AllArgs, NumThreads, NumBlocks,
diff -Naur -x .git -x __pycache__ llvm-project.upstream/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt
--- llvm-project.upstream/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt	2023-02-27 09:21:59.251665276 -0500
+++ llvm-project/openmp/libomptarget/plugins-nextgen/cuda/CMakeLists.txt	2023-03-28 15:19:22.762748013 -0400
@@ -38,10 +38,12 @@
   MemoryManager
   PluginInterface
   ${OPENMP_PTHREAD_LIB}
+  "-Wl,--whole-archive nvptx_hostexec_services -Wl,--no-whole-archive"
   "-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/../exports,-z,defs"
 
   NO_INSTALL_RPATH
 )
+add_dependencies(omptarget.rtl.cuda.nextgen nvptx_hostexec_services)
 
 if(LIBOMPTARGET_DEP_CUDA_FOUND AND NOT LIBOMPTARGET_FORCE_DLOPEN_LIBCUDA)
   libomptarget_say("Building CUDA plugin linked against libcuda")
